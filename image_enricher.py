# FILE: image_enricher.py
import os
import requests
import numpy as np
from typing import List, Dict, Optional
from bs4 import BeautifulSoup
from urllib.parse import urlparse

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ ÙˆØ­Ø¯Ø§Øª Ø§Ù„Ù…Ø´Ø±ÙˆØ¹
import image_processor 
from config import log

# --- AI Intelligence (Ù„Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠØ©) ---
try:
    from sentence_transformers import SentenceTransformer
    from sklearn.metrics.pairwise import cosine_similarity
    AI_MODEL = SentenceTransformer('all-MiniLM-L6-v2')
    HAS_AI = True
except ImportError:
    log("âš ï¸ sentence-transformers not found. Ranking logic will be limited.")
    HAS_AI = False
    AI_MODEL = None

# --- Configuration ---
# Ø¹Ø¯Ø¯ Ø§Ù„ØµÙˆØ± Ø§Ù„ØªÙŠ Ù†Ø¬Ù„Ø¨Ù‡Ø§ ÙÙŠ ÙƒÙ„ Ø¹Ù…Ù„ÙŠØ© Ø¨Ø­Ø«
SEARCH_PAGE_SIZE = 8
# Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„ØµÙˆØ± ÙÙŠ Ø§Ù„Ù…Ù‚Ø§Ù„
MAX_IMAGES = 10 
# --- End Configuration ---

def google_json_api_search(query: str, num_results: int = SEARCH_PAGE_SIZE) -> List[Dict]:
    """
    Ø¨Ø­Ø« Ù…Ø¨Ø§Ø´Ø± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Google Custom Search JSON API.
    """
    api_key = os.getenv("GOOGLE_SEARCH_API_KEY")
    cx = os.getenv("GOOGLE_SEARCH_CX")
    
    if not api_key or not cx:
        log("   âŒ Google API Keys Missing!")
        return []

    url = "https://www.googleapis.com/customsearch/v1"
    params = {
        "key": api_key,
        "cx": cx,
        "q": query,
        "searchType": "image",
        "num": num_results,
        "safe": "high",
        "imgSize": "large",  # Ù†Ø±ÙŠØ¯ ØµÙˆØ±Ø§Ù‹ ÙˆØ§Ø¶Ø­Ø© Ù„Ù„Ø´Ø±Ø­
        "fileType": "jpg,png,webp"
    }
    
    try:
        response = requests.get(url, params=params, timeout=15)
        if response.status_code != 200: return []
        data = response.json()
        
        candidates = []
        for item in data.get("items", []):
            link = item.get("link")
            # Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ø§Ù„Ø£ÙŠÙ‚ÙˆÙ†Ø§Øª ÙˆØ§Ù„ØµÙˆØ± Ø§Ù„ØµØºÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹
            if any(x in link for x in ['logo', 'icon', 'favicon']): continue
            
            candidates.append({
                "url": link,
                "description": item.get("snippet", "") + " " + item.get("title", ""),
                "source": urlparse(item.get("displayLink", "")).netloc,
                "type": "google_search"
            })
        return candidates
    except Exception as e:
        log(f"      âš ï¸ Google Search Error: {e}")
        return []

def gather_general_pool(article_title: str, article_meta: Dict, direct_images: List[Dict]) -> List[Dict]:
    """
    ÙŠØ¬Ù…Ø¹ 'Ø§Ù„Ù…Ø³Ø¨Ø­ Ø§Ù„Ø¹Ø§Ù…': ØµÙˆØ± Ø±Ø³Ù…ÙŠØ© + Ø¨Ø­Ø« Ø¬ÙˆØ¬Ù„ Ø¹Ø§Ù… Ø¹Ù† Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© ÙˆØ§Ù„Ù…Ø®Ø·Ø·Ø§Øª.
    """
    pool = []
    
    # 1. Ø§Ù„ØµÙˆØ± Ø§Ù„Ø±Ø³Ù…ÙŠØ© (Official Extraction) - Ø£ÙˆÙ„ÙˆÙŠØ© Ù‚ØµÙˆÙ‰
    for img in direct_images:
        # Ù†Ø¶ÙŠÙ ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ù„Ù„ÙˆØµÙ Ù„Ø²ÙŠØ§Ø¯Ø© ÙØ±ØµØ© Ø§Ø®ØªÙŠØ§Ø±Ù‡
        img['description'] = f"{img.get('description', '')} official interface screenshot {article_title}"
        pool.append(img)
    
    clean_title = article_title.split(':')[0].replace("Review", "").strip()

    # 2. Ø¨Ø­Ø« Ø¬ÙˆØ¬Ù„ Ø§Ù„Ø¹Ø§Ù… (General Context)
    # Ù†Ø¨Ø­Ø« Ø¹Ù† ØµÙˆØ± Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø¹Ø§Ù…Ø© (Dashboard/UI)
    pool.extend(google_json_api_search(f"{clean_title} dashboard user interface screenshot"))
    
    # Ù†Ø¨Ø­Ø« Ø¹Ù† ØµÙˆØ± Ø¨ÙŠØ§Ù†ÙŠØ© Ø£Ùˆ Ù…Ù‚Ø§Ø±Ù†Ø§Øª (Charts)
    pool.extend(google_json_api_search(f"{clean_title} architecture diagram comparison chart"))

    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±
    unique_pool = {v['url']: v for v in pool}.values()
    log(f"   ğŸŒŠ Image Pool Created: {len(unique_pool)} candidates.")
    return list(unique_pool)

def find_image_for_slot(slot_context: str, article_title: str, image_pool: List[Dict], used_urls: set) -> Optional[Dict]:
    """
    Ø§Ù„Ù…Ù†Ø·Ù‚ Ø§Ù„Ø°ÙƒÙŠ:
    1. Ø§Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ø³Ø¨Ø­ Ø¹Ù† ØµÙˆØ±Ø© Ù…Ø·Ø§Ø¨Ù‚Ø© Ù„Ù„Ø³ÙŠØ§Ù‚.
    2. Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ -> Ø§Ø¨Ø­Ø« ÙÙŠ Ø¬ÙˆØ¬Ù„ Ø®ØµÙŠØµØ§Ù‹ Ù„Ù‡Ø°Ø§ Ø§Ù„Ø³ÙŠØ§Ù‚.
    """
    clean_title = article_title.split(':')[0]
    
    # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ø³Ø¨Ø­ (Pool) ---
    available = [img for img in image_pool if img['url'] not in used_urls]
    
    best_pool_image = None
    if available and HAS_AI:
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Embeddings Ù„Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©
        slot_emb = AI_MODEL.encode(slot_context)
        img_embs = AI_MODEL.encode([img['description'] for img in available])
        scores = cosine_similarity([slot_emb], img_embs)[0]
        
        best_idx = np.argmax(scores)
        best_score = scores[best_idx]
        
        # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„ØµÙˆØ±Ø© Ù…Ù„Ø§Ø¦Ù…Ø© Ø¬Ø¯Ø§Ù‹ (Score > 0.30)ØŒ Ù†Ø£Ø®Ø°Ù‡Ø§ ÙˆÙ†ÙˆÙØ± Ø¨Ø­Ø« Ø¬ÙˆØ¬Ù„
        if best_score > 0.30:
            log(f"      âœ… Found relevant image in Pool (Score: {best_score:.2f})")
            return available[best_idx]

    # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø®Ø§Øµ (Specific Fallback Search) ---
    # Ù„Ù… Ù†Ø¬Ø¯ ØµÙˆØ±Ø© ÙÙŠ Ø§Ù„Ù…Ø³Ø¨Ø­ ØªÙ†Ø§Ø³Ø¨ Ù‡Ø°Ù‡ Ø§Ù„ÙÙ‚Ø±Ø© (Ù…Ø«Ù„Ø§Ù‹: "Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ«Ø¨ÙŠØª")
    # Ø¥Ø°Ù†ØŒ Ù†Ø¨Ø­Ø« Ø®ØµÙŠØµØ§Ù‹ Ø¹Ù†Ù‡Ø§.
    
    # Ù†Ø³ØªØ®Ø±Ø¬ ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¨Ø­Ø« Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ (Ø£ÙˆÙ„ 6 ÙƒÙ„Ù…Ø§Øª Ù…Ø¹Ø¨Ø±Ø©)
    specific_query = f"{clean_title} {slot_context} screenshot"
    log(f"      ğŸ•µï¸â€â™‚ï¸ Pool failed. Triggering SPECIFIC search: '{specific_query[:40]}...'")
    
    specific_results = google_json_api_search(specific_query, num_results=2)
    
    if specific_results:
        candidate = specific_results[0]
        candidate['type'] = 'specific_search' # Ù†Ø¶Ø¹ Ø¹Ù„Ø§Ù…Ø© Ø£Ù†Ù‡Ø§ Ø¨Ø­Ø« Ø®Ø§Øµ
        return candidate

    return None

def enrich_article_html(html: str, article_title: str, article_meta: Dict, direct_images: List[Dict] = []) -> str:
    """
    Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø§Ù„ØªÙŠ ÙŠØ³ØªØ¯Ø¹ÙŠÙ‡Ø§ main.py
    """
    log("âœ¨ [Image Enricher] Starting Analysis...")
    
    # 1. ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù…Ø³Ø¨Ø­ Ø§Ù„Ø¹Ø§Ù…
    image_pool = gather_general_pool(article_title, article_meta, direct_images)
    
    soup = BeautifulSoup(html, 'html.parser')
    
    # 2. ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£Ù…Ø§ÙƒÙ† Ø§Ù„ØªÙŠ ØªØ­ØªØ§Ø¬ ØµÙˆØ± (Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„ÙØ±Ø¹ÙŠØ© H2, H3)
    slots = []
    for element in soup.find_all(['h2', 'h3']):
        text = element.get_text(strip=True)
        # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹ Ø£Ùˆ Ø§Ù„Ø®Ø§ØªÙ…Ø©
        if len(text) < 5 or any(x in text.lower() for x in ["conclusion", "faq", "verdict"]): 
            continue
        
        # Ù†Ø£Ø®Ø° Ø§Ù„Ø¹Ù†ÙˆØ§Ù† + Ø¬Ø²Ø¡ Ù…Ù† Ø§Ù„ÙÙ‚Ø±Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© ÙƒØ³ÙŠØ§Ù‚ Ù„Ù„Ø¨Ø­Ø«
        context = text
        next_tag = element.find_next()
        if next_tag and next_tag.name == 'p':
            context += " " + next_tag.get_text(strip=True)[:100]
            
        slots.append({"context": context, "element": element})

    log(f"   ğŸ“ Identified {len(slots)} slots needing images.")
    
    used_urls = set()
    images_count = 0

    for slot in slots:
        if images_count >= MAX_IMAGES: break
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© (Ù…Ù† Ø§Ù„Ù…Ø³Ø¨Ø­ Ø£Ùˆ Ø¨Ø­Ø« Ø®Ø§Øµ)
        best_image = find_image_for_slot(slot['context'], article_title, image_pool, used_urls)
        
        if best_image:
            # --- Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØ§Ù„Ø±ÙØ¹ (Self-Hosting) ---
            final_url = image_processor.upload_external_image(best_image['url'], f"{article_title} {images_count}")
            
            if not final_url: 
                log("      âš ï¸ Upload failed, skipping image.")
                continue

            # --- Ø§Ù„ØªØ¶Ù…ÙŠÙ† ÙÙŠ Ø§Ù„Ù…Ù‚Ø§Ù„ ---
            caption = best_image['description'].split('...')[0][:100]
            source_lbl = f"Source: {best_image['source']}"
            
            figure = BeautifulSoup(f'''
            <figure style="margin: 30px auto; text-align: center; max-width: 90%;">
                <img src="{final_url}" alt="{caption}" loading="lazy" 
                     style="width: 100%; height: auto; border-radius: 8px; box-shadow: 0 4px 10px rgba(0,0,0,0.1); border: 1px solid #eee;">
                <figcaption style="font-size: 13px; color: #666; margin-top: 8px; font-style: italic;">
                    {caption} <span style="opacity: 0.7;">({source_lbl})</span>
                </figcaption>
            </figure>
            ''', 'html.parser')
            
            slot['element'].insert_after(figure)
            used_urls.add(best_image['url'])
            images_count += 1
            log(f"      âœ… Image inserted for: {slot['context'][:30]}...")

    return str(soup)
