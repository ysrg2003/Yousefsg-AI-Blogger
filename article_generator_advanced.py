import os
import json
import time
import requests
import re
import random
import sys
import datetime
import urllib.parse
import base64
import feedparser
from bs4 import BeautifulSoup
import social_manager
import video_renderer
import youtube_manager
from google import genai
from google.genai import types
import selenium
import webdriver_manager
# ---- Ø£Ø¶Ù Ù‡Ø°Ù‡ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª Ø¨Ø§Ù„Ù‚Ø±Ø¨ Ù…Ù† Ø£Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù„Ù ----
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
# webdriver-manager Ø³ÙŠØ³Ù…Ø­ Ø¨ØªÙ†Ø²ÙŠÙ„ ChromeDriver ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
from webdriver_manager.chrome import ChromeDriverManager
# (Ù‚Ø¯ ØªØ­ØªØ§Ø¬ Ø£ÙŠØ¶Ø§Ù‹ Ù‡Ø°Ù‡ Ù„Ùˆ ÙƒÙ†Øª ØªØ³ØªØ®Ø¯Ù… find_element ÙˆØºÙŠØ±Ù‡Ø§)
from selenium.webdriver.common.by import By
# -------------------------------------------------
import url_resolver
import trafilatura
import ast


# ==============================================================================
# 0. CONFIG & LOGGING
# ==============================================================================

FORBIDDEN_PHRASES = [
    "In today's digital age",
    "The world of AI is ever-evolving",
    "unveils",
    "unveiled",
    "poised to",
    "delve into",
    "game-changer",
    "paradigm shift",
    "tapestry",
    "robust",
    "leverage",
    "underscore",
    "testament to",
    "beacon of",
    "In conclusion",
    "Remember that",
    "It is important to note",
    "Imagine a world",
    "fast-paced world",
    "cutting-edge",
    "realm of"
]

def log(msg):
    timestamp = datetime.datetime.now().strftime("%H:%M:%S")
    print(f"[{timestamp}] {msg}", flush=True)

# ==============================================================================
# 1. CSS STYLING
# ==============================================================================
ARTICLE_STYLE = """
<style>
    .post-body { font-family: 'Segoe UI', Roboto, Helvetica, Arial, sans-serif; line-height: 1.85; color: #2c3e50; font-size: 19px; }
    h2 { color: #111; font-weight: 800; margin-top: 55px; margin-bottom: 25px; border-bottom: 4px solid #f1c40f; padding-bottom: 8px; display: inline-block; font-size: 28px; }
    h3 { color: #2980b9; font-weight: 700; margin-top: 40px; font-size: 24px; }
    .toc-box { background: #ffffff; border: 1px solid #e1e4e8; padding: 30px; margin: 40px 0; border-radius: 12px; display: inline-block; min-width: 60%; box-shadow: 0 8px 16px rgba(0,0,0,0.05); }
    .toc-box h3 { margin-top: 0; font-size: 22px; border-bottom: 2px solid #3498db; padding-bottom: 8px; display: inline-block; margin-bottom: 15px; color: #222; }
    .toc-box ul { list-style: none; padding: 0; margin: 0; }
    .toc-box li { margin-bottom: 12px; border-bottom: 1px dashed #f0f0f0; padding-bottom: 8px; padding-left: 20px; position: relative; }
    .toc-box li:before { content: "â–º"; color: #3498db; position: absolute; left: 0; font-size: 14px; top: 4px; }
    .toc-box a { color: #444; font-weight: 600; font-size: 18px; text-decoration: none; transition: 0.2s; }
    .toc-box a:hover { color: #3498db; padding-left: 8px; }
    .table-wrapper { overflow-x: auto; margin: 45px 0; border-radius: 12px; box-shadow: 0 5px 15px rgba(0,0,0,0.08); border: 1px solid #eee; }
    table { width: 100%; border-collapse: collapse; background: #fff; min-width: 600px; font-size: 18px; }
    th { background: #2c3e50; color: #fff; padding: 18px; text-align: left; text-transform: uppercase; font-size: 16px; letter-spacing: 1px; }
    td { padding: 16px 18px; border-bottom: 1px solid #eee; color: #34495e; }
    tr:nth-child(even) { background-color: #f8f9fa; }
    tr:hover { background-color: #e9ecef; transition: 0.3s; }
    .takeaways-box { background: linear-gradient(135deg, #fdfbf7 0%, #fff 100%); border-left: 6px solid #e67e22; padding: 30px; margin: 40px 0; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.02); }
    .takeaways-box h3 { margin-top: 0; color: #d35400; font-size: 22px; margin-bottom: 20px; display: flex; align-items: center; }
    .takeaways-box ul { margin: 0; padding-left: 25px; }
    .takeaways-box li { margin-bottom: 10px; font-weight: 600; font-size: 18px; color: #222; }
    .faq-section { margin-top: 70px; border-top: 3px solid #f1f1f1; padding-top: 50px; background: #fffcf5; padding: 40px; border-radius: 20px; }
    .faq-title { font-size: 30px; font-weight: 900; color: #222; margin-bottom: 35px; text-align: center; }
    .faq-q { font-weight: 700; font-size: 20px; color: #d35400; margin-bottom: 10px; display: block; }
    .faq-a { font-size: 19px; color: #555; line-height: 1.8; }
    .separator img { border-radius: 15px; box-shadow: 0 10px 40px rgba(0,0,0,0.12); max-width: 100%; height: auto; display: block; }
    blockquote { background: #ffffff; border-left: 5px solid #f1c40f; margin: 40px 0; padding: 25px 35px; font-style: italic; color: #555; font-size: 1.3em; line-height: 1.6; box-shadow: 0 3px 10px rgba(0,0,0,0.05); }
    a { color: #2980b9; text-decoration: none; font-weight: 600; border-bottom: 2px dotted #2980b9; transition: all 0.3s; }
    a:hover { color: #e67e22; border-bottom: 2px solid #e67e22; background-color: #fff8e1; }
</style>
"""

# ==============================================================================
# 2. PROMPTS (PASTE HERE)
# ==============================================================================

# ğŸ›‘ Ø§Ù„ØµÙ‚ Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨ØªØ§Øª Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ© "Beast Mode" Ù‡Ù†Ø§ ğŸ›‘
# (ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† PROMPT_B ÙŠØ·Ù„Ø¨ Ø§Ø³ØªØ®Ø¯Ø§Ù… Source Text)


# ==============================================================================
# 2. PROMPTS DEFINITIONS (v10.0 - THE "CREATOR" MODE)
# ==============================================================================

# ------------------------------------------------------------------
# PROMPT A: TOPIC SELECTION (Filter: "Is this clickable content?")
# ------------------------------------------------------------------
PROMPT_A_TRENDING = """
A: You are a Viral Content Strategist for a Tech Blog.
INPUT RSS DATA: {rss_data}
SECTION: {section_focus}
IGNORE: {recent_titles}

**YOUR GOAL:**
Ignore boring corporate news (stocks, quarterly earnings, lawsuits). 
Find the ONE story that a **YouTuber** or **TikToker** would make a video about today.

**SELECTION CRITERIA (The "WIIFM" Factor - What's In It For Me?):**
1. **Utility:** Does this solve a problem? (e.g., "New tool fixes bad wifi").
2. **Curiosity:** Is it weird or scary? (e.g., "AI can now fake your voice").
3. **Mass Appeal:** Does it affect phones/apps everyone uses? (WhatsApp, Instagram, iPhone, Android).

**OUTPUT JSON:**
{{
  "headline": "Create a 'How-to' or 'Explainer' style headline. (NOT a news headline). Ex: 'The New [Feature] is Here: How to Use It'",
  "original_rss_link": "URL",
  "original_rss_title": "Original Title",
  "topic_focus": "Practical application for the user",
  "why_selected": "High practical value for beginners",
  "date_context": "{today_date}"
}}
**CRITICAL OUTPUT RULES:**
1. Return PURE VALID JSON ONLY.
2. No Markdown (```json).
3. No conversational filler.
"""

# ------------------------------------------------------------------
# PROMPT B: CONTENT CREATOR (The "Friendly Expert")
# ------------------------------------------------------------------
PROMPT_B_TEMPLATE = """
B: You are 'LatestAI', a popular Tech Blogger and Creator.
**CONTEXT:** You are NOT a journalist. You are a guide. 
Use the `SOURCE CONTENT` facts, but rewrite them as a helpful blog post.

INPUT: {json_input}
FORBIDDEN WORDS: {forbidden_phrases} (Plus: "Reportedly", "According to sources", "Allegedly").

**STRATEGY:**
- **Tone:** Enthusiastic, Personal, Clear. Use "I", "We", "You".
- **Goal:** Take complex news and turn it into actionable advice.

**STRICT HTML STRUCTURE:**
1. **The Hook (Intro):** Start with a relatable problem. "Don't you hate it when...?" Then introduce the news as the solution.
2. **What Actually Happened:** `<h2>` Title. Simple explanation of the news (No jargon).
3. **The "Cheat Sheet":** 
   - `<h2>` titled "**Quick Summary**".
   - Under it: `<ul>` with 5 bullet points (Emojis allowed here).
4. **Navigation:** Insert: `[[TOC_PLACEHOLDER]]`.
5. **Why This Matters to YOU:** `<h2>` Title. 
   - Explain the *benefit*. Will it save time? Save money?
6. **The Tutorial / How-To (Crucial):** `<h2>` Title (e.g., "**How to Try It**" or "**What to Expect**").
   - Even if the feature isn't out, explain the steps to get ready.
7. **The Verdict:** `<h2>` titled "**My Opinion**". Is it hype or real?
8. **Comparison Table:** HTML `<table>` comparing "Old Way" vs "New Way".

**WRITING RULES:**
- **Short & Punchy:** Paragraphs = 2-3 lines max.
- **Visuals:** Use `<strong>` to highlight cool features.
- **Explain:** If you say "latency", write "(aka lag)".

Output JSON ONLY:
{{
  "draftTitle": "A Title That Sounds Like a YouTube Video (e.g. 'Stop Doing This...')",
  "draftContent": "<html>...Content...</html>",
  "excerpt": "Compelling summary for social media.",
  "sources_used": ["List"]
}}
**CRITICAL OUTPUT RULES:**
1. Return PURE VALID JSON ONLY.
2. **ESCAPE QUOTES:** You are writing HTML inside JSON. You MUST escape quotes.
   - WRONG: "content": "<div class="box">"
   - RIGHT: "content": "<div class=\\"box\\">"
3. No Markdown blocks.
"""

# ------------------------------------------------------------------
# PROMPT C: VISUALS & SEO (The "Magazine Editor")
# ------------------------------------------------------------------
PROMPT_C_TEMPLATE = """
C: Polish the content for a high-end blog.
INPUT: {json_input}
KG LINKS: {knowledge_graph}

**TASKS:**

1. **Format Injection:**
   - Replace `[[TOC_PLACEHOLDER]]` with `<div class="toc-box"><h3>Table of Contents</h3><ul>...</ul></div>`.
   - Add `id="sec-X"` to all H2 headers.

2. **Styling Wrappers (Match CSS):**
   - Wrap "Quick Summary" list in: `<div class="takeaways-box">...</div>`.
   - Wrap Table in: `<div class="table-wrapper">...</div>`.
   - **NEW:** Find the "My Opinion" or "Verdict" section and wrap it in: `<blockquote>...</blockquote>` to make it stand out.

3. **FAQ for Beginners:**
   - Add `<div class="faq-section">` at the end.
   - Questions must be basic: "Is it free?", "Is it safe?", "When can I get it?".

4. **Internal Linking:** 
   - Link to other topics naturally.

5. **Schema:** 
   - Use `Article` or `TechArticle` schema (better than NewsArticle for evergreen content).

Output JSON ONLY:
{{
  "finalTitle": "...",
  "finalContent": "<html>...</html>",
  "imageGenPrompt": "Minimalist 3D render of [Subject], abstract technology, soft studio lighting, pastel colors, high quality, --no text, --no faces",
  "imageOverlayText": "Simple Label (e.g. 'NEW UPDATE')",
  "seo": {{
      "metaTitle": "Clicky Title (60 chars)",
      "metaDescription": "Benefit-driven description (150 chars).",
      "tags": ["tag1", "tag2", "tag3"],
      "imageAltText": "Abstract representation of [Topic]"
  }},
  "schemaMarkup": {{ ... }}
}}
**CRITICAL OUTPUT RULES:**
1. Return PURE VALID JSON ONLY.
2. **ESCAPE QUOTES:** Ensure all HTML attributes use escaped quotes (\\").
3. Do NOT truncate content.
"""

# ------------------------------------------------------------------
# PROMPT D: HUMANIZER (The "Vibe Check")
# ------------------------------------------------------------------
PROMPT_D_TEMPLATE = """
PROMPT D â€” Final Polish
Input: {json_input}

**MISSION:** Kill the "Robot". Make it "Human".

**RULES:**
1. **Sentence Length:** If a sentence is too long (20+ words), split it.
2. **Vocabulary:** 
   - Change "Utilize" -> "Use".
   - Change "Facilitate" -> "Help".
   - Change "Furthermore" -> "Also".
   - Change "In conclusion" -> "The Bottom Line".
3. **Formatting:** Ensure the `takeaways-box` and `toc-box` classes are present.

Output JSON ONLY (Keep structure):
{{"finalTitle":"...", "finalContent":"...", "imageGenPrompt":"...", "imageOverlayText":"...", "seo": {{...}}, "schemaMarkup":{{...}}, "sources":[...], "excerpt":"..."}}
**CRITICAL OUTPUT RULES:**
1. Return PURE VALID JSON ONLY.
2. Maintain valid HTML escaping.
3. No Markdown.
"""

# ------------------------------------------------------------------
# PROMPT E: CLEANER
# ------------------------------------------------------------------
PROMPT_E_TEMPLATE = """
E: You are a JSON Formatter.
Input: {json_input}

Task: Output the exact same JSON, but ensure it is syntactically correct.
1. Escape all double quotes inside HTML attributes (e.g. `class=\\"box\\"`)
2. Do NOT change the content logic.

**CRITICAL OUTPUT RULES:**
1. Return RAW JSON STRING ONLY.
2. Remove any markdown fences (```).
3. Ensure no trailing commas.
"""
# ------------------------------------------------------------------
# SOCIAL: ENGAGEMENT FOCUSED
# ------------------------------------------------------------------

# ------------------------------------------------------------------
# SOCIAL: VIDEO SCRIPT (FIXED KEYS)
# ------------------------------------------------------------------
PROMPT_VIDEO_SCRIPT = """
Role: Scriptwriter for Viral Tech Shorts.
Input: "{title}" & "{text_summary}"

Task: Create a dialogue script between two characters.
Characters:
- "User" (Curious, asks questions).
- "Pro" (Expert, explains solution).

**CRITICAL JSON RULES:**
1. You MUST use exactly these keys: "speaker", "type", "text".
2. "type" must be either "send" (Right side) or "receive" (Left side).
3. "text" is the dialogue.

Example Output:
[
  {{"speaker": "User", "type": "receive", "text": "Wait, did you see this update?"}},
  {{"speaker": "Pro", "type": "send", "text": "Yes! It's actually insane. Here is why..."}}
]

**CRITICAL OUTPUT RULES:**
1. Return PURE VALID JSON ARRAY ONLY ( [ ... ] ).
2. No Markdown.
"""

PROMPT_YOUTUBE_METADATA = """
Role: YouTube Expert.
Input: {draft_title}

Output JSON: {{"title": "...", "description": "...", "tags": [...]}}

**CRITICAL OUTPUT RULES:**
1. Return PURE VALID JSON ONLY.
2. No Markdown.
"""

PROMPT_FACEBOOK_HOOK = """
Role: Community Manager.
Input: {title}

**Strategy:** 
- Start with a question.
- "Tag a friend who needs this".
- Keep it under 280 chars.

**CRITICAL OUTPUT RULES:**
1. Return PURE VALID JSON ONLY.
2. No Markdown.
"""


# ==============================================================================
# 3. HELPER UTILITIES
# ==============================================================================

class KeyManager:
    def __init__(self):
        self.keys = []
        for i in range(1, 11):
            k = os.getenv(f'GEMINI_API_KEY_{i}')
            if k: self.keys.append(k)
        if not self.keys:
            k = os.getenv('GEMINI_API_KEY')
            if k: self.keys.append(k)
        self.current_index = 0
        log(f"ğŸ”‘ Loaded {len(self.keys)} API Keys.")

    def get_current_key(self):
        if not self.keys: return None
        return self.keys[self.current_index]

    def switch_key(self):
        if self.current_index < len(self.keys) - 1:
            self.current_index += 1
            log(f"ğŸ”„ Switching Key #{self.current_index + 1}...")
            return True
        log("âŒ ALL KEYS EXHAUSTED.")  
        return False

key_manager = KeyManager()


# ==============================================================================
# UPDATED JSON UTILITIES (AUTO-REPAIR MODE)
# ==============================================================================

def clean_json(text):
    """
    Advanced cleaner: Removes Markdown, extracts the JSON array/object only.
    """
    if not text: return ""
    text = text.strip()
    
    # 1. ØªÙ†Ø¸ÙŠÙ ÙƒÙˆØ¯ Ø§Ù„Ù…Ø§Ø±ÙƒØ¯ÙˆØ§Ù†
    match = re.search(r'```(?:json)?\s*(.*?)\s*```', text, re.DOTALL)
    if match: text = match.group(1)
    
    # 2. Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£ÙˆÙ„ Ù‚ÙˆØ³ { Ø£Ùˆ [ ÙˆØ¢Ø®Ø± Ù‚ÙˆØ³ } Ø£Ùˆ ]
    # Ù‡Ø°Ø§ ÙŠØ²ÙŠÙ„ Ø£ÙŠ Ù…Ù‚Ø¯Ù…Ø§Øª Ù…Ø«Ù„ "Here is the JSON code:"
    start_brace = text.find('{')
    start_bracket = text.find('[')
    
    start_index = -1
    if start_brace != -1 and start_bracket != -1:
        start_index = min(start_brace, start_bracket)
    elif start_brace != -1:
        start_index = start_brace
    elif start_bracket != -1:
        start_index = start_bracket
        
    if start_index != -1:
        text = text[start_index:]
        
    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù†Ù‡Ø§ÙŠØ©
    end_brace = text.rfind('}')
    end_bracket = text.rfind(']')
    
    end_index = -1
    if end_brace != -1 and end_bracket != -1:
        end_index = max(end_brace, end_bracket)
    elif end_brace != -1:
        end_index = end_brace
    elif end_bracket != -1:
        end_index = end_bracket
        
    if end_index != -1:
        text = text[:end_index+1]
        
    return text.strip()

def try_parse_json(text, context=""):
    """
    Robust parser: Tries standard JSON, then Python Eval, then manual fixes.
    """
    if not text: return None
    
    # Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© 1: JSON Ù‚ÙŠØ§Ø³ÙŠ
    try:
        return json.loads(text)
    except:
        pass

    # Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© 2: Python Literal Eval
    # (Ù…ÙˆØ¯ÙŠÙ„Ø§Øª AI ØºØ§Ù„Ø¨Ø§Ù‹ ØªØ³ØªØ®Ø¯Ù… ' Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† " ÙˆÙ‡Ø°Ø§ ÙŠÙƒØ³Ø± JSON Ù„ÙƒÙ†Ù‡ Ù…Ù‚Ø¨ÙˆÙ„ ÙÙŠ Ø¨Ø§ÙŠØ«ÙˆÙ†)
    try:
        return ast.literal_eval(text)
    except:
        pass

    # Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© 3: Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ù‡Ø±ÙˆØ¨ (Escaping Fix) - Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥Ù†Ù‚Ø§Ø° HTML Ø§Ù„Ù…ÙƒØ³ÙˆØ±
    try:
        # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ø£Ø³Ø·Ø± Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© Ø¨Ù€ \n
        fixed_text = text.replace('\n', '\\n').replace('\r', '')
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø£Ø®ÙŠØ±Ø© Ø¨ÙˆØ¶Ø¹ strict=False
        return json.loads(fixed_text, strict=False)
    except:
        log(f"      âŒ JSON Parse Error ({context}). Input length: {len(text)}")
        # ÙÙŠ Ø­Ø§Ù„ Ø§Ù„ÙØ´Ù„ Ø§Ù„ØªØ§Ù…ØŒ Ù†Ø·Ø¨Ø¹ Ø¬Ø²Ø¡Ø§Ù‹ Ù…Ù† Ø§Ù„Ù†Øµ Ù„Ù†Ø¹Ø±Ù Ø§Ù„Ø³Ø¨Ø¨
        log(f"      ğŸ” Broken Snippet: {text[:100]}...")
        return None


def fetch_full_article(url):
    """
    ğŸš€ SCRAPER v11: 100% Local (Selenium + Trafilatura).
    No 3rd party APIs like Jina. High success rate.
    """
    # 1. Ø¬Ù„Ø¨ Ø§Ù„Ù€ HTML ÙˆØ§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Selenium
    data = url_resolver.get_page_html(url)
    
    if not data or not data.get('html'):
        log(f"      âš ï¸ Selenium failed to get page source.")
        return None
        
    real_url = data['url']
    html_content = data['html']
    
    log(f"      ğŸ§© Extracting content locally from: {real_url[:50]}...")
    
    try:
        # 2. Ø§Ø³ØªØ®Ø¯Ø§Ù… Trafilatura Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Øµ Ø§Ù„Ù…Ù‚Ø§Ù„Ø© Ù…Ù† Ø§Ù„Ù€ HTML
        # include_comments=False: Ù„Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª
        # include_tables=True: Ù„Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ù…Ù‡Ù…Ø©
        extracted_text = trafilatura.extract(
            html_content, 
            include_comments=False, 
            include_tables=True,
            favor_precision=True # Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø¯Ù‚Ø© Ø§Ù„Ù†Øµ ÙˆÙ„ÙŠØ³ ÙƒØ«Ø±ØªÙ‡
        )
        
        if extracted_text and len(extracted_text) > 500:
            log(f"      âœ… Extraction Success! {len(extracted_text)} chars found.")
            return extracted_text[:12000]
        else:
            log("      âš ï¸ Trafilatura found very little text. Trying fallback...")
            
            # Fallback: Ù…Ø­Ø§ÙˆÙ„Ø© Ø¨Ø³ÙŠØ·Ø© ÙÙŠ Ø­Ø§Ù„ ÙØ´Ù„ Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…ØªØ®ØµØµØ©
            soup = BeautifulSoup(html_content, 'html.parser')
            # Ø­Ø°Ù Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ø²Ø¹Ø¬Ø© ÙŠØ¯ÙˆÙŠØ§Ù‹
            for script in soup(["script", "style", "nav", "footer", "header", "aside"]):
                script.extract()
            text = soup.get_text(separator='\n')
            
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„ÙØ±Ø§ØºØ§Øª
            lines = [line.strip() for line in text.splitlines() if line.strip()]
            clean_text = '\n'.join(lines)
            
            if len(clean_text) > 500:
                log(f"      âœ… Fallback Success (BS4): {len(clean_text)} chars.")
                return clean_text[:12000]

    except Exception as e:
        log(f"      âŒ Extraction Error: {e}")
        
    return None


def get_real_news_rss(query_keywords, category):
    try:
        if "," in query_keywords:
            topics = [t.strip() for t in query_keywords.split(',') if t.strip()]
            focused = random.choice(topics)
            log(f"   ğŸ¯ Targeted Search: '{focused}'")
            full_query = f"{focused} when:1d"
        else:
            full_query = f"{query_keywords} when:1d"

        encoded = urllib.parse.quote(full_query)
        url = f"https://news.google.com/rss/search?q={encoded}&hl=en-US&gl=US&ceid=US:en"
        
        feed = feedparser.parse(url)
        items = []
        if feed.entries:
            for entry in feed.entries[:8]:
                pub = entry.published if 'published' in entry else "Today"
                title_clean = entry.title.split(' - ')[0]
                items.append({"title": title_clean, "link": entry.link, "date": pub})
            return items 
        else:
            log(f"   âš ï¸ RSS Empty. Fallback.")
            fb = f"{category} news when:1d"
            url = f"https://news.google.com/rss/search?q={urllib.parse.quote(fb)}&hl=en-US&gl=US&ceid=US:en"
            feed = feedparser.parse(url)
            for entry in feed.entries[:5]:
                items.append({"title": entry.title, "link": entry.link, "date": "Today"})
            return items
            
    except Exception as e:
        log(f"âŒ RSS Error: {e}")
        return []

def get_blogger_token():
    payload = {
        'client_id': os.getenv('BLOGGER_CLIENT_ID'),
        'client_secret': os.getenv('BLOGGER_CLIENT_SECRET'),
        'refresh_token': os.getenv('BLOGGER_REFRESH_TOKEN'),
        'grant_type': 'refresh_token'
    }
    try:
        r = requests.post('https://oauth2.googleapis.com/token', data=payload)
        return r.json().get('access_token') if r.status_code == 200 else None
    except: return None

def publish_post(title, content, labels):
    token = get_blogger_token()
    if not token: return None
    
    url = f"https://www.googleapis.com/blogger/v3/blogs/{os.getenv('BLOGGER_BLOG_ID')}/posts?isDraft=false"
    headers = {"Authorization": f"Bearer {token}", "Content-Type": "application/json"}
    body = {"title": title, "content": content, "labels": labels}
    
    try:
        r = requests.post(url, headers=headers, json=body)
        if r.status_code == 200:
            link = r.json().get('url')
            log(f"âœ… Published LIVE: {link}")
            return link
        log(f"âŒ Blogger Error: {r.text}")
        return None
    except Exception as e:
        log(f"âŒ Connection Fail: {e}")
        return None

def generate_and_upload_image(prompt_text, overlay_text=""):
    key = os.getenv('IMGBB_API_KEY')
    if not key: return None
    log(f"   ğŸ¨ Flux Image Gen...")
    for i in range(3):
        try:
            safe = urllib.parse.quote(f"{prompt_text}, abstract tech, 8k, --no text")
            txt = f"&text={urllib.parse.quote(overlay_text)}&font=roboto&fontsize=48&color=white" if overlay_text else ""
            url = f"https://image.pollinations.ai/prompt/{safe}?width=1280&height=720&model=flux&nologo=true&seed={random.randint(1,999)}{txt}"
            r = requests.get(url, timeout=60)
            if r.status_code == 200:
                res = requests.post("https://api.imgbb.com/1/upload", data={"key":key}, files={"image":r.content}, timeout=60)
                if res.status_code == 200: return res.json()['data']['url']
        except: time.sleep(3)
    return None

def load_kg():
    try:
        if os.path.exists('knowledge_graph.json'): return json.load(open('knowledge_graph.json','r'))
    except: pass
    return []

def get_recent_titles_string(limit=50):
    kg = load_kg()
    if not kg: return "None"
    return ", ".join([i.get('title','') for i in kg[-limit:]])

def get_relevant_kg_for_linking(category, limit=60):
    kg = load_kg()
    links = [{"title":i['title'],"url":i['url']} for i in kg if i.get('section')==category][:limit]
    return json.dumps(links)

def update_kg(title, url, section):
    try:
        data = load_kg()
        for i in data:
            if i['url']==url: return
        data.append({"title":title, "url":url, "section":section, "date":str(datetime.date.today())})
        with open('knowledge_graph.json','w') as f: json.dump(data, f, indent=2)
    except: pass

def perform_maintenance_cleanup():
    try:
        if os.path.exists('knowledge_graph.json'):
            with open('knowledge_graph.json','r') as f: d=json.load(f)
            if len(d)>800: json.dump(d[-400:], open('knowledge_graph.json','w'), indent=2)
    except: pass

def generate_step(model, prompt, step):
    log(f"   ğŸ‘‰ Generating: {step}")
    while True:
        key = key_manager.get_current_key()
        if not key: 
            log("âŒ FATAL: Keys exhausted.")
            return None
        client = genai.Client(api_key=key)
        try:
            r = client.models.generate_content(
                model=model, contents=prompt, config=types.GenerateContentConfig(response_mime_type="application/json")
            )
            return clean_json(r.text)
        except Exception as e:
            if "429" in str(e) or "quota" in str(e).lower():
                if not key_manager.switch_key(): return None
            else: return None
# ==============================================================================
# 4. ADVANCED SCRAPING (UPDATED FOR HIGH QUALITY & LOGGING)
# ==============================================================================
def resolve_and_scrape(google_url):
    """
    Open Google URL -> Resolve -> Get Page Source -> Extract Text.
    Returns: (final_url, page_title, text_content)
    """
    log(f"      ğŸ•µï¸â€â™‚ï¸ Selenium: Opening & Resolving: {google_url[:60]}...")
    
    # Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ù…ØªØµÙØ­
    chrome_options = Options()
    chrome_options.add_argument("--headless=new")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    # Ø§Ù†ØªØ­Ø§Ù„ Ø´Ø®ØµÙŠØ© Ù…ØªØµÙØ­ Ø­Ù‚ÙŠÙ‚ÙŠ Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø­Ø¸Ø±
    chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
    chrome_options.add_argument("--mute-audio") # ÙƒØªÙ… Ø§Ù„ØµÙˆØª Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„ØªØ­Ù…ÙŠÙ„

    driver = None
    try:
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        driver.set_page_load_timeout(25) # Ù…Ù‡Ù„Ø© ØªØ­Ù…ÙŠÙ„ 25 Ø«Ø§Ù†ÙŠØ©
        
        driver.get(google_url)
        
        # Ø­Ù„Ù‚Ø© Ø§Ù†ØªØ¸Ø§Ø± Ù„Ù„Ø®Ø±ÙˆØ¬ Ù…Ù† Ø¬ÙˆØ¬Ù„
        start_wait = time.time()
        final_url = google_url
        
        while time.time() - start_wait < 15: # Ø§Ù†ØªØ¸Ø§Ø± 15 Ø«Ø§Ù†ÙŠØ© ÙƒØ­Ø¯ Ø£Ù‚ØµÙ‰ Ù„Ù„ØªØ­ÙˆÙŠÙ„
            current = driver.current_url
            if "news.google.com" not in current and "google.com" not in current:
                final_url = current
                break
            time.sleep(1) # ÙØ­Øµ ÙƒÙ„ Ø«Ø§Ù†ÙŠØ©
        
        # Ø§Ù„ØªÙ‚Ø§Ø· Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ Ù„Ù„ØµÙØ­Ø©
        final_title = driver.title
        page_source = driver.page_source
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø±ÙˆØ§Ø¨Ø· ØºÙŠØ± Ø§Ù„Ù…Ø±ØºÙˆØ¨Ø© (ÙÙŠØ¯ÙŠÙˆØŒ Ù…Ø¹Ø§Ø±Ø¶ ØµÙˆØ±)
        # Ù‡Ø°Ø§ ÙŠÙ…Ù†Ø¹ Ù…Ø´ÙƒÙ„Ø© "Washington Post Video" Ø§Ù„ØªÙŠ ÙˆØ§Ø¬Ù‡ØªÙ‡Ø§
        bad_segments = ["/video/", "/watch", "/gallery/", "/photos/", "youtube.com"]
        if any(seg in final_url.lower() for seg in bad_segments):
            log(f"      âš ï¸ Skipped Video/Gallery URL: {final_url}")
            return None, None, None

        log(f"      ğŸ”— Resolved URL: {final_url[:70]}...")
        log(f"      ğŸ·ï¸ Real Page Title: {final_title[:70]}...")

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Trafilatura
        extracted_text = trafilatura.extract(
            page_source, 
            include_comments=False, 
            include_tables=True,
            favor_precision=True
        )
        
        if extracted_text and len(extracted_text) > 1000:
            return final_url, final_title, extracted_text

        # Fallback (BS4) Ø¥Ø°Ø§ ÙØ´Ù„ Trafilatura
        soup = BeautifulSoup(page_source, 'html.parser')
        for script in soup(["script", "style", "nav", "footer", "header", "aside", "noscript"]):
            script.extract()
        fallback_text = soup.get_text(" ", strip=True)
        
        return final_url, final_title, fallback_text

    except Exception as e:
        log(f"      âŒ Selenium Error: {e}")
        return None, None, None
    finally:
        if driver:
            driver.quit()


# ==============================================================================
# 5. CORE PIPELINE LOGIC (OFFLINE DECODER + JINA)
# ==============================================================================


def run_pipeline(category, config, mode="trending"):
    model = config['settings'].get('model_name')
    cat_conf = config['categories'][category]
    
    log(f"\nğŸš€ INIT PIPELINE: {category} (High Quality Mode)")
    
    # ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±
    recent_titles = get_recent_titles_string(limit=60)
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø§Ù„Ø¨Ø­Ø« (Ù„ØªØºÙŠÙŠØ± Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ Ù…Ù‚Ø§Ù„Ø§Øª)
    max_global_retries = 6 
    article_found = False
    
    selected_item = None
    source_content = None
    final_resolved_url = None
    
    # ğŸ”„ Ø§Ù„Ø­Ù„Ù‚Ø© Ø§Ù„ÙƒØ¨Ø±Ù‰: ØªØ¯ÙˆØ± ÙˆØªØºÙŠØ± Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø­ØªÙ‰ ØªØ¬Ø¯ Ù…Ù‚Ø§Ù„Ø§Ù‹ Ù…Ù†Ø§Ø³Ø¨Ø§Ù‹
    for attempt in range(max_global_retries):
        if article_found: break
        
        log(f"\nğŸ” Attempt #{attempt+1} of {max_global_retries}")
        
        # 1. Ø§Ø®ØªÙŠØ§Ø± Ù…ÙˆØ¶ÙˆØ¹ Ø¹Ø´ÙˆØ§Ø¦ÙŠ (Keyword Rotation)
        query_keywords = cat_conf.get('trending_focus', category)
        current_topic = category # Default
        
        if "," in query_keywords:
            topics = [t.strip() for t in query_keywords.split(',') if t.strip()]
            # Ø§Ø®ØªÙŠØ§Ø± Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ù„Ø¶Ù…Ø§Ù† Ø§Ù„ØªÙ†ÙˆØ¹ ÙÙŠ ÙƒÙ„ Ù…Ø­Ø§ÙˆÙ„Ø©
            current_topic = random.choice(topics)
            log(f"   ğŸ¯ Targeted Search: '{current_topic}'")
            # Ù†Ø¶ÙŠÙ 'when:1d' Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„ÙŠÙˆÙ…ØŒ Ø£Ùˆ 'when:3d' Ø¥Ø°Ø§ Ø£Ø±Ø¯Øª Ù†Ø·Ø§Ù‚ Ø£ÙˆØ³Ø¹
            rss_query = f"{current_topic} when:2d" 
        else:
            rss_query = f"{query_keywords} when:2d"

        # 2. Ø¬Ù„Ø¨ RSS
        rss_items = get_real_news_rss(rss_query.replace("when:2d","").strip(), category)
        
        if not rss_items:
            log("   âš ï¸ No RSS items found for this keyword. Retrying next...")
            continue
            
        # 3. Ø­Ù„Ù‚Ø© ÙØ­Øµ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø¯Ø§Ø®Ù„ RSS
        for item in rss_items:
            # ÙÙ„ØªØ±Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±
            if item['title'][:20] in recent_titles: 
                log(f"   â­ï¸ Skipped duplicate: {item['title'][:30]}")
                continue
            
            log(f"   ğŸ“Œ Checking RSS Item: {item['title'][:50]}...")
            
            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„ÙÙƒ ÙˆØ§Ù„Ø¬Ù„Ø¨
            r_url, r_title, text = resolve_and_scrape(item['link'])
            
            # --- Ø´Ø±Ø· Ø§Ù„Ø¬ÙˆØ¯Ø© (The Quality Check) ---
            # 1. ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙˆØ¬Ø¯ Ù†Øµ
            # 2. ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ù†Øµ Ø£Ø·ÙˆÙ„ Ù…Ù† 1500 Ø­Ø±Ù (Ù„Ø¶Ù…Ø§Ù† Ø£Ù†Ù‡ Ù…Ù‚Ø§Ù„ ÙƒØ§Ù…Ù„ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ Ø£Ùˆ Ø®Ø¨Ø± Ù‚ØµÙŠØ±)
            if text and len(text) >= 1500:
                log(f"      âœ… High Quality Content Found! ({len(text)} chars)")
                log(f"      âš–ï¸  Match Check:\n          RSS: {item['title']}\n          Pg : {r_title}")
                
                selected_item = item
                selected_item['link'] = r_url # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø±Ø§Ø¨Ø· Ø¨Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ
                selected_item['real_title'] = r_title # Ø­ÙØ¸ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ù„Ø§Ø­Ù‚Ø§Ù‹
                source_content = text
                article_found = True
                break # Ù†Ø®Ø±Ø¬ Ù…Ù† Ø­Ù„Ù‚Ø© Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª
            else:
                current_len = len(text) if text else 0
                log(f"      âš ï¸ Content too short ({current_len} chars) or failed. Looking for better article...")
                time.sleep(2) # Ø±Ø§Ø­Ø© Ù‚ØµÙŠØ±Ø©
        
        if not article_found:
            log("   âš ï¸ None of the RSS items met the quality standards. Switching topic...")
            time.sleep(3)

    # 4. Ø¥Ø°Ø§ Ø§Ù†ØªÙ‡Øª ÙƒÙ„ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§Øª ÙˆÙ„Ù… Ù†Ø¬Ø¯ Ø´ÙŠØ¦Ø§Ù‹
    if not selected_item or not source_content:
        log("âŒ FATAL: Could not find ANY high-quality article after multiple attempts.")
        log("âŒ Skipping this run to preserve quality.")
        return

    # =======================================================
    # B. DRAFTING PHASE (Now we are sure we have good content)
    # =======================================================
    log(f"\nâœï¸ Starting Writing Process for: {selected_item['title']}")
    
    # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù€ Payload
    json_ctx = {
        "rss_headline": selected_item['title'],
        "resolved_headline": selected_item.get('real_title', ''),
        "original_link": selected_item['link'],
        "date": selected_item['date']
    }
    
    # Ø¯Ù…Ø¬ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†ÙŠÙ† Ù„Ù„Ù…ÙˆØ¯ÙŠÙ„ Ù„ÙŠÙÙ‡Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ Ø£ÙØ¶Ù„
    prefix = "*** FULL SOURCE TEXT (High Quality Scrape) ***"
    payload = f"METADATA: {json.dumps(json_ctx)}\n\n{prefix}\n\n{source_content}"
    
    # Step B
    json_b = try_parse_json(generate_step(model, PROMPT_B_TEMPLATE.format(json_input=payload, forbidden_phrases=str(FORBIDDEN_PHRASES)), "Step B"), "B")
    if not json_b: return

    # Step C (SEO)
    kg_links = get_relevant_kg_for_linking(category)
    prompt_c = PROMPT_C_TEMPLATE.format(json_input=json.dumps(json_b), knowledge_graph=kg_links)
    json_c = try_parse_json(generate_step(model, prompt_c, "Step C"), "C")
    if not json_c: return

    # Step D (Audit)
    prompt_d = PROMPT_D_TEMPLATE.format(json_input=json.dumps(json_c))
    json_d = try_parse_json(generate_step(model, prompt_d, "Step D"), "D")
    if not json_d: return

    # Step E (Publish)
    prompt_e = PROMPT_E_TEMPLATE.format(json_input=json.dumps(json_d))
    final = try_parse_json(generate_step(model, prompt_e, "Step E"), "E")
    if not final: final = json_d 

    title = final.get('finalTitle', selected_item['title'])
    
    # =======================================================
    # SOCIALS & VIDEO
    # =======================================================
    log("   ğŸ§  Socials...")
    yt_meta = try_parse_json(generate_step(model, PROMPT_YOUTUBE_METADATA.format(draft_title=title), "YT Meta"))
    # ØªØ£Ù…ÙŠÙ† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
    if not yt_meta: yt_meta = {"title": title, "description": f"Read full story: {selected_item['link']}", "tags": []}
    
    fb_dat = try_parse_json(generate_step(model, PROMPT_FACEBOOK_HOOK.format(title=title, category=category), "FB Hook"))
    fb_cap = fb_dat.get('facebook', title) if fb_dat else title

    # Video Gen
    vid_html, vid_main, vid_short, fb_path = "", None, None, None
    try:
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù„Ø®Øµ Ù†ØµÙŠ Ù†Ø¸ÙŠÙ Ù„Ù„Ø³ÙƒØ±Ø¨Øª
        summ = re.sub('<[^<]+?>','', final.get('finalContent',''))[:2000]
        
        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØªÙ…Ø±ÙŠØ± Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„ Ù„ØªÙˆÙ„ÙŠØ¯ Ø³ÙƒØ±ÙŠØ¨Øª Ø¬ÙŠØ¯
        script = try_parse_json(generate_step(model, PROMPT_VIDEO_SCRIPT.format(title=title, text_summary=summ), "Script"))
        
        if script:
            os.makedirs("output", exist_ok=True)
            rr = video_renderer.VideoRenderer()
            # ÙÙŠØ¯ÙŠÙˆ Ø¹Ø±Ø¶ÙŠ Ù„Ù„ÙŠÙˆØªÙŠÙˆØ¨
            pm = rr.render_video(script, title, f"output/main_{int(time.time())}.mp4")
            if pm:
                desc = f"{yt_meta.get('description','')}\n\nğŸš€ Article Link Coming Soon.\n\n#{category.replace(' ','')} #AI"
                vid_main, _ = youtube_manager.upload_video_to_youtube(pm, yt_meta.get('title',title)[:100], desc, yt_meta.get('tags',[]))
                if vid_main:
                    vid_html = f'<div class="video-container" style="position:relative;padding-bottom:56.25%;margin:35px 0;border-radius:12px;overflow:hidden;box-shadow:0 4px 12px rgba(0,0,0,0.1);"><iframe style="position:absolute;top:0;left:0;width:100%;height:100%;" src="https://www.youtube.com/embed/{vid_main}" frameborder="0" allowfullscreen></iframe></div>'
            
            # ÙÙŠØ¯ÙŠÙˆ Ø·ÙˆÙ„ÙŠ (Reels/Shorts)
            rs = video_renderer.VideoRenderer(width=1080, height=1920)
            ps = rs.render_video(script, title, f"output/short_{int(time.time())}.mp4")
            if ps:
                fb_path = ps
                vid_short, _ = youtube_manager.upload_video_to_youtube(ps, f"{yt_meta.get('title',title)[:90]} #Shorts", desc, yt_meta.get('tags',[])+['shorts'])
    except Exception as e: log(f"âš ï¸ Video Error: {e}")

    # =======================================================
    # PUBLISHING
    # =======================================================
    body = ARTICLE_STYLE
    img = generate_and_upload_image(final.get('imageGenPrompt', title), final.get('imageOverlayText', 'News'))
    if img: body += f'<div class="separator" style="clear:both;text-align:center;margin-bottom:30px;"><a href="{img}"><img src="{img}" alt="{final.get("seo",{}).get("imageAltText","News")}" /></a></div>'
    
    if vid_html: body += vid_html
    body += final.get('finalContent', '')
    
    if 'schemaMarkup' in final:
        try: body += f'\n<script type="application/ld+json">\n{json.dumps(final["schemaMarkup"])}\n</script>'
        except: pass
    
    url = publish_post(title, body, [category])
    
    if url:
        update_kg(title, url, category)
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„ÙˆØµÙ Ø¨Ø§Ù„Ø±Ø§Ø¨Ø·
        upd_desc = f"{yt_meta.get('description','')}\n\nğŸ‘‡ FULL STORY:\n{url}\n\n#AI #TechNews"
        if vid_main: youtube_manager.update_video_description(vid_main, upd_desc)
        if vid_short: youtube_manager.update_video_description(vid_short, upd_desc)
        
        try:
            if fb_path: 
                # Ù†Ø´Ø± Ø§Ù„Ø±ÙŠÙ„Ø² Ù…Ø¹ Ø§Ù„Ù‡Ø§Ø´ØªØ§ØºØ§Øª ÙˆØ§Ù„Ø±Ø§Ø¨Ø·
                fb_full_text = f"{fb_cap}\n\nRead more: {url}\n\n#AI #Technology #{category.replace(' ','')}"
                social_manager.post_reel_to_facebook(fb_path, fb_full_text)
                time.sleep(15)
            elif img:
                social_manager.distribute_content(f"{fb_cap}\n\nğŸ‘‡ Read Article:\n{url}", url, img)
        except Exception as e: log(f"Social Post Error: {e}")

# ==============================================================================
# 7. MAIN
# ==============================================================================

def main():
    try:
        with open('config_advanced.json','r') as f: cfg = json.load(f)
    except:
        log("âŒ No Config.")
        return
    
    cat = random.choice(list(cfg['categories'].keys()))
    run_pipeline(cat, cfg, mode="trending")
    perform_maintenance_cleanup()
    log("âœ… Finished.")

if __name__ == "__main__":
    main()
