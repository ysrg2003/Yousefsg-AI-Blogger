import os
import json
import time
import requests
import re
import random
import sys
import datetime
import urllib.parse
import base64
import feedparser
from bs4 import BeautifulSoup
import social_manager
import video_renderer
import youtube_manager
from google import genai
from google.genai import types
import selenium
import webdriver_manager
# ---- Ø£Ø¶Ù Ù‡Ø°Ù‡ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª Ø¨Ø§Ù„Ù‚Ø±Ø¨ Ù…Ù† Ø£Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù„Ù ----
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
# webdriver-manager Ø³ÙŠØ³Ù…Ø­ Ø¨ØªÙ†Ø²ÙŠÙ„ ChromeDriver ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
from webdriver_manager.chrome import ChromeDriverManager
# (Ù‚Ø¯ ØªØ­ØªØ§Ø¬ Ø£ÙŠØ¶Ø§Ù‹ Ù‡Ø°Ù‡ Ù„Ùˆ ÙƒÙ†Øª ØªØ³ØªØ®Ø¯Ù… find_element ÙˆØºÙŠØ±Ù‡Ø§)
from selenium.webdriver.common.by import By
# -------------------------------------------------
import url_resolver
import trafilatura
import ast
import json_repair # ÙŠØ¬Ø¨ ØªØ«Ø¨ÙŠØªÙ‡Ø§: pip install json_repair
import regex # ÙŠØ¬Ø¨ ØªØ«Ø¨ÙŠØªÙ‡Ø§: pip install regex
import pydantic
from tenacity import (
    retry, 
    stop_after_attempt, 
    wait_exponential, 
    retry_if_exception_type, 
    before_sleep_log
)
from PIL import Image, ImageDraw, ImageFont, ImageOps
from io import BytesIO
from prompts import *

# ==============================================================================
# 0. CONFIG & LOGGING
# ==============================================================================

FORBIDDEN_PHRASES = [
    "In today's digital age",
    "The world of AI is ever-evolving",
    "unveils",
    "unveiled",
    "poised to",
    "delve into",
    "game-changer",
    "paradigm shift",
    "tapestry",
    "robust",
    "leverage",
    "underscore",
    "testament to",
    "beacon of",
    "In conclusion",
    "Remember that",
    "It is important to note",
    "Imagine a world",
    "fast-paced world",
    "cutting-edge",
    "realm of"
]

BORING_KEYWORDS = [
    "CFO", "CEO", "Quarterly", "Earnings", "Report", "Market Cap", 
    "Dividend", "Shareholders", "Acquisition", "Merger", "Appointment", 
    "Executive", "Knorex", "Partner", "Agreement", "B2B", "Enterprise"
]

def log(msg):
    timestamp = datetime.datetime.now().strftime("%H:%M:%S")
    print(f"[{timestamp}] {msg}", flush=True)

# ==============================================================================
# 1. CSS STYLING (UPDATED v14.1 - Fixed Layout & TOC)
# ==============================================================================
ARTICLE_STYLE = """
<style>
    /* Global Settings */
    .post-body { 
        font-family: 'Segoe UI', Roboto, Helvetica, Arial, sans-serif; 
        line-height: 1.8; 
        color: #333; 
        font-size: 18px; 
        max-width: 100%;
        overflow-x: hidden; /* Prevents horizontal scroll */
    }
    
    /* Headers */
    h2 { 
        color: #111; 
        font-weight: 800; 
        margin-top: 60px; 
        margin-bottom: 25px; 
        border-bottom: 3px solid #f1c40f; 
        padding-bottom: 10px; 
        font-size: 28px; 
        clear: both; /* Fixes overlap issue */
        display: block; /* Ensures it takes full width */
    }
    h3 { 
        color: #2980b9; 
        font-weight: 700; 
        margin-top: 40px; 
        margin-bottom: 20px;
        font-size: 24px; 
        clear: both;
    }
    
    /* Table of Contents (Fixed) */
    .toc-box { 
        background: #fdfdfd; 
        border: 1px solid #e1e4e8; 
        padding: 25px; 
        margin: 30px 0 50px 0; /* Extra bottom margin to prevent overlap */
        border-radius: 12px; 
        display: block; /* Changed from inline-block to block */
        width: 100%; /* Full width to avoid wrapping issues */
        box-sizing: border-box;
        box-shadow: 0 4px 12px rgba(0,0,0,0.05); 
    }
    .toc-box h3 { 
        margin-top: 0; 
        font-size: 22px; 
        border-bottom: 2px solid #3498db; 
        padding-bottom: 10px; 
        margin-bottom: 20px; 
        color: #2c3e50;
        display: inline-block;
    }
    .toc-box ul { 
        list-style: none !important; 
        padding: 0 !important; 
        margin: 0 !important; 
    }
    .toc-box li { 
        margin-bottom: 12px; 
        border-bottom: 1px dashed #eee; 
        padding-bottom: 8px; 
        padding-left: 0; /* Reset padding */
        position: relative; 
    }
    .toc-box a { 
        color: #444; 
        font-weight: 600; 
        font-size: 18px; 
        text-decoration: none; 
        transition: 0.2s; 
        display: flex; /* Aligns icon and text */
        align-items: center;
        border: none;
    }
    .toc-box a:before { 
        content: "ğŸ‘‰"; /* Simple emoji icon */
        margin-right: 10px; 
        font-size: 16px; 
    }
    .toc-box a:hover { 
        color: #3498db; 
        padding-left: 5px; 
        background: none;
    }

    /* Takeaways Box */
    .takeaways-box { 
        background: linear-gradient(135deg, #fffcf5 0%, #fff 100%); 
        border-left: 6px solid #e67e22; 
        padding: 25px; 
        margin: 40px 0; 
        border-radius: 8px; 
        box-shadow: 0 4px 6px rgba(0,0,0,0.03); 
    }
    
    /* Tables */
    .table-wrapper { 
        overflow-x: auto; 
        margin: 40px 0; 
        border-radius: 8px; 
        border: 1px solid #eee; 
    }
    table { 
        width: 100%; 
        border-collapse: collapse; 
        background: #fff; 
        font-size: 17px; 
    }
    th { 
        background: #2c3e50; 
        color: #fff; 
        padding: 15px; 
        text-align: left; 
    }
    td { 
        padding: 15px; 
        border-bottom: 1px solid #eee; 
        color: #444; 
    }

    /* Quote/Verdict */
    blockquote { 
        background: #f8f9fa; 
        border-left: 5px solid #27ae60; 
        margin: 40px 0; 
        padding: 20px 30px; 
        font-style: italic; 
        color: #555; 
        font-size: 1.2em; 
    }

    /* Links */
    a { 
        color: #2980b9; 
        text-decoration: none; 
        font-weight: 600; 
        border-bottom: 2px dotted #2980b9; 
        transition: all 0.3s; 
    }
    a:hover { 
        color: #e67e22; 
        border-bottom: 2px solid #e67e22; 
    }
    
    /* FAQ */
    .faq-section { 
        margin-top: 60px; 
        background: #fdfdfd; 
        padding: 30px; 
        border-radius: 15px; 
        border: 1px solid #eee; 
    }
    .faq-q { color: #d35400; font-weight: bold; font-size: 20px; display: block; margin-bottom: 10px; }
    
    /* Sources */
    .Sources { font-size: 0.9em; color: #777; margin-top: 50px; border-top: 1px solid #eee; padding-top: 20px; }
    .Sources ul { list-style-type: disc; padding-left: 20px; }
</style>
"""

# ==============================================================================
# 2. PROMPTS (PASTE HERE)
# ==============================================================================

# ğŸ›‘ ØªØ§ÙƒØ¯ Ù…Ù† ÙƒØªØ§Ø¨Ø© from prompts import *"Beast Mode" Ù‡Ù†Ø§ ğŸ›‘
# (ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† PROMPT_B ÙŠØ·Ù„Ø¨ Ø§Ø³ØªØ®Ø¯Ø§Ù… Source Text)

# ==============================================================================
# 3. HELPER UTILITIES
# ==============================================================================

class KeyManager:
    def __init__(self):
        self.keys = []
        for i in range(1, 11):
            k = os.getenv(f'GEMINI_API_KEY_{i}')
            if k: self.keys.append(k)
        if not self.keys:
            k = os.getenv('GEMINI_API_KEY')
            if k: self.keys.append(k)
        self.current_index = 0
        log(f"ğŸ”‘ Loaded {len(self.keys)} API Keys.")

    def get_current_key(self):
        if not self.keys: return None
        return self.keys[self.current_index]

    def switch_key(self):
        if self.current_index < len(self.keys) - 1:
            self.current_index += 1
            log(f"ğŸ”„ Switching Key #{self.current_index + 1}...")
            return True
        log("âŒ ALL KEYS EXHAUSTED.")  
        return False

key_manager = KeyManager()


# ==============================================================================
# UPDATED JSON UTILITIES (AUTO-REPAIR MODE)
# ==============================================================================


# ==============================================================================
# 5. ADVANCED AI ENGINE: THE "UNBREAKABLE" PIPELINE
# ==============================================================================
import logging
import json
import json_repair  # pip install json_repair
import regex        # pip install regex
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
    before_sleep_log
)
from google import genai
from google.genai import types

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù„ÙˆØ¬Ø± Ø§Ù„Ø®Ø§Øµ Ø¨Ù…ÙƒØªØ¨Ø© Tenacity Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§Øª ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©
logger = logging.getLogger("RetryEngine")
logger.setLevel(logging.INFO)

# ==============================================================================
# A. CUSTOM EXCEPTIONS & STRICT INSTRUCTIONS
# ==============================================================================

class JSONValidationError(Exception):
    """ÙŠÙØ«Ø§Ø± Ù‡Ø°Ø§ Ø§Ù„Ø®Ø·Ø£ Ø¹Ù†Ø¯Ù…Ø§ ÙŠÙƒÙˆÙ† Ø§Ù„Ù€ JSON ØµØ§Ù„Ø­Ø§Ù‹ Ù†Ø­ÙˆÙŠØ§Ù‹ ÙˆÙ„ÙƒÙ† ØªÙ†Ù‚ØµÙ‡ Ù…ÙØ§ØªÙŠØ­ Ø£Ø³Ø§Ø³ÙŠØ©."""
    pass

class JSONParsingError(Exception):
    """ÙŠÙØ«Ø§Ø± Ù‡Ø°Ø§ Ø§Ù„Ø®Ø·Ø£ Ø¹Ù†Ø¯Ù…Ø§ ÙŠÙØ´Ù„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ JSON ØªÙ…Ø§Ù…Ø§Ù‹ Ø­ØªÙ‰ Ø¨Ø¹Ø¯ Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø§Ù„Ø¥ØµÙ„Ø§Ø­."""
    pass

# Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª Ø§Ù„ØµØ§Ø±Ù… Ø§Ù„Ø°ÙŠ ÙŠØ¬Ø¨Ø± Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØµÙ…Øª ÙˆØ§Ù„Ø§Ù„ØªØ²Ø§Ù… Ø¨Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ ÙÙ‚Ø·
STRICT_SYSTEM_PROMPT = """
You are an assistant that MUST return ONLY the exact output requested. 
No explanations, no headings, no extra text, no apologies. 
Output exactly and only what the user asked for. 
If the user requests JSON, return PURE JSON. 
Obey safety policy.
"""

# ==============================================================================
# B. HELPER PARSERS & VALIDATORS
# ==============================================================================

def master_json_parser(text):
    """
    Ù…Ø­Ø±Ùƒ ØªØ­Ù„ÙŠÙ„ JSON Ø´Ø§Ù…Ù„ ÙŠØ³ØªØ®Ø¯Ù… Regex Ùˆ json_repair Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø£ÙŠ Ù†Øµ ÙÙˆØ¶ÙˆÙŠ.
    """
    if not text: return None
    
    # 1. Regex Extraction: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø§ Ø¨ÙŠÙ† Ø§Ù„Ø£Ù‚ÙˆØ§Ø³ Ø§Ù„Ù…Ø¹Ù‚ÙˆÙØ© {}
    # Ù‡Ø°Ø§ ÙŠØ²ÙŠÙ„ Ø£ÙŠ Ù†ØµÙˆØµ Ù‚Ø¨Ù„ Ø£Ùˆ Ø¨Ø¹Ø¯ Ø§Ù„Ù€ JSON
    match = regex.search(r'\{(?:[^{}]|(?R))*\}', text, regex.DOTALL)
    candidate = match.group(0) if match else text
    
    # 2. json_repair: Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ ÙˆØ§Ù„Ø£Ù‚ÙˆÙ‰ Ù„Ù„Ø¥ØµÙ„Ø§Ø­
    try:
        decoded = json_repair.repair_json(candidate, return_objects=True)
        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù†ØªÙŠØ¬Ø© Ù‡ÙŠ Ù‚Ø§Ù…ÙˆØ³ Ø£Ùˆ Ù‚Ø§Ø¦Ù…Ø© ÙˆÙ„ÙŠØ³Øª Ù†ØµØ§Ù‹
        if isinstance(decoded, (dict, list)):
            return decoded
    except Exception:
        pass

    # 3. Fallback: Ù…Ø­Ø§ÙˆÙ„Ø© ØªÙ†Ø¸ÙŠÙ Ø¨Ø³ÙŠØ·Ø© ÙˆØ§Ø³ØªØ®Ø¯Ø§Ù… json Ø§Ù„Ù‚ÙŠØ§Ø³ÙŠ
    try:
        clean = candidate.replace("```json", "").replace("```", "").strip()
        return json.loads(clean)
    except:
        return None

def validate_structure(data, required_keys):
    """
    Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Validation).
    ÙŠØ±ÙØ¹ Ø§Ø³ØªØ«Ù†Ø§Ø¡ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù†Ø§Ù‚ØµØ© Ù„ÙŠØ¬Ø¨Ø± Tenacity Ø¹Ù„Ù‰ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©.
    """
    if not isinstance(data, dict):
        raise JSONValidationError(f"Expected Dictionary output, but got type: {type(data)}")
    
    missing_keys = [key for key in required_keys if key not in data]
    
    if missing_keys:
        # Ù‡Ø°Ø§ Ø§Ù„Ø®Ø·Ø£ Ø³ÙŠØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡ Ø¨ÙˆØ§Ø³Ø·Ø© Tenacity Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©
        raise JSONValidationError(f"JSON is valid but missing required keys: {missing_keys}")
    
    return True

# ==============================================================================
# C. THE MAIN STRICT GENERATION FUNCTION
# ==============================================================================

@retry(
    # Ø§Ù„ØªÙˆÙ‚Ù Ø¨Ø¹Ø¯ 5 Ù…Ø­Ø§ÙˆÙ„Ø§Øª ÙØ§Ø´Ù„Ø©
    stop=stop_after_attempt(5),
    
    # Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ø§Ù„Ø£Ø³ÙŠ: ÙŠØ¨Ø¯Ø£ Ø¨Ù€ 4 Ø«ÙˆØ§Ù†ÙŠØŒ Ø«Ù… ÙŠØªØ¶Ø§Ø¹Ù Ø­ØªÙ‰ ÙŠØµÙ„ Ù„Ø£Ù‚ØµÙ‰ Ø­Ø¯ 15 Ø«Ø§Ù†ÙŠØ©
    wait=wait_exponential(multiplier=1, min=4, max=15),
    
    # Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© ÙÙ‚Ø· ÙÙŠ Ø­Ø§Ù„Ø© Ù‡Ø°Ù‡ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©
    retry=retry_if_exception_type((JSONParsingError, JSONValidationError, Exception)),
    
    # ØªØ³Ø¬ÙŠÙ„ Ø±Ø³Ø§Ù„Ø© ÙÙŠ Ø§Ù„Ù„ÙˆØ¬ Ù‚Ø¨Ù„ Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ù„Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©
    before_sleep=before_sleep_log(logger, logging.WARNING)
)
def generate_step_strict(model_name, prompt, step_name, required_keys=[]):
    """
    Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ø­ØªÙˆÙ‰.
    - ØªØ³ØªØ®Ø¯Ù… System Instructions Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ù†ØªÙŠØ¬Ø©.
    - ØªØ³ØªØ®Ø¯Ù… Tenacity Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø¹Ù†Ø¯ Ø§Ù„ÙØ´Ù„.
    - ØªØ³ØªØ®Ø¯Ù… AI Repair Ù„Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ù†Ø­ÙˆÙŠØ© Ø°Ø§ØªÙŠØ§Ù‹.
    - ØªØ¯ÙŠØ± ØªØ¨Ø¯ÙŠÙ„ Ø§Ù„Ù…ÙØ§ØªÙŠØ­ (Key Rotation) Ø¹Ù†Ø¯ Ø§Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„ÙƒÙˆØªØ§.
    """
    log(f"   ğŸ”„ [Tenacity] Executing: {step_name}...")
    
    # 1. Ø¬Ù„Ø¨ Ù…ÙØªØ§Ø­ API Ø§Ù„Ø­Ø§Ù„ÙŠ
    key = key_manager.get_current_key()
    if not key:
        # Ø¥Ø°Ø§ Ù†ÙØ¯Øª Ø§Ù„Ù…ÙØ§ØªÙŠØ­ØŒ Ù†Ø±ÙØ¹ Ø®Ø·Ø£ Ù‚Ø§ØªÙ„Ø§Ù‹ Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø¹Ù‡
        raise RuntimeError("FATAL: All API Keys exhausted.")
    
    client = genai.Client(api_key=key)
    
    try:
        # 2. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ÙƒÙˆÙ†ÙÙŠØ¬ Ø§Ù„ØµØ§Ø±Ù…
        generation_config = types.GenerateContentConfig(
            response_mime_type="application/json",  # Ø¥Ø¬Ø¨Ø§Ø± Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø¹Ù„Ù‰ JSON
            system_instruction=STRICT_SYSTEM_PROMPT,  # Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„ØµØ§Ø±Ù…Ø©
            temperature=0.3,  # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ù„Ù„Ø¯Ù‚Ø©
            top_p=0.8
        )

        # 3. Ø§Ù„Ø·Ù„Ø¨ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù…Ù† Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„
        response = client.models.generate_content(
            model=model_name, 
            contents=prompt, 
            config=generation_config
        )
        
        raw_text = response.text
        
        # 4. Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£ÙˆÙ„Ù‰
        parsed_data = master_json_parser(raw_text)
        
        # 5. Ù…Ù†Ø·Ù‚ Ø§Ù„Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ø°Ø§ØªÙŠ (AI Self-Correction)
        # Ø¥Ø°Ø§ ÙØ´Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ØŒ Ù†Ø·Ù„Ø¨ Ù…Ù† Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¥ØµÙ„Ø§Ø­ Ù…Ø§ Ø£ÙØ³Ø¯Ù‡
        if not parsed_data:
            log(f"      âš ï¸ Parsing failed locally for {step_name}. Triggering AI Repair...")
            
            repair_prompt = f"""
            SYSTEM ALERT: You generated INVALID JSON in the previous step.
            Your output could not be parsed.
            
            TASK: Fix the syntax errors in the content below.
            RULES:
            1. Return ONLY the valid JSON object.
            2. Do NOT add markdown blocks.
            3. Fix unescaped quotes and trailing commas.
            
            BROKEN CONTENT:
            {raw_text[:10000]}
            """
            
            # Ù†Ø³ØªØ®Ø¯Ù… Ù…ÙˆØ¯ÙŠÙ„ Ø³Ø±ÙŠØ¹ (Flash) Ù„Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø¥ØµÙ„Ø§Ø­ Ù„ØªÙˆÙÙŠØ± Ø§Ù„ÙˆÙ‚Øª
            repair_response = client.models.generate_content(
                model="gemini-2.5-flash", 
                contents=repair_prompt,
                config=generation_config # Ù†Ø³ØªØ®Ø¯Ù… Ù†ÙØ³ Ø§Ù„ÙƒÙˆÙ†ÙÙŠØ¬ Ø§Ù„ØµØ§Ø±Ù…
            )
            
            # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ Ø§Ù„Ù…ØµÙ„Ø­
            parsed_data = master_json_parser(repair_response.text)
            
            # Ø¥Ø°Ø§ Ø§Ø³ØªÙ…Ø± Ø§Ù„ÙØ´Ù„ØŒ Ù†Ø±ÙØ¹ Ø®Ø·Ø£ Parsing Ù„ÙŠÙ‚ÙˆÙ… Tenacity Ø¨Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ù† Ø§Ù„ØµÙØ±
            if not parsed_data:
                raise JSONParsingError(f"Failed to parse JSON even after AI repair for step: {step_name}")
            else:
                log(f"      âœ… AI Repair Successful for {step_name}!")

        # 6. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù‡ÙŠÙƒÙ„ (Validation)
        # Ù‡Ù„ Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù…ÙˆØ¬ÙˆØ¯Ø©ØŸ
        if required_keys:
            validate_structure(parsed_data, required_keys)
            
        # Ø¥Ø°Ø§ ÙˆØµÙ„Ù†Ø§ Ù‡Ù†Ø§ØŒ ÙØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø³Ù„ÙŠÙ…Ø© 100%
        log(f"      âœ… Success: {step_name} completed.")
        return parsed_data

    except Exception as e:
        # Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„ÙƒÙˆØªØ§ (429) Ø¨Ø´ÙƒÙ„ Ø®Ø§Øµ
        error_msg = str(e).lower()
        if "429" in error_msg or "quota" in error_msg or "resource exhausted" in error_msg:
            log("      âš ï¸ Quota Exceeded (429). Switching Key & Retrying immediately...")
            if key_manager.switch_key():
                # Ù†Ø±ÙØ¹ Ø§Ù„Ø®Ø·Ø£ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ù„ÙŠÙ‚ÙˆÙ… Tenacity Ø¨Ø§Ù„ØªÙ‚Ø§Ø·Ù‡ ÙˆØ¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø¨Ø§Ù„Ù…ÙØªØ§Ø­ Ø§Ù„Ø¬Ø¯ÙŠØ¯
                raise e 
            else:
                raise RuntimeError("FATAL: All keys exhausted during retry.")
        
        # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø®Ø·Ø£ ÙˆØ±ÙØ¹Ù‡ Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©
        log(f"      âŒ Attempt Failed for {step_name}: {str(e)[:200]}")
        raise e
            

def fetch_full_article(url):
    """
    ğŸš€ SCRAPER v11: 100% Local (Selenium + Trafilatura).
    No 3rd party APIs like Jina. High success rate.
    """
    # 1. Ø¬Ù„Ø¨ Ø§Ù„Ù€ HTML ÙˆØ§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Selenium
    data = url_resolver.get_page_html(url)
    
    if not data or not data.get('html'):
        log(f"      âš ï¸ Selenium failed to get page source.")
        return None
        
    real_url = data['url']
    html_content = data['html']
    
    log(f"      ğŸ§© Extracting content locally from: {real_url[:50]}...")
    
    try:
        # 2. Ø§Ø³ØªØ®Ø¯Ø§Ù… Trafilatura Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Øµ Ø§Ù„Ù…Ù‚Ø§Ù„Ø© Ù…Ù† Ø§Ù„Ù€ HTML
        # include_comments=False: Ù„Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª
        # include_tables=True: Ù„Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ù…Ù‡Ù…Ø©
        extracted_text = trafilatura.extract(
            html_content, 
            include_comments=False, 
            include_tables=True,
            favor_precision=True # Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø¯Ù‚Ø© Ø§Ù„Ù†Øµ ÙˆÙ„ÙŠØ³ ÙƒØ«Ø±ØªÙ‡
        )
        
        if extracted_text and len(extracted_text) > 500:
            log(f"      âœ… Extraction Success! {len(extracted_text)} chars found.")
            return extracted_text[:12000]
        else:
            log("      âš ï¸ Trafilatura found very little text. Trying fallback...")
            
            # Fallback: Ù…Ø­Ø§ÙˆÙ„Ø© Ø¨Ø³ÙŠØ·Ø© ÙÙŠ Ø­Ø§Ù„ ÙØ´Ù„ Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…ØªØ®ØµØµØ©
            soup = BeautifulSoup(html_content, 'html.parser')
            # Ø­Ø°Ù Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ø²Ø¹Ø¬Ø© ÙŠØ¯ÙˆÙŠØ§Ù‹
            for script in soup(["script", "style", "nav", "footer", "header", "aside"]):
                script.extract()
            text = soup.get_text(separator='\n')
            
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„ÙØ±Ø§ØºØ§Øª
            lines = [line.strip() for line in text.splitlines() if line.strip()]
            clean_text = '\n'.join(lines)
            
            if len(clean_text) > 500:
                log(f"      âœ… Fallback Success (BS4): {len(clean_text)} chars.")
                return clean_text[:12000]

    except Exception as e:
        log(f"      âŒ Extraction Error: {e}")
        
    return None


def get_real_news_rss(query_keywords, category):
    try:
        if "," in query_keywords:
            topics = [t.strip() for t in query_keywords.split(',') if t.strip()]
            focused = random.choice(topics)
            log(f"   ğŸ¯ Targeted Search: '{focused}'")
            full_query = f"{focused} when:1d"
        else:
            full_query = f"{query_keywords} when:1d"

        encoded = urllib.parse.quote(full_query)
        url = f"https://news.google.com/rss/search?q={encoded}&hl=en-US&gl=US&ceid=US:en"
        
        feed = feedparser.parse(url)
        items = []
        if feed.entries:
            for entry in feed.entries[:8]:
                pub = entry.published if 'published' in entry else "Today"
                title_clean = entry.title.split(' - ')[0]
                items.append({"title": title_clean, "link": entry.link, "date": pub})
            return items 
        else:
            log(f"   âš ï¸ RSS Empty. Fallback.")
            fb = f"{category} news when:1d"
            url = f"https://news.google.com/rss/search?q={urllib.parse.quote(fb)}&hl=en-US&gl=US&ceid=US:en"
            feed = feedparser.parse(url)
            for entry in feed.entries[:5]:
                items.append({"title": entry.title, "link": entry.link, "date": "Today"})
            return items
            
    except Exception as e:
        log(f"âŒ RSS Error: {e}")
        return []

def get_blogger_token():
    payload = {
        'client_id': os.getenv('BLOGGER_CLIENT_ID'),
        'client_secret': os.getenv('BLOGGER_CLIENT_SECRET'),
        'refresh_token': os.getenv('BLOGGER_REFRESH_TOKEN'),
        'grant_type': 'refresh_token'
    }
    try:
        r = requests.post('https://oauth2.googleapis.com/token', data=payload)
        return r.json().get('access_token') if r.status_code == 200 else None
    except: return None

def publish_post(title, content, labels):
    token = get_blogger_token()
    if not token: return None
    
    url = f"https://www.googleapis.com/blogger/v3/blogs/{os.getenv('BLOGGER_BLOG_ID')}/posts?isDraft=false"
    headers = {"Authorization": f"Bearer {token}", "Content-Type": "application/json"}
    body = {"title": title, "content": content, "labels": labels}
    
    try:
        r = requests.post(url, headers=headers, json=body)
        if r.status_code == 200:
            link = r.json().get('url')
            log(f"âœ… Published LIVE: {link}")
            return link
        log(f"âŒ Blogger Error: {r.text}")
        return None
    except Exception as e:
        log(f"âŒ Connection Fail: {e}")
        return None
        

def generate_and_upload_image(prompt_text, overlay_text=""):
    key = os.getenv('IMGBB_API_KEY')
    if not key: return None
    
    log(f"   ğŸ¨ Generating Thumbnail (Flux + Local Overlay)...")

    # 1. ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª (Flux Realism)
    enhancers = ", photorealistic, shot on Sony A7R IV, 85mm lens, f/1.8, cinematic lighting, youtube thumbnail style, 8k, --no cartoon, --no illustration, --no 3d render"
    final_prompt = urllib.parse.quote(f"{prompt_text}{enhancers}")
    seed = random.randint(1, 99999)
    
    # Ù†Ø·Ù„Ø¨ Ø§Ù„ØµÙˆØ±Ø© "Ø®Ø§Ù…" Ø¨Ø¯ÙˆÙ† Ù†Øµ Ù…Ù† Pollinations
    image_url = f"https://image.pollinations.ai/prompt/{final_prompt}?width=1280&height=720&model=flux&seed={seed}&nologo=true"

    try:
        # 2. ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        r = requests.get(image_url, timeout=60)
        if r.status_code != 200: return None
        
        img = Image.open(BytesIO(r.content)).convert("RGBA")
        
        # 3. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ (Overlay) Ù…Ø­Ù„ÙŠØ§Ù‹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… PIL
        if overlay_text:
            draw = ImageDraw.Draw(img)
            W, H = img.size
            
            # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù…ÙŠÙ„ Ø®Ø· Ø¹Ø±ÙŠØ¶ØŒ Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ
            try:
                # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®Ø·ÙˆØ· Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
                font_path = "/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf"
                if not os.path.exists(font_path):
                    font_path = "arial.ttf"
                font = ImageFont.truetype(font_path, 80)
            except:
                font = ImageFont.load_default()

            # ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù†Øµ (Ø£Ø­Ø±Ù ÙƒØ¨ÙŠØ±Ø©)
            text = overlay_text.upper()
            
            # Ø­Ø³Ø§Ø¨ Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ù†Øµ Ù„ØªÙˆØ³ÙŠØ·Ù‡
            bbox = draw.textbbox((0, 0), text, font=font)
            text_w = bbox[2] - bbox[0]
            text_h = bbox[3] - bbox[1]
            
            x = (W - text_w) / 2
            y = H - text_h - 50 # Ù…Ø³Ø§ÙØ© Ù…Ù† Ø§Ù„Ø£Ø³ÙÙ„
            
            # Ø±Ø³Ù… Ø­Ø¯ÙˆØ¯ Ø³ÙˆØ¯Ø§Ø¡ Ù„Ù„Ù†Øµ (Stroke) Ù„Ø²ÙŠØ§Ø¯Ø© Ø§Ù„ÙˆØ¶ÙˆØ­
            stroke_width = 4
            draw.text((x, y), text, font=font, fill="yellow", stroke_width=stroke_width, stroke_fill="black")

        # 4. ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø¹Ø¯Ù„Ø© Ù„Ù„Ø±ÙØ¹
        img_byte_arr = BytesIO()
        img.convert("RGB").save(img_byte_arr, format='JPEG', quality=95)
        img_byte_arr.seek(0)
        
        # 5. Ø§Ù„Ø±ÙØ¹ Ø¥Ù„Ù‰ ImgBB
        log("      âœ… Uploading processed thumbnail...")
        res = requests.post(
            "https://api.imgbb.com/1/upload", 
            data={"key": key, "name": f"thumb_{seed}"}, 
            files={"image": img_byte_arr}, 
            timeout=60
        )
        
        if res.status_code == 200:
            return res.json()['data']['url']
            
    except Exception as e:
        log(f"      âš ï¸ Image Generation/Upload Error: {e}")
    
    return None 

def load_kg():
    try:
        if os.path.exists('knowledge_graph.json'): return json.load(open('knowledge_graph.json','r'))
    except: pass
    return []

def get_recent_titles_string(category=None, limit=100):
    """
    ØªÙ‚Ø±Ø£ Ù…Ù„Ù knowledge_graph.json ÙˆØªØ¹ÙŠØ¯ Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©.
    Ø§Ù„ØªØ­Ø³ÙŠÙ†: ØªÙ‚ÙˆÙ… Ø¨ÙÙ„ØªØ±Ø© Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ø­Ø³Ø¨ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ© Ù„Ø²ÙŠØ§Ø¯Ø© Ø¯Ù‚Ø© Ù…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø±.
    """
    kg = load_kg()
    if not kg: return "No previous articles found."
    
    # ØªØµÙÙŠØ© Ø§Ù„Ù†ØªØ§Ø¦Ø¬: Ù†Ø£Ø®Ø° ÙÙ‚Ø· Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ù„ØªÙŠ ØªÙ†ØªÙ…ÙŠ Ù„Ù†ÙØ³ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©
    # Ø£Ùˆ Ù†Ø£Ø®Ø° Ø§Ù„ÙƒÙ„ Ø¥Ø°Ø§ Ù„Ù… Ù†Ø­Ø¯Ø¯ ÙØ¦Ø©
    if category:
        relevant_items = [i for i in kg if i.get('section') == category]
    else:
        relevant_items = kg

    # Ù†Ø£Ø®Ø° Ø¢Ø®Ø± 'limit' Ø¹Ù†ØµØ± (Ø§Ù„Ø£Ø­Ø¯Ø«)
    recent_items = relevant_items[-limit:]
    
    # Ù†Ø¯Ù…Ø¬Ù‡Ø§ ÙÙŠ Ù†Øµ ÙˆØ§Ø­Ø¯ Ù…ÙØµÙˆÙ„ Ø¨Ù€ " | " Ù„ÙŠØ³Ù‡Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ù‚Ø±Ø§Ø¡ØªÙ‡
    titles = [f"- {i.get('title','Unknown')}" for i in recent_items]
    
    if not titles:
        return "No previous articles in this category."
        
    return "\n".join(titles)
    
def get_relevant_kg_for_linking(category, limit=60):
    kg = load_kg()
    links = [{"title":i['title'],"url":i['url']} for i in kg if i.get('section')==category][:limit]
    return json.dumps(links)

def update_kg(title, url, section):
    try:
        data = load_kg()
        for i in data:
            if i['url']==url: return
        data.append({"title":title, "url":url, "section":section, "date":str(datetime.date.today())})
        with open('knowledge_graph.json','w') as f: json.dump(data, f, indent=2)
    except: pass

def perform_maintenance_cleanup():
    try:
        if os.path.exists('knowledge_graph.json'):
            with open('knowledge_graph.json','r') as f: d=json.load(f)
            if len(d)>800: json.dump(d[-400:], open('knowledge_graph.json','w'), indent=2)
    except: pass

def generate_step(model, prompt, step):
    log(f"   ğŸ‘‰ Generating: {step}")
    while True:
        key = key_manager.get_current_key()
        if not key: 
            log("âŒ FATAL: Keys exhausted.")
            return None
        client = genai.Client(api_key=key)
        try:
            r = client.models.generate_content(
                model=model, contents=prompt, config=types.GenerateContentConfig(response_mime_type="application/json")
            )
            return clean_json(r.text)
        except Exception as e:
            if "429" in str(e) or "quota" in str(e).lower():
                if not key_manager.switch_key(): return None
            else: return None
        
# ==============================================================================
# 4. ADVANCED SCRAPING (UPDATED FOR HIGH QUALITY & LOGGING)
# ==============================================================================
def resolve_and_scrape(google_url):
    """
    Open Google URL -> Resolve -> Get Page Source -> Extract Text.
    Returns: (final_url, page_title, text_content)
    """
    log(f"      ğŸ•µï¸â€â™‚ï¸ Selenium: Opening & Resolving: {google_url[:60]}...")
    
    # Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ù…ØªØµÙØ­
    chrome_options = Options()
    chrome_options.add_argument("--headless=new")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    # Ø§Ù†ØªØ­Ø§Ù„ Ø´Ø®ØµÙŠØ© Ù…ØªØµÙØ­ Ø­Ù‚ÙŠÙ‚ÙŠ Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø­Ø¸Ø±
    chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
    chrome_options.add_argument("--mute-audio") # ÙƒØªÙ… Ø§Ù„ØµÙˆØª Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„ØªØ­Ù…ÙŠÙ„

    driver = None
    try:
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        driver.set_page_load_timeout(25) # Ù…Ù‡Ù„Ø© ØªØ­Ù…ÙŠÙ„ 25 Ø«Ø§Ù†ÙŠØ©
        
        driver.get(google_url)
        
        # Ø­Ù„Ù‚Ø© Ø§Ù†ØªØ¸Ø§Ø± Ù„Ù„Ø®Ø±ÙˆØ¬ Ù…Ù† Ø¬ÙˆØ¬Ù„
        start_wait = time.time()
        final_url = google_url
        
        while time.time() - start_wait < 15: # Ø§Ù†ØªØ¸Ø§Ø± 15 Ø«Ø§Ù†ÙŠØ© ÙƒØ­Ø¯ Ø£Ù‚ØµÙ‰ Ù„Ù„ØªØ­ÙˆÙŠÙ„
            current = driver.current_url
            if "news.google.com" not in current and "google.com" not in current:
                final_url = current
                break
            time.sleep(1) # ÙØ­Øµ ÙƒÙ„ Ø«Ø§Ù†ÙŠØ©
        
        # Ø§Ù„ØªÙ‚Ø§Ø· Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ Ù„Ù„ØµÙØ­Ø©
        final_title = driver.title
        page_source = driver.page_source
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø±ÙˆØ§Ø¨Ø· ØºÙŠØ± Ø§Ù„Ù…Ø±ØºÙˆØ¨Ø© (ÙÙŠØ¯ÙŠÙˆØŒ Ù…Ø¹Ø§Ø±Ø¶ ØµÙˆØ±)
        # Ù‡Ø°Ø§ ÙŠÙ…Ù†Ø¹ Ù…Ø´ÙƒÙ„Ø© "Washington Post Video" Ø§Ù„ØªÙŠ ÙˆØ§Ø¬Ù‡ØªÙ‡Ø§
        bad_segments = ["/video/", "/watch", "/gallery/", "/photos/", "youtube.com"]
        if any(seg in final_url.lower() for seg in bad_segments):
            log(f"      âš ï¸ Skipped Video/Gallery URL: {final_url}")
            return None, None, None

        log(f"      ğŸ”— Resolved URL: {final_url[:70]}...")
        log(f"      ğŸ·ï¸ Real Page Title: {final_title[:70]}...")

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Trafilatura
        extracted_text = trafilatura.extract(
            page_source, 
            include_comments=False, 
            include_tables=True,
            favor_precision=True
        )
        
        if extracted_text and len(extracted_text) > 1000:
            return final_url, final_title, extracted_text

        # Fallback (BS4) Ø¥Ø°Ø§ ÙØ´Ù„ Trafilatura
        soup = BeautifulSoup(page_source, 'html.parser')
        for script in soup(["script", "style", "nav", "footer", "header", "aside", "noscript"]):
            script.extract()
        fallback_text = soup.get_text(" ", strip=True)
        
        return final_url, final_title, fallback_text

    except Exception as e:
        log(f"      âŒ Selenium Error: {e}")
        return None, None, None
    finally:
        if driver:
            driver.quit()

# ==============================================================================
# HELPER: IMAGE EXTRACTION & PROCESSING (Real Images)
# ==============================================================================

def extract_og_image(html_content):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø¨Ø§Ø±Ø²Ø© Ù…Ù† ÙƒÙˆØ¯ HTML"""
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        # Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© 1: Open Graph
        meta = soup.find('meta', property='og:image')
        if meta and meta.get('content'): return meta['content']
        # Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© 2: Twitter Card
        meta = soup.find('meta', name='twitter:image')
        if meta and meta.get('content'): return meta['content']
        return None
    except:
        return None

def draw_text_with_outline(draw, position, text, font, fill_color, outline_color, outline_width):
    """Ø±Ø³Ù… Ù†Øµ Ù…Ø¹ Ø­Ø¯ÙˆØ¯ Ø®Ø§Ø±Ø¬ÙŠØ© Ù„Ø²ÙŠØ§Ø¯Ø© Ø§Ù„ÙˆØ¶ÙˆØ­"""
    x, y = position
    for dx in range(-outline_width, outline_width + 1):
        for dy in range(-outline_width, outline_width + 1):
            if dx != 0 or dy != 0:
                draw.text((x + dx, y + dy), text, font=font, fill=outline_color)
    draw.text(position, text, font=font, fill=fill_color)

def process_source_image(source_url, overlay_text, filename_title):
    """
    ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©ØŒ ØªØ¹ØªÙŠÙ…Ù‡Ø§ØŒ ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù†ØµØŒ ÙˆØ±ÙØ¹Ù‡Ø§ Ø¨Ø§Ø³Ù… Ù…Ø®ØµØµ.
    """
    key = os.getenv('IMGBB_API_KEY')
    if not key: return None

    log(f"   ğŸ–¼ï¸ Processing Source Image: {source_url[:60]}...")
    
    try:
        # 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© (ØªÙ…ÙˆÙŠÙ‡ Ø§Ù„Ù…ØªØµÙØ­)
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
        r = requests.get(source_url, headers=headers, timeout=15, stream=True)
        if r.status_code != 200: return None
        
        original_img = Image.open(BytesIO(r.content)).convert("RGBA")
        
        # 2. ØªØºÙŠÙŠØ± Ø§Ù„Ø­Ø¬Ù… ÙˆØ§Ù„Ù‚Øµ (Standard Blog Size 1200x630)
        target_w, target_h = 1200, 630
        img_ratio = original_img.width / original_img.height
        target_ratio = target_w / target_h
        
        if img_ratio > target_ratio:
            new_height = target_h
            new_width = int(new_height * img_ratio)
        else:
            new_width = target_w
            new_height = int(new_width / img_ratio)
            
        original_img = original_img.resize((new_width, new_height), Image.Resampling.LANCZOS)
        
        # Center Crop
        left = (new_width - target_w) / 2
        top = (new_height - target_h) / 2
        right = (new_width + target_w) / 2
        bottom = (new_height + target_h) / 2
        base_img = original_img.crop((left, top, right, bottom))
        
        # 3. Ø¥Ø¶Ø§ÙØ© Ø·Ø¨Ù‚Ø© ØªØ¹ØªÙŠÙ… (Dark Overlay)
        overlay = Image.new('RGBA', base_img.size, (0, 0, 0, 90)) # ØªØ¹ØªÙŠÙ… Ø¨Ù†Ø³Ø¨Ø© 35% ØªÙ‚Ø±ÙŠØ¨Ø§Ù‹
        base_img = Image.alpha_composite(base_img, overlay)
        
        # 4. ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù†Øµ (Overlay Text)
        if overlay_text:
            draw = ImageDraw.Draw(base_img)
            W, H = base_img.size
            
            # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù…ÙŠÙ„ Ø®Ø· Ø¹Ø±ÙŠØ¶
            try:
                # Ù…Ø³Ø§Ø±Ø§Øª Ø®Ø·ÙˆØ· Ù„ÙŠÙ†ÙƒØ³ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
                font_path = "/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf"
                if not os.path.exists(font_path): font_path = "arialbd.ttf"
                font = ImageFont.truetype(font_path, 70)
            except:
                font = ImageFont.load_default()
            
            # Ø§Ù„ØªÙØ§Ù Ø§Ù„Ù†Øµ (Word Wrap)
            words = overlay_text.upper().split()
            lines = []
            current_line = []
            for word in words:
                test_line = ' '.join(current_line + [word])
                bbox = draw.textbbox((0, 0), test_line, font=font)
                if bbox[2] < W - 100: current_line.append(word)
                else:
                    lines.append(' '.join(current_line))
                    current_line = [word]
            lines.append(' '.join(current_line))
            
            # Ø±Ø³Ù… Ø§Ù„Ø£Ø³Ø·Ø± ÙÙŠ Ø§Ù„Ù…Ù†ØªØµÙ
            text_y = H / 2 - (len(lines) * 80 / 2)
            for line in lines:
                bbox = draw.textbbox((0, 0), line, font=font)
                line_x = (W - (bbox[2] - bbox[0])) / 2
                draw_text_with_outline(draw, (line_x, text_y), line, font, "#FFD700", "black", 4)
                text_y += 85

        # 5. ØªÙ†Ø¸ÙŠÙ Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù (SEO Friendly Filename)
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø¥Ù„Ù‰ ØµÙŠØºØ©: this-is-the-title
        safe_filename = re.sub(r'[^a-zA-Z0-9\s-]', '', filename_title).strip().replace(' ', '-').lower()
        safe_filename = safe_filename[:50] # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø·ÙˆÙ„

        # 6. Ø§Ù„Ø±ÙØ¹
        img_byte_arr = BytesIO()
        base_img.convert("RGB").save(img_byte_arr, format='JPEG', quality=92)
        img_byte_arr.seek(0)
        
        log(f"      ğŸ“¤ Uploading processed image as: {safe_filename}.jpg")
        res = requests.post(
            "https://api.imgbb.com/1/upload", 
            data={"key": key, "name": safe_filename}, # Ù‡Ù†Ø§ Ù†Ø±Ø³Ù„ Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ù…Ø®ØµØµ
            files={"image": img_byte_arr}, 
            timeout=60
        )
        
        if res.status_code == 200:
            return res.json()['data']['url']
            
    except Exception as e:
        log(f"      âš ï¸ Image Processing Error: {e}")
    
    return None

def run_pipeline(category, config, mode="trending"):
    """
    The Master Pipeline v14.0 (Viral B2C + Strict Integration + Robust Multimedia).
    Integrates social_manager, video_renderer, and youtube_manager flawlessly.
    """
    # 1. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
    model_name = config['settings'].get('model_name')
    cat_conf = config['categories'][category]
    
    log(f"\nğŸš€ INIT PIPELINE: {category} (Viral Explainer Mode âš¡)")
    
    # ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±
    # ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±
    # Ù†Ù…Ø±Ø± Ø§Ù„Ù€ category Ù„Ù†Ø­ØµÙ„ Ø¹Ù„Ù‰ ØªØ§Ø±ÙŠØ® Ù‡Ø°Ø§ Ø§Ù„Ù‚Ø³Ù… ÙÙ‚Ø·
    recent_titles = get_recent_titles_string(category=category, limit=100)
    
    # Ø·Ø¨Ø§Ø¹Ø© Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù†Ø¸Ø§Ù… ÙŠØ±Ù‰ Ø§Ù„ØªØ§Ø±ÙŠØ®
    # log(f"   ğŸ“š Knowledge Graph Loaded: Found {len(recent_titles.split(' | '))} previous articles in '{category}'.")

    # =====================================================
    # STEP 0: SEO STRATEGY (THE BRAIN)
    # =====================================================
    log("   ğŸ§  Consulting SEO Strategist for a winning keyword...")
    
    target_keyword = ""

    try:
        # Ø·Ù„Ø¨ ÙƒÙ„Ù…Ø© Ù…ÙØªØ§Ø­ÙŠØ© Ø°ÙƒÙŠØ©
        seo_prompt = PROMPT_ZERO_SEO.format(category=category, date=datetime.date.today())
        
        seo_plan = generate_step_strict(
            model_name, 
            seo_prompt, 
            "Step 0 (SEO Strategy)", 
            required_keys=["target_keyword"]
        )
        
        target_keyword = seo_plan.get('target_keyword')
        log(f"   ğŸ¯ Strategy Defined: Targeting keyword '{target_keyword}'")
        
    except Exception as e:
        log(f"   âš ï¸ SEO Step failed. Using fallback.")
        target_keyword = cat_conf.get('trending_focus', category)
        if "," in target_keyword:
            target_keyword = random.choice([t.strip() for t in target_keyword.split(',')])

    # =====================================================
    # STEP 1: MULTI-SOURCE RESEARCH (THE HUNTER)
    # =====================================================
    # (Pseudo-code logic to add)
    reddit_query = f"{target_keyword} site:reddit.com"
    # Ù‚Ù… Ø¨Ø¬Ù„Ø¨ Ø£ÙˆÙ„ Ù†ØªÙŠØ¬Ø© Ù…Ù† Ø±ÙŠØ¯ÙŠØª ÙˆØ§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ù†Øµ Ù…Ù†Ù‡Ø§
    # Ø£Ø¶Ù Ù‡Ø°Ø§ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ "Payload" Ø§Ù„Ù…Ø±Ø³Ù„ Ù„Ù„Ù…ÙˆØ¯ÙŠÙ„ ØªØ­Øª Ø¹Ù†ÙˆØ§Ù†:
    # *** REAL USER OPINIONS (REDDIT) ***
    # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø£Ø®Ø¨Ø§Ø± Ø¬ÙˆØ¬Ù„
    rss_query = f"{target_keyword} when:3d"
    rss_items = get_real_news_rss(rss_query.replace("when:3d","").strip(), category)
    
    if not rss_items:
        log("   âš ï¸ No specific news found. Retrying broad search...")
        rss_items = get_real_news_rss(category, category)
        if not rss_items:
            log("âŒ FATAL: No RSS items found. Aborting.")
            return

    collected_sources = []
    main_headline = ""
    main_link = ""
    
    log(f"   ğŸ•µï¸â€â™‚ï¸ Investigating sources for: '{target_keyword}'...")
    
    for item in rss_items[:6]:
        if item['title'][:20] in recent_titles: continue
            # --- NEW BLOCKER ---
        # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø¹Ù†ÙˆØ§Ù† ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ù…Ù…Ù„Ø©ØŒ ØªØ¬Ø§ÙˆØ²Ù‡ ÙÙˆØ±Ø§Ù‹
        if any(b_word.lower() in item['title'].lower() for b_word in BORING_KEYWORDS):
            log(f"         â›” Skipped Boring Corporate Topic: {item['title']}")
            continue
        # -------------------
        if any(src['domain'] in item['link'] for src in collected_sources): continue

        log(f"      ğŸ“Œ Checking Source: {item['title'][:40]}...")
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¯Ø§Ù„Ø© url_resolver Ø§Ù„ØªÙŠ Ø£Ø±ÙÙ‚ØªÙ‡Ø§
        data = url_resolver.get_page_html(item['link'])
        
        if data and data.get('html'):
            r_url = data['url']
            html_content = data['html']
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Trafilatura
            text = trafilatura.extract(html_content, include_comments=False, include_tables=True)
            
            # Fallback Ø¥Ø°Ø§ ÙØ´Ù„ Trafilatura
            if not text:
                soup = BeautifulSoup(html_content, 'html.parser')
                for script in soup(["script", "style", "nav", "footer"]): script.extract()
                text = soup.get_text(" ", strip=True)


                # ... (Ø¯Ø§Ø®Ù„ Ø­Ù„Ù‚Ø© ÙØ­Øµ Ø§Ù„Ù…ØµØ§Ø¯Ø±) ...
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ© Ù…Ù† Ø§Ù„Ù€ HTML Ø§Ù„Ø°ÙŠ Ø¬Ù„Ø¨Ù†Ø§Ù‡
            og_image = extract_og_image(html_content)

            # Ù‚Ø¨ÙˆÙ„ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø°Ø§Øª Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨
            if text and len(text) >= 800:
                log(f"         âœ… Accepted Source! ({len(text)} chars). Has Image? {'Yes' if og_image else 'No'}")
                domain = urllib.parse.urlparse(r_url).netloc
                r_title = item['title']

                collected_sources.append({
                    "title": r_title,
                    "text": text,
                    "domain": domain,
                    "url": r_url,
                    "date": item['date'],
                    "source_image": og_image  # <-- Ù†Ø­ÙØ¸ Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙˆØ±Ø© Ù‡Ù†Ø§
                })
                
                if not main_headline:
                    # ...
                
                if not main_headline:
                    main_headline = item['title']
                    main_link = item['link']
                
                if len(collected_sources) >= 3: break
            else:
                log("         âš ï¸ Content too short or extraction failed.")
        else:
             log("         âš ï¸ Selenium failed to resolve URL.")
             
        time.sleep(2) # Ø±Ø§Ø­Ø© Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø­Ø¸Ø±

    if not collected_sources:
        log("âŒ FATAL: No valid sources found. Skipping.")
        return

    # =====================================================
    # STEP 2: DRAFTING & SYNTHESIS (THE STRICT CHAIN)
    # =====================================================
    log(f"\nâœï¸ Synthesizing Content from {len(collected_sources)} sources...")
    
    # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù†Øµ Ù„Ù„Ù…ÙˆØ¯ÙŠÙ„
    combined_text = ""
    for i, src in enumerate(collected_sources):
        combined_text += f"\n--- SOURCE {i+1}: {src['domain']} ---\nTitle: {src['title']}\nDate: {src['date']}\nCONTENT:\n{src['text'][:9000]}\n"

    # ØªØ­Ø¶ÙŠØ± Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ØµØ§Ø¯Ø± Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ù„Ø§Ø­Ù‚Ø§Ù‹ ÙÙŠ Step C
    sources_list_formatted = [{"title": s['title'], "url": s['url']} for s in collected_sources]

# ... Ø¯Ø§Ø®Ù„ run_pipeline ...

    # Ø¥Ø¶Ø§ÙØ© ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
    json_ctx = {
        "rss_headline": main_headline,
        "keyword_focus": target_keyword,
        "source_count": len(collected_sources),
        "date": str(datetime.date.today()),
        "style_guide": "Critical, First-Person, Beginner-Focused, Honest Review" # <-- Ø¥Ø¶Ø§ÙØ© Ù‡Ø°Ø§ Ø§Ù„Ø³Ø·Ø±
    }
    
    # ... Ø§Ù„Ø¨Ø§Ù‚ÙŠ ÙƒÙ…Ø§ Ù‡Ùˆ ...
    
    payload = f"METADATA: {json.dumps(json_ctx)}\n\n*** RESEARCH DATA ***\n{combined_text}"
    
    try:
        # --- Step B: Writer (B2C Style) ---
        required_b = ["headline", "hook", "article_body", "verdict"]
        
        json_b = generate_step_strict(
            model_name, 
            PROMPT_B_TEMPLATE.format(json_input=payload, forbidden_phrases=str(FORBIDDEN_PHRASES)), 
            "Step B (Writer)", 
            required_keys=required_b
        )

        # --- Step C: SEO & Style ---
        kg_links = get_relevant_kg_for_linking(category)
        
        input_for_c = {
            "draft_content": json_b,
            "sources_data": sources_list_formatted 
        }
        
        required_c = ["finalTitle", "finalContent", "seo", "imageGenPrompt"]
        prompt_c = PROMPT_C_TEMPLATE.format(json_input=json.dumps(input_for_c), knowledge_graph=kg_links)
        
        json_c = generate_step_strict(
            model_name, 
            prompt_c, 
            "Step C (SEO & Style)", 
            required_keys=required_c
        )

        # --- Step D: Humanizer ---
        required_d = ["finalTitle", "finalContent"]
        prompt_d = PROMPT_D_TEMPLATE.format(json_input=json.dumps(json_c))
        
        json_d = generate_step_strict(
            model_name, 
            prompt_d, 
            "Step D (Humanizer)", 
            required_keys=required_d
        )

        # --- Step E: Final Polish ---
        required_e = ["finalTitle", "finalContent", "imageGenPrompt", "seo"]
        prompt_e = PROMPT_E_TEMPLATE.format(json_input=json.dumps(json_d))
        
        final = generate_step_strict(
            model_name, 
            prompt_e, 
            "Step E (Final Polish)", 
            required_keys=required_e
        )
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
        title = final['finalTitle']
        content_html = final['finalContent']
        seo_data = final.get('seo', {})
        img_prompt = final.get('imageGenPrompt', title)
        img_overlay = final.get('imageOverlayText', 'News')

    except Exception as e:
        log(f"âŒ PIPELINE CRASHED during generation: {e}")
        import traceback
        traceback.print_exc()
        return

    # =====================================================
    # STEP 3: MULTIMEDIA GENERATION (Priority: Real -> AI)
    # =====================================================
    log("   ğŸ§  Generating Multimedia Assets...")
    
    yt_meta = {}
    fb_cap = title
    vid_html = ""
    img_url = None

    try:
        # 1. Social Metadata & Hooks
        yt_meta = generate_step_strict(
            model_name, 
            PROMPT_YOUTUBE_METADATA.format(draft_title=title), 
            "YT Meta",
            required_keys=["title", "description", "tags"]
        )
        
        fb_dat = generate_step_strict(
            model_name, 
            PROMPT_FACEBOOK_HOOK.format(title=title), 
            "FB Hook",
            required_keys=["FB_Hook"]
        )
        fb_cap = fb_dat.get('FB_Hook', title)

        # 2. Image Strategy: Try Sources First, Then AI
        overlay_text_clean = img_overlay if img_overlay else "NEWS UPDATE"
        
        log("   ğŸ–¼ï¸ Starting Image Strategy (Real Sources First)...")
        
        # Ø§Ù„Ø­Ù„Ù‚Ø© Ø§Ù„Ø°ÙƒÙŠØ©: Ù†Ø­Ø§ÙˆÙ„ Ø§Ù„Ù…ØµØ¯Ø± Ø§Ù„Ø£ÙˆÙ„ØŒ Ø«Ù… Ø§Ù„Ø«Ø§Ù†ÙŠØŒ ÙˆÙ‡ÙƒØ°Ø§
        for src in collected_sources:
            raw_img_url = src.get('source_image')
            if raw_img_url:
                log(f"      ğŸ¯ Trying image from source: {src['domain']}...")
                # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
                img_url = process_source_image(raw_img_url, overlay_text_clean, title)
                
                if img_url:
                    log("      âœ… Success! Real source image processed & uploaded.")
                    break # ÙˆØ¬Ø¯Ù†Ø§ ØµÙˆØ±Ø©ØŒ Ù†Ø®Ø±Ø¬ Ù…Ù† Ø§Ù„Ø­Ù„Ù‚Ø©
                else:
                    log("      âš ï¸ Failed to process this source image. Trying next...")
        
        # Ø¥Ø°Ø§ Ø§Ù†ØªÙ‡Øª Ø§Ù„Ø­Ù„Ù‚Ø© ÙˆÙ„Ù… Ù†Ø¬Ø¯ ØµÙˆØ±Ø© (img_url Ù„Ø§ ÙŠØ²Ø§Ù„ None)
        if not img_url:
            log("      ğŸ¨ No valid source images found/processed. Fallback to AI Generation...")
            img_url = generate_and_upload_image(img_prompt, img_overlay)

        # 3. Video Generation (ÙƒÙ…Ø§ Ù‡Ùˆ Ø¨Ø¯ÙˆÙ† ØªØºÙŠÙŠØ±...)
        # ...

        # 3. Video Generation (Robust Key Search)
        summ_clean = re.sub('<[^<]+?>','', content_html)[:2500]
        
        script_json = None
        
        # Ø­Ù„Ù‚Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© (3 Ù…Ø­Ø§ÙˆÙ„Ø§Øª)
        for attempt in range(1, 4):
            log(f"      ğŸ¬ Generating Script (Attempt {attempt}/3)...")
            try:
                raw_result = generate_step_strict(
                    model_name, 
                    PROMPT_VIDEO_SCRIPT.format(title=title, text_summary=summ_clean), 
                    f"Video Script (Att {attempt})"
                )
                
                # Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
                if isinstance(raw_result, dict):
                    # 1. Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø¨Ø§Ø´Ø± Ø¹Ù† Ø§Ù„Ù…ÙØªØ§Ø­ Ø§Ù„Ø°ÙŠ Ø·Ù„Ø¨Ù†Ø§Ù‡
                    if 'video_script' in raw_result and isinstance(raw_result['video_script'], list):
                        script_json = raw_result['video_script']
                        log("      âœ… Found 'video_script' key directly.")
                        break
                    
                    # 2. Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…ÙØ§ØªÙŠØ­ Ø¨Ø¯ÙŠÙ„Ø© Ù…Ø­ØªÙ…Ù„Ø©
                    for key in ['script', 'dialogue', 'conversation', 'scenes', 'content']:
                        if key in raw_result and isinstance(raw_result[key], list):
                            script_json = raw_result[key]
                            log(f"      âœ… Found script under key: '{key}'")
                            break
                    
                    # 3. Ø¥Ø°Ø§ ÙØ´Ù„ØŒ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£ÙŠ Ù‚Ø§Ø¦Ù…Ø© Ø¯Ø§Ø®Ù„ Ø§Ù„Ù‚ÙŠÙ…
                    if not script_json:
                        for val in raw_result.values():
                            if isinstance(val, list) and len(val) > 0:
                                if isinstance(val[0], dict) and 'text' in val[0]:
                                    script_json = val
                                    log("      âœ… Found script hidden in values.")
                                    break
                
                elif isinstance(raw_result, list):
                    script_json = raw_result
                    log("      âœ… Received List directly.")
                    break
                
                if not script_json:
                     log("      âŒ Attempt failed. Retrying...")

            except Exception as e:
                log(f"      âš ï¸ Script Generation Error: {e}")

        # Ø¨Ø¯Ø¡ Ø§Ù„Ø±ÙŠÙ†Ø¯Ø± Ø¥Ø°Ø§ ÙˆØ¬Ø¯Ù†Ø§ Ø§Ù„Ø³ÙƒØ±Ø¨Øª
        if script_json and len(script_json) > 0:
            timestamp = int(time.time())
            base_output_dir = os.path.abspath("output")
            os.makedirs(base_output_dir, exist_ok=True)
            
            # --- Main Video ---
            log(f"      ğŸ¬ Rendering Main Video...")
            try:
                rr = video_renderer.VideoRenderer(output_dir=base_output_dir, width=1920, height=1080)
                main_video_path = os.path.join(base_output_dir, f"main_{timestamp}.mp4")
                pm = rr.render_video(script_json, title, main_video_path)
                
                if pm and os.path.exists(pm):
                    desc = f"{yt_meta.get('description','')}\n\nğŸš€ Full Story: {main_link}\n\n#{category.replace(' ','')}"
                    vid_main, _ = youtube_manager.upload_video_to_youtube(
                        pm, yt_meta.get('title',title)[:100], desc, yt_meta.get('tags',[])
                    )
                    if vid_main:
                        vid_html = f'<div class="video-container" style="position:relative;padding-bottom:56.25%;margin:35px 0;border-radius:12px;overflow:hidden;box-shadow:0 4px 12px rgba(0,0,0,0.1);"><iframe style="position:absolute;top:0;left:0;width:100%;height:100%;" src="https://www.youtube.com/embed/{vid_main}" frameborder="0" allowfullscreen></iframe></div>'
            except Exception as e:
                log(f"      âš ï¸ Main Video Error: {e}")

            # --- Short Video ---
            log(f"      ğŸ¬ Rendering Short Video...")
            try:
                rs = video_renderer.VideoRenderer(output_dir=base_output_dir, width=1080, height=1920)
                short_video_path = os.path.join(base_output_dir, f"short_{timestamp}.mp4")
                ps = rs.render_video(script_json, title, short_video_path)
                
                if ps and os.path.exists(ps):
                    fb_path = ps
                    vid_short, _ = youtube_manager.upload_video_to_youtube(
                        ps, f"{yt_meta.get('title',title)[:90]} #Shorts", desc, yt_meta.get('tags',[])+['shorts']
                    )
            except Exception as e:
                log(f"      âš ï¸ Short Video Error: {e}")

        else:
            log(f"      âŒ Failed to extract script after 3 attempts.")

    except Exception as e:
        log(f"âš ï¸ Multimedia Process Error: {e}")
        import traceback
        traceback.print_exc()
    
    # =====================================================
    # STEP 4: PUBLISHING
    # =====================================================
    log("   ğŸš€ Publishing to Blogger...")
    
    # --- NEW: AUTHOR BOX INJECTION ---
    # Ù‡Ø°Ø§ Ø§Ù„ØµÙ†Ø¯ÙˆÙ‚ Ø³ÙŠØ¸Ù‡Ø± ÙÙŠ Ù†Ù‡Ø§ÙŠØ© ÙƒÙ„ Ù…Ù‚Ø§Ù„
    author_box = """
    <div style="margin-top:40px; padding:25px; background:#f4f6f8; border-radius:12px; display:flex; align-items:center; border:1px solid #e1e4e8;">
        <img src="https://blogger.googleusercontent.com/img/a/AVvXsEiBbaQkbZWlda1fzUdjXD69xtyL8TDw44wnUhcPI_l2drrbyNq-Bd9iPcIdOCUGbonBc43Ld8vx4p7Zo0DxsM63TndOywKpXdoPINtGT7_S3vfBOsJVR5AGZMoE8CJyLMKo8KUi4iKGdI023U9QLqJNkxrBxD_bMVDpHByG2wDx_gZEFjIGaYHlXmEdZ14=s791" 
             style="width:70px; height:70px; border-radius:50%; margin-right:15px; border:2px solid #fff; box-shadow:0 2px 5px rgba(0,0,0,0.1);" alt="Yousef Sameer">
        <div>
            <h4 style="margin:0; font-size:18px; color:#2c3e50;">Tech Reviewer</h4>
            <p style="margin:5px 0 0; font-size:14px; color:#666; line-height:1.4;">
                I test AI tools so you don't have to break your device. 
                <br><strong>Brutally honest reviews. No fluff.</strong>
            </p>
        </div>
    </div>
    """
    
    # Ø¯Ù…Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰: Ø§Ù„Ù…Ù‚Ø§Ù„ + ØµÙ†Ø¯ÙˆÙ‚ Ø§Ù„ÙƒØ§ØªØ¨ + Ø§Ù„Ù…ØµØ§Ø¯Ø± (Ø§Ù„Ù…ØµØ§Ø¯Ø± Ù…ÙˆØ¬ÙˆØ¯Ø© Ø£ØµÙ„Ø§Ù‹ ÙÙŠ content_html Ù…Ù† Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª)
    # ... (Ø¯Ø§Ø®Ù„ run_pipeline) ...

    # =====================================================
    # ğŸ› ï¸ FIX BROKEN LINKS (CRITICAL)
    # =====================================================
    log("   ğŸ”§ Sanitizing HTML links...")
    
    # 1. Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù‡Ø±ÙˆØ¨ Ø§Ù„Ø®Ø§Ø·Ø¦ (Escaped Quotes) Ø§Ù„Ø°ÙŠ ÙŠØ¶Ø¹Ù‡ Gemini Ø£Ø­ÙŠØ§Ù†Ø§Ù‹
    # ÙŠØ­ÙˆÙ„ href=\"https...\" Ø¥Ù„Ù‰ href="https..."
    content_html = content_html.replace('href=\\"', 'href="').replace('\\">', '">')
    
    # 2. Ø¥Ø²Ø§Ù„Ø© Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªÙ†ØµÙŠØµ Ø§Ù„Ù…Ø²Ø¯ÙˆØ¬Ø© (Double Quotes) Ø§Ù„ØªÙŠ ØªØ³Ø¨Ø¨ Ø®Ø·Ø£ 404
    # ÙŠØ­ÙˆÙ„ href=""https..."" Ø¥Ù„Ù‰ href="https..."
    content_html = content_html.replace('href=""', 'href="').replace('"" target', '" target')
    
    # 3. ØªÙ†Ø¸ÙŠÙ Ù‚ÙˆÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Regex Ù„Ø£ÙŠ Ø­Ø§Ù„Ø§Øª Ù…Ø¹Ù‚Ø¯Ø© Ø£Ø®Ø±Ù‰
    # ÙŠØ¨Ø­Ø« Ø¹Ù† Ø£ÙŠ Ø±Ø§Ø¨Ø· ÙŠØ¨Ø¯Ø£ Ø¨Ù€ http ÙˆÙŠØ­Ø§ØµØ±Ù‡ Ø¹Ù„Ø§Ù…Ø§Øª ØªÙ†ØµÙŠØµ ÙÙˆØ¶ÙˆÙŠØ©
    content_html = re.sub(r'href=["\']\\?["\']?(http[^"\']+)\\?["\']?["\']', r'href="\1"', content_html)
    
    # =====================================================

    # Ø§Ù„Ø¢Ù† Ù†Ø¯Ù…Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù†Ø¸ÙŠÙ Ù…Ø¹ ØµÙ†Ø¯ÙˆÙ‚ Ø§Ù„ÙƒØ§ØªØ¨
    final_content_with_author = content_html + author_box
    

    # ---------------------------------

    full_body = ARTICLE_STYLE
    
    if img_url: 
        alt_text = seo_data.get("imageAltText", title)
        full_body += f'<div class="separator" style="clear:both;text-align:center;margin-bottom:30px;"><a href="{img_url}"><img src="{img_url}" alt="{alt_text}" style="max-width:100%; border-radius:10px; box-shadow:0 5px 15px rgba(0,0,0,0.1);" /></a></div>'
    
    if vid_html: full_body += vid_html
    
    # Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…ØªØºÙŠØ± Ø§Ù„Ø¬Ø¯ÙŠØ¯
    full_body += final_content_with_author
    
    # ... (Ø¨Ø§Ù‚ÙŠ ÙƒÙˆØ¯ Ø§Ù„Ù†Ø´Ø± ÙƒÙ…Ø§ Ù‡Ùˆ) ...
 
    
    if 'schemaMarkup' in final:
        try: full_body += f'\n<script type="application/ld+json">\n{json.dumps(final["schemaMarkup"])}\n</script>'
        except: pass
    
    published_url = publish_post(title, full_body, [category, "Tech News", "Explainers"])
    
    # =====================================================
    # STEP 5: DISTRIBUTION & UPDATES
    # =====================================================
    if published_url:
        log(f"âœ… PUBLISHED: {published_url}")
        update_kg(title, published_url, category)
        
        # 1. ØªØ­Ø¯ÙŠØ« Ø§Ù„ÙˆØµÙ ÙÙŠ ÙŠÙˆØªÙŠÙˆØ¨
        new_desc = f"{yt_meta.get('description','')}\n\nğŸ‘‡ READ THE FULL ARTICLE HERE:\n{published_url}\n\n#AI #TechNews"
        if vid_main: youtube_manager.update_video_description(vid_main, new_desc)
        if vid_short: youtube_manager.update_video_description(vid_short, new_desc)
        
        # 2. Ø§Ù„Ù†Ø´Ø± Ø¹Ù„Ù‰ ÙÙŠØ³Ø¨ÙˆÙƒ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… social_manager
        try:
            log("   ğŸ“¢ Distributing to Facebook...")
            
            # Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ù„Ù„Ø±ÙŠÙ„Ø² (Reels) Ù„Ø£Ù†Ù‡Ø§ ØªØ¬Ù„Ø¨ ØªÙØ§Ø¹Ù„Ø§Ù‹ Ø£ÙƒØ¨Ø±
            if fb_path and os.path.exists(fb_path): 
                fb_text = f"{fb_cap}\n\nRead more: {published_url}\n\n#AI"
                # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ø¯Ø§Ù„Ø© Ù…Ù† social_manager
                social_manager.post_reel_to_facebook(fb_path, fb_text)
            
            # Ø¥Ø°Ø§ Ù„Ù… ÙŠÙˆØ¬Ø¯ ÙÙŠØ¯ÙŠÙˆØŒ Ù†Ù†Ø´Ø± Ø§Ù„ØµÙˆØ±Ø© ÙˆØ§Ù„Ù…Ù‚Ø§Ù„
            elif img_url:
                social_manager.distribute_content(f"{fb_cap}\n\nğŸ‘‡ Read Article:\n{published_url}", published_url, img_url)
                
        except Exception as e:
            log(f"   âš ï¸ Social Distribution Error: {e}")
    else:
        log("âŒ Blogger Publish Failed.")

# ==============================================================================
# 7. MAIN
# ==============================================================================

def main():
    try:
        with open('config_advanced.json','r') as f: cfg = json.load(f)
    except:
        log("âŒ No Config.")
        return
    
    cat = random.choice(list(cfg['categories'].keys()))
    run_pipeline(cat, cfg, mode="trending")
    perform_maintenance_cleanup()
    log("âœ… Finished.")

if __name__ == "__main__":
    main()
