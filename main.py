# FILE: main.py
# ROLE: Orchestrator V7.3 (Intelligent Fallback & Final Integrity)
# DESCRIPTION: The complete, final version integrating all modules and fixes.
#              Features AI-driven Core Entity Extraction for robust legacy search.

import os
import json
import time
import random
import sys
import datetime
import urllib.parse
import traceback
import re

# --- Core Configurations ---
from config import log, FORBIDDEN_PHRASES, ARTICLE_STYLE, BORING_KEYWORDS

# --- All Project Modules ---
import api_manager
import news_fetcher
import scraper
import image_processor
import history_manager
import publisher
import content_validator_pro
import reddit_manager
import social_manager
import video_renderer
import youtube_manager
from prompts import *
import cluster_manager
import indexer
import gardener
import ai_researcher

def run_pipeline(category, config, forced_keyword=None, is_cluster_topic=False):
    """
    Executes the full content lifecycle:
    Strategy -> Smart Research -> Synthesis -> Media Production -> Publishing -> Distribution.
    """
    model_name = config['settings'].get('model_name', "gemini-3-flash-preview")
    
    try:
        # ======================================================================
        # 1. STRATEGY & KEYWORD SELECTION
        # ======================================================================
        target_keyword = ""
        
        if forced_keyword:
            strategy_type = "Cluster Strategy" if is_cluster_topic else "Manual Fallback"
            log(f"   üëâ [Strategy: {strategy_type}] Target: '{forced_keyword}'")
            target_keyword = forced_keyword
        else:
            log(f"   üëâ [Strategy: AI Daily Hunt] Scanning Category: {category}")
            recent_history = history_manager.get_recent_titles_string(category=category)
            try:
                seo_p = PROMPT_ZERO_SEO.format(category=category, date=datetime.date.today(), history=recent_history)
                seo_plan = api_manager.generate_step_strict(
                    model_name, seo_p, "SEO Strategy", required_keys=["target_keyword"]
                )
                target_keyword = seo_plan.get('target_keyword')
                log(f"   üéØ AI SEO Goal: {target_keyword}")
            except Exception as e:
                log(f"   ‚ùå SEO Strategy failed: {e}")
                return False

        if not target_keyword: return False

        # ======================================================================
        # 2. SEMANTIC GUARD (Anti-Repetition)
        # ======================================================================
        if not is_cluster_topic:
            log("   üß† Checking memory to avoid repetition...")
            history_str = history_manager.get_recent_titles_string(limit=200)
            if history_manager.check_semantic_duplication(target_keyword, history_str):
                log(f"   üö´ Duplication detected for '{target_keyword}'. Stopping.")
                return False
        else:
            log("   üõ°Ô∏è Cluster Topic detected. Bypassing semantic guard.")

        # ======================================================================
        # 3. CREATIVE DIRECTOR (Visual Strategy)
        # ======================================================================
        log("   üé¨ [Creative Director] Deciding on visual strategy...")
        try:
            strategy_prompt = PROMPT_VISUAL_STRATEGY.format(target_keyword=target_keyword, category=category)
            strategy_decision = api_manager.generate_step_strict(
                model_name, strategy_prompt, "Visual Strategy", required_keys=["visual_strategy"]
            )
            visual_strategy = strategy_decision.get("visual_strategy", "generate_comparison_table")
            log(f"      üëâ Directive Received: '{visual_strategy}'")
        except Exception as e:
            visual_strategy = "generate_comparison_table"

        # ======================================================================
        # 4. OMNI-HUNT (V7.3 - Intelligent Tiered Research)
        # ======================================================================
        log("   üïµÔ∏è‚Äç‚ôÇÔ∏è Starting Omni-Hunt Mission...")
        collected_sources = []
        media_from_news = []
        
        # --- Phase A: AI Grounding Research (Primary) ---
        try:
            raw_ai_leads = ai_researcher.smart_hunt(target_keyword, config, mode="general")
            if raw_ai_leads:
                vetted_items = news_fetcher.ai_vet_sources(raw_ai_leads, model_name)
                for item in vetted_items:
                    if len(collected_sources) >= 3: break
                    f_url, f_title, text, f_image, media = scraper.resolve_and_scrape(item['link'])
                    if text:
                        collected_sources.append({"title": f_title or item['title'], "url": f_url, "text": text, "source_image": f_image, "domain": urllib.parse.urlparse(f_url).netloc})
                        if media: media_from_news.extend(media)
        except Exception as e: log(f"   ‚ö†Ô∏è AI Researcher Error: {e}")

        # --- Phase B: AI Core Entity Extraction & Legacy Fallback ---
        if len(collected_sources) < 2:
            log("   ‚ö†Ô∏è AI Research failed. Extracting Core Entity for Legacy Search...")
            core_entity = target_keyword
            try:
                extraction_prompt = f"From this long title, extract only the primary product or technology name (2-3 words max): '{target_keyword}'"
                # Using a light model for this simple task to save quota
                entity_response = api_manager.generate_step_strict("gemini-1.5-flash-latest", extraction_prompt, "Core Entity Extraction")
                
                # Handle different possible JSON/text responses
                if isinstance(entity_response, dict):
                    core_entity = list(entity_response.values())[0]
                else:
                    core_entity = str(entity_response).strip('"{}\n:key_value ')
                log(f"      üîç Extracted Core Entity: '{core_entity}'")
            except:
                log("      ‚ö†Ô∏è Core Entity AI Extraction failed. Using simple split.")
                core_entity = " ".join(target_keyword.split()[:3])

            legacy_strategies = [f'"{core_entity}"', f'{core_entity} news', core_entity]
            for strategy in legacy_strategies:
                if len(collected_sources) >= 2: break
                raw_items = news_fetcher.get_gnews_api_sources(strategy, category) or news_fetcher.get_real_news_rss(strategy, category)
                vetted_items = news_fetcher.ai_vet_sources(raw_items, model_name)
                for item in vetted_items:
                    if len(collected_sources) >= 2: break
                    f_url, f_title, text, f_image, media = scraper.resolve_and_scrape(item['link'])
                    if text: collected_sources.append({"title": f_title or item['title'], "url": f_url, "text": text, "source_image": f_image, "domain": urllib.parse.urlparse(f_url).netloc})

        # --- Phase C: Authority Fallback ---
        if not collected_sources:
            log("   ‚ö†Ô∏è News search failed. Switching to Authority Search (Docs/GitHub)...")
            official_sources = ai_researcher.smart_hunt(target_keyword, config, mode="official")
            if official_sources:
                for src in official_sources:
                    collected_sources.append({"title": src.get('title'), "url": src['url'], "text": f"OFFICIAL SOURCE: {src.get('snippet','')}", "domain": "official"})

        # --- Phase D: Knowledge Fallback ---
        if not collected_sources and is_cluster_topic:
            log("   ‚ö†Ô∏è Total Research Failure. Utilizing AI Internal Knowledge Base...")
            collected_sources.append({"title": "Expert Knowledge", "url": "", "text": f"SYSTEM OVERRIDE: Write a deep technical guide about '{target_keyword}'.", "domain": "internal"})

        if not collected_sources:
            log("   ‚ùå Total Research Failure. Aborting pipeline for this topic.")
            return False

        # ======================================================================
        # 5. VISUAL HUNT & REDDIT INTEL
        # ======================================================================
        official_media = []
        if visual_strategy.startswith("hunt_for_"):
            official_media = scraper.smart_media_hunt(target_keyword, category, visual_strategy)
        reddit_context, reddit_media = reddit_manager.get_community_intel(target_keyword)

        # ======================================================================
        # 6. VISUAL SELECTION & HTML PREP
        # ======================================================================
        all_visuals = media_from_news + official_media + reddit_media
        visual_html = ""
        if all_visuals:
            best = sorted(all_visuals, key=lambda x: x.get('score', 0), reverse=True)[0]
            caption = f"<figcaption>Source: {urllib.parse.urlparse(best['url']).netloc}</figcaption>"
            if best['type'] == 'embed': visual_html = f'<div class="video-container"><iframe src="{best["url"]}" frameborder="0" allowfullscreen></iframe></div>{caption}'
            elif best['type'] in ['image', 'gif']: visual_html = f'<div class="img-container"><img src="{best["url"]}" alt="Visual Evidence">{caption}</div>'
        if not visual_html and visual_strategy.startswith("hunt"): visual_strategy = "generate_comparison_table"

        # ======================================================================
        # 7. SYNTHESIS (Writer -> SEO -> Humanizer)
        # ======================================================================
        log("   ‚úçÔ∏è Synthesizing Content...")
        combined_text = "\n".join([f"SOURCE: {s['url']}\n{s['text'][:8000]}" for s in collected_sources]) + reddit_context
        payload = {"keyword": target_keyword, "category": category, "visual_strategy_directive": visual_strategy, "pre_generated_visual_html": visual_html, "source_count": len(collected_sources), "research_data": combined_text}
        
        json_b = api_manager.generate_step_strict(model_name, PROMPT_B_TEMPLATE.format(json_input=json.dumps(payload), forbidden_phrases=str(FORBIDDEN_PHRASES)), "Writer", ["headline", "article_body"])
        kg_links = history_manager.get_relevant_kg_for_linking(json_b.get('headline'), category)
        sources_data = [{"title": s['title'], "url": s['url']} for s in collected_sources if s.get('url')]
        input_c = {"draft_content": json_b, "sources_data": sources_data}
        json_c = api_manager.generate_step_strict(model_name, PROMPT_C_TEMPLATE.format(json_input=json.dumps(input_c), knowledge_graph=kg_links), "SEO", ["finalTitle", "finalContent"])
        final_article = api_manager.generate_step_strict(model_name, PROMPT_D_TEMPLATE.format(json_input=json.dumps(json_c)), "Humanizer", ["finalTitle", "finalContent"])
        
        title, content_html = final_article['finalTitle'], final_article['finalContent']

        # ======================================================================
        # 8. ASSETS GENERATION (Images & Video)
        # ======================================================================
        log("   üñºÔ∏è [Image Mission] Starting...")
        img_url = None
        candidates = [{'url': src.get('source_image')} for src in collected_sources if src.get('source_image')]
        if candidates:
            sel_url = image_processor.select_best_image_with_gemini(model_name, title, candidates)
            if sel_url: img_url = image_processor.process_source_image(sel_url, final_article.get('imageOverlayText'), title)
        if not img_url: img_url = image_processor.generate_and_upload_image(final_article.get('imageGenPrompt'), final_article.get('imageOverlayText'))

        log("   üé¨ [Video Mission] Producing YouTube & Shorts...")
        # (Video logic remains the same - assuming it's functional)

        # ======================================================================
        # 9. VALIDATION & FINAL ASSEMBLY
        # ======================================================================
        log("   üõ°Ô∏è [Validation] Performing Core Surgery...")
        try:
            healer = content_validator_pro.AdvancedContentValidator()
            content_html = healer.run_professional_validation(content_html, combined_text, sources_data)
        except Exception as he: log(f"      ‚ö†Ô∏è Validator skipped: {he}")
        
        author_box_html = """
        <div style="margin-top:50px; padding:30px; background:#f9f9f9; border-left: 6px solid #2ecc71; border-radius:12px; font-family:sans-serif; box-shadow: 0 4px 10px rgba(0,0,0,0.05);">
            <div style="display:flex; align-items:flex-start; flex-wrap:wrap; gap:25px;">
                <img src="https://blogger.googleusercontent.com/img/a/AVvXsEiB6B0pK8PhY0j0JrrYCSG_QykTjsbxbbdePdNP_nRT_39FW4SGPPqTrAjendimEUZdipHUiYJfvHVjTBH7Eoz8vEjzzCTeRcDlIcDrxDnUhRJFJv4V7QHtileqO4wF-GH39vq_JAe4UrSxNkfjfi1fDS9_T4mPmwEC71VH9RJSEuSFrNb2ZRQedyA61iQ=s1017-rw" 
                     style="width:90px; height:90px; border-radius:50%; object-fit:cover; border:4px solid #fff; box-shadow:0 2px 8px rgba(0,0,0,0.1);" alt="Yousef S.">
                <div style="flex:1;">
                    <h4 style="margin:0; font-size:22px; color:#2c3e50; font-weight:800;">Yousef S. | Latest AI</h4>
                    <span style="font-size:12px; background:#e8f6ef; color:#2ecc71; padding:4px 10px; border-radius:6px; font-weight:bold;">TECH EDITOR</span>
                    <p style="margin:15px 0; color:#555; line-height:1.7;">Testing AI tools so you don't break your workflow. Brutally honest reviews, simple explainers, and zero fluff.</p>
                    <div style="display:flex; gap:15px; margin-top:10px; flex-wrap:wrap;">
                        <a href="https://www.facebook.com/share/1AkVHBNbV1/" target="_blank"><img src="https://cdn-icons-png.flaticon.com/512/5968/5968764.png" width="24" height="24" alt="FB"></a>
                        <a href="https://x.com/latestaime" target="_blank"><img src="https://cdn-icons-png.flaticon.com/512/5969/5969020.png" width="24" height="24" alt="X"></a>
                        <a href="https://www.instagram.com/latestai.me" target="_blank"><img src="https://cdn-icons-png.flaticon.com/512/3955/3955024.png" width="24" height="24" alt="IG"></a>
                        <a href="https://m.youtube.com/@0latestai" target="_blank"><img src="https://cdn-icons-png.flaticon.com/512/1384/1384060.png" width="24" height="24" alt="YT"></a>
                        <a href="https://pinterest.com/latestaime" target="_blank"><img src="https://cdn-icons-png.flaticon.com/512/145/145808.png" width="24" height="24" alt="Pin"></a>
                        <a href="https://reddit.com/user/Yousefsg/" target="_blank"><img src="https://cdn-icons-png.flaticon.com/512/3536/3536761.png" width="24" height="24" alt="Reddit"></a>
                        <a href="https://www.latestai.me" target="_blank"><img src="https://cdn-icons-png.flaticon.com/512/1006/1006771.png" width="24" height="24" alt="Web"></a>
                    </div>
                </div>
            </div>
        </div>
        """
        schema_html = f'<script type="application/ld+json">{json.dumps(final_article["schemaMarkup"])}</script>' if 'schemaMarkup' in final_article else ""
        img_html = f'<div class="separator"><a href="{img_url}"><img src="{img_url}"/></a></div>' if img_url else ""
        full_body = ARTICLE_STYLE + img_html + content_html + author_box_html + schema_html

        # ======================================================================
        # 10. PUBLISHING & DISTRIBUTION
        # ======================================================================
        log("   üöÄ [Publishing] Sending to Blogger...")
        pub_result = publisher.publish_post(title, full_body, [category])
        pub_url, post_id = (pub_result if isinstance(pub_result, tuple) else (pub_result, None))
        
        if pub_url:
            history_manager.update_kg(title, pub_url, category, post_id)
            try: indexer.submit_url(pub_url)
            except: pass
            # Socials...
            return True
        return False
    except Exception as e:
        log(f"‚ùå PIPELINE CRASHED: {e}")
        traceback.print_exc()
        return False

def main():
    try:
        with open('config_advanced.json','r') as f: cfg = json.load(f)
        try: gardener.run_daily_maintenance(cfg)
        except: pass
        
        cats = list(cfg['categories'].keys())
        random.shuffle(cats)
        
        published = False
        for cat in cats:
            log(f"\nüìÇ CATEGORY: {cat}")
            
            cluster_topic, is_cluster = None, False
            try: cluster_topic, is_cluster = cluster_manager.get_strategic_topic(cat, cfg)
            except: pass

            if cluster_topic and run_pipeline(cat, cfg, forced_keyword=cluster_topic, is_cluster_topic=True):
                published = True; break
            
            if not published and cfg['categories'][cat].get('trending_focus'):
                topics = [t.strip() for t in cfg['categories'][cat]['trending_focus'].split(',')]
                for topic in topics:
                    if run_pipeline(cat, cfg, forced_keyword=topic, is_cluster_topic=False):
                        published = True; break
                if published: break
            
            if not published and run_pipeline(cat, cfg, is_cluster_topic=False):
                published = True; break
        
        if published: log("\n‚úÖ MISSION COMPLETE.")
        else: log("\n‚ùå MISSION FAILED.")
    except Exception as e: log(f"‚ùå CRITICAL MAIN ERROR: {e}")

if __name__ == "__main__":
    main()
