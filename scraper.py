import time
import random
import urllib.parse
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
import trafilatura
from bs4 import BeautifulSoup
from config import log, USER_AGENTS

# ==============================================================================
# 1. CONFIGURATION & BLACKLISTS
# ==============================================================================

# Ù‚Ø§Ø¦Ù…Ø© Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ù„Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø¥Ø®Ø¨Ø§Ø±ÙŠØ© (Ù†Ø­Ù† Ù†Ø±ÙŠØ¯ Ø§Ù„Ù…ØµØ¯Ø± Ø§Ù„Ø±Ø³Ù…ÙŠ ÙÙ‚Ø· Ø¹Ù†Ø¯ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ³Ø§Ø¦Ø·)
NEWS_DOMAINS_BLACKLIST = [
    "techcrunch", "theverge", "engadget", "wired", "cnet", "forbes", 
    "businessinsider", "nytimes", "wsj", "bloomberg", "reuters", "cnn",
    "bbc", "medium", "reddit", "youtube", "wikipedia", "latestai", "techradar"
]

# ==============================================================================
# 2. HELPER FUNCTIONS (SMART CONTEXT)
# ==============================================================================

def get_smart_query_by_category(keyword, category):
    """
    ØªÙˆÙ„ÙŠØ¯ Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¨Ø­Ø« Ø°ÙƒÙŠ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙØ¦Ø© Ù„Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ 'Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø¹Ù…ÙŠÙ‚Ø©' Ø§Ù„ØµØ­ÙŠØ­Ø©.
    """
    base = f"{keyword} official"
    
    if "Video" in category or "Media" in category:
        return f"{base} demo showcase video"
    elif "Code" in category or "Dev" in category or "Tools" in category:
        return f"{base} documentation features blog"
    elif "Robotics" in category or "Hardware" in category:
        return f"{base} reveal video demonstration"
    elif "Business" in category or "Income" in category:
        return f"{base} pricing case study press release"
    else:
        return f"{base} announcement blog"

def is_official_looking_url(url, keyword):
    """
    ØªØ­Ù‚Ù‚ Ø°ÙƒÙŠ: Ù‡Ù„ Ù‡Ø°Ø§ Ø§Ù„Ø±Ø§Ø¨Ø· ÙŠØ¨Ø¯Ùˆ ÙƒÙ…ØµØ¯Ø± Ø±Ø³Ù…ÙŠØŸ
    """
    try:
        domain = urllib.parse.urlparse(url).netloc.lower()
        # Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù…Ø¹Ø±ÙˆÙØ©
        if any(news in domain for news in NEWS_DOMAINS_BLACKLIST):
            return False
        return True
    except: return False

def extract_element_context(element):
    """
    ÙŠØ³ØªØ®Ø±Ø¬ ÙˆØµÙØ§Ù‹ Ù†ØµÙŠØ§Ù‹ Ù„Ù„ÙˆØ³ÙŠØ· (ØµÙˆØ±Ø©/ÙÙŠØ¯ÙŠÙˆ) Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙÙŠ Ø§Ù„Ø§Ø®ØªÙŠØ§Ø±.
    """
    context = []
    
    # 1. Ø§Ù„Ù†Øµ Ø§Ù„Ø¨Ø¯ÙŠÙ„ ÙˆØ§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†
    if element.get('alt'): context.append(element['alt'])
    if element.get('title'): context.append(element['title'])
    if element.get('aria-label'): context.append(element['aria-label'])
    
    # 2. Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø­ÙŠØ· (Ø§Ù„Ø£Ø¨ Ø£Ùˆ Ø§Ù„Ø£Ø® Ø§Ù„Ø³Ø§Ø¨Ù‚)
    parent = element.parent
    if parent:
        text = parent.get_text(strip=True)[:150] # Ø£ÙˆÙ„ 150 Ø­Ø±Ù Ù…Ù† Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø­ÙŠØ·
        if text: context.append(text)
        
    return " | ".join(context) if context else "No description available"

# ==============================================================================
# 3. CORE FUNCTION: SMART MEDIA HUNT (THE VISUAL DETECTIVE)
# ==============================================================================

def smart_media_hunt(target_keyword, category):
    """
    Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ø°ÙƒÙŠ: ÙŠØ¨Ø­Ø« Ø¹Ù† Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙŠØ³Ø­Ø¨ Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„ÙØ¦Ø© Ù…Ø¹ Ø§Ù„Ø³ÙŠØ§Ù‚.
    """
    search_query = get_smart_query_by_category(target_keyword, category)
    log(f"      ğŸ•µï¸â€â™‚ï¸ Smart Sniper: Hunting for official media using query: '{search_query}'...")

    chrome_options = Options()
    chrome_options.add_argument("--headless=new")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument(f'user-agent={random.choice(USER_AGENTS)}')
    
    # Ù…Ù„Ø§Ø­Ø¸Ø©: Ù‡Ù†Ø§ Ù„Ø§ Ù†Ø­Ø¸Ø± Ø§Ù„ØµÙˆØ± ØªÙ…Ø§Ù…Ø§Ù‹ Ù„Ø£Ù†Ù†Ø§ Ù†Ø±ÙŠØ¯ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯Ù‡Ø§ ÙÙŠ DOMØŒ 
    # Ù„ÙƒÙ†Ù†Ø§ Ù†Ø¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø±Ø¹Ø©.
    
    driver = None
    found_media = []
    
    try:
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        driver.set_page_load_timeout(60)
        
        # 1. Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø¬ÙˆØ¬Ù„
        driver.get(f"https://www.google.com/search?q={urllib.parse.quote(search_query)}")
        time.sleep(2)
        
        # 2. Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø±Ø§Ø¨Ø· "Ø§Ù„Ø¹Ù…ÙŠÙ‚" Ø§Ù„Ù…Ù†Ø§Ø³Ø¨
        links = driver.find_elements(By.CSS_SELECTOR, 'div.g a')
        target_url = None
        
        for link in links[:6]: # Ù†ÙØ­Øµ Ø£ÙˆÙ„ 6 Ù†ØªØ§Ø¦Ø¬
            url = link.get_attribute('href')
            if url and is_official_looking_url(url, target_keyword):
                target_url = url
                break
        
        if not target_url:
            log("      âš ï¸ No official-looking source found via Smart Hunt.")
            return []

        log(f"      ğŸ¯ Locked on Target: {target_url}")
        driver.get(target_url)
        time.sleep(5) # Ø§Ù†ØªØ¸Ø§Ø± ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ (JS)
        
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        # Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„Ø¥Ø´Ø§Ø±Ø§Øª
        positive_signals = ["demo", "showcase", "tutorial", "interface", "example", "generated", "result", "how to", "workflow", "reveal", "trailer"]
        negative_signals = ["logo", "icon", "background", "hero", "banner", "loader", "spinner", "team", "hiring", "avatar", "profile", "footer"]

        # 3. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ø¨Ø°ÙƒØ§Ø¡
        
        # Ø£) Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª (MP4/WebM)
        for video in soup.find_all('video'):
            src = video.get('src')
            if not src:
                src = video.find('source', src=True)
                if src: src = src['src']
            
            if src and (src.endswith('.mp4') or src.endswith('.webm')):
                if src.startswith('/'): src = urllib.parse.urljoin(target_url, src)
                
                context = extract_element_context(video).lower()
                if any(bad in context or bad in src for bad in negative_signals): continue
                
                found_media.append({
                    "type": "video", 
                    "url": src, 
                    "description": context,
                    "score": sum(1 for sig in positive_signals if sig in context) + 2 # Video gets bonus score
                })

        # Ø¨) Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† YouTube/Vimeo Embeds
        for iframe in soup.find_all('iframe'):
            src = iframe.get('src', '')
            if 'youtube.com/embed' in src or 'player.vimeo.com' in src:
                context = extract_element_context(iframe).lower()
                found_media.append({
                    "type": "embed", 
                    "url": src, 
                    "description": context,
                    "score": sum(1 for sig in positive_signals if sig in context) + 1
                })

        # Ø¬) Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† GIFs (Ù„Ù„Ø£Ø¯ÙˆØ§Øª ÙˆØ§Ù„Ø´Ø±ÙˆØ­Ø§Øª)
        for img in soup.find_all('img', src=True):
            src = img['src']
            if src.endswith('.gif'):
                if src.startswith('/'): src = urllib.parse.urljoin(target_url, src)
                
                context = extract_element_context(img).lower()
                if any(bad in context or bad in src for bad in negative_signals): continue
                if "loading" in src or "pixel" in src: continue

                found_media.append({
                    "type": "gif", 
                    "url": src, 
                    "description": context,
                    "score": sum(1 for sig in positive_signals if sig in context)
                })
                    
        # Ø¯) Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ØµÙˆØ± Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø¬ÙˆØ¯Ø© (ÙÙ‚Ø· Ù„Ù„Ø±ÙˆØ¨ÙˆØªØ§Øª ÙˆØ§Ù„Ù‡Ø§Ø±Ø¯ÙˆÙŠØ±)
        if "Robotics" in category or "Hardware" in category:
            for img in soup.find_all('img', src=True):
                src = img['src']
                # Ù†Ø¨Ø­Ø« Ø¹Ù† ØµÙˆØ± ÙƒØ¨ÙŠØ±Ø© Ø£Ùˆ Hero Images
                if 'hero' in str(img.get('class', '')) or 'banner' in str(img.get('class', '')):
                     if src.startswith('/'): src = urllib.parse.urljoin(target_url, src)
                     found_media.append({"type": "image", "url": src, "description": "Hero Product Shot", "score": 1})

    except Exception as e:
        log(f"      âš ï¸ Smart Hunt Error: {e}")
    finally:
        if driver: 
            try: driver.quit()
            except: pass
        
    # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø­Ø³Ø¨ Ø§Ù„Ù†Ù‚Ø§Ø· (Score)
    found_media.sort(key=lambda x: x['score'], reverse=True)
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±
    unique_media = list({v['url']:v for v in found_media}.values())
    
    log(f"      ğŸ“¸ Extracted {len(unique_media)} context-verified assets from official source.")
    return unique_media[:3] # Ù†ÙƒØªÙÙŠ Ø¨Ø£ÙØ¶Ù„ 3

# ==============================================================================
# 4. EXISTING FUNCTION: NEWS SCRAPER (FOR TEXT)
# ==============================================================================

def resolve_and_scrape(google_url):
    log(f"      ğŸ•µï¸â€â™‚ï¸ Selenium: Resolving Link & Hunting Image...")
    
    chrome_options = Options()
    chrome_options.add_argument("--headless=new")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument(f'user-agent={random.choice(USER_AGENTS)}')
    chrome_options.add_argument("--mute-audio") 

    # Optimization: Block images/css for text scraping speed
    prefs = {
        "profile.managed_default_content_settings.images": 2,
        "profile.managed_default_content_settings.stylesheets": 2,
        "profile.managed_default_content_settings.fonts": 2,
        "profile.default_content_setting_values.notifications": 2,
        "profile.managed_default_content_settings.popups": 2,
    }
    chrome_options.add_experimental_option("prefs", prefs)

    driver = None
    try:
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        driver.set_page_load_timeout(90) 
        
        driver.get(google_url)
        
        start_wait = time.time()
        final_url = google_url
        while time.time() - start_wait < 15: 
            current = driver.current_url
            if "news.google.com" not in current and "google.com" not in current:
                final_url = current
                break
            time.sleep(1) 
        
        final_title = driver.title
        page_source = driver.page_source
        
        bad_segments = ["/video/", "/watch", "/gallery/", "/photos/", "youtube.com"]
        if any(seg in final_url.lower() for seg in bad_segments):
            log(f"      âš ï¸ Skipped Video/Gallery URL: {final_url}")
            return None, None, None, None

        soup = BeautifulSoup(page_source, 'html.parser')
        og_image = None
        try:
            meta_img = soup.find('meta', property='og:image')
            if meta_img: og_image = meta_img.get('content')
            if not og_image:
                meta_img = soup.find('meta', name='twitter:image')
                if meta_img: og_image = meta_img.get('content')
        except: pass

        extracted_text = trafilatura.extract(
            page_source, 
            include_comments=False, 
            include_tables=True,
            favor_precision=True
        )
        
        if extracted_text and len(extracted_text) > 800:
            return final_url, final_title, extracted_text, og_image

        for script in soup(["script", "style", "nav", "footer", "header", "aside", "noscript"]):
            script.extract()
        fallback_text = soup.get_text(" ", strip=True)
        
        if fallback_text and len(fallback_text) > 800:
            return final_url, final_title, fallback_text, og_image
            
        return None, None, None, None

    except Exception as e:
        log(f"      âŒ Selenium Error: {str(e)[:100]}")
        return None, None, None, None
    finally:
        if driver:
            try: driver.quit()
            except: pass
