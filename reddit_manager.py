# FILE: reddit_manager.py
# ROLE: Reddit Intelligence Gatherer (No-API / JSON Trick Edition)
# DESCRIPTION: Finds threads via DuckDuckGo and fetches data using Reddit's public JSON endpoints.
#              Bypasses the need for API Keys and Selenium bloat.

import requests
import re
import time
import random
from duckduckgo_search import DDGS
from config import log

class RedditManager:
    def __init__(self):
        # Ù†Ø³ØªØ®Ø¯Ù… User-Agent ÙŠØ¨Ø¯Ùˆ ÙƒÙ…ØªØµÙØ­ Ø­Ù‚ÙŠÙ‚ÙŠ Ø¬Ø¯Ø§Ù‹ Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø­Ø¸Ø±
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)

    def search_threads(self, keyword, limit=5):
        """
        ÙŠØ¨Ø­Ø« Ø¹Ù† Ø±ÙˆØ§Ø¨Ø· Reddit Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… DuckDuckGo (Ù„Ø£Ù†Ù‡ Ù„Ø§ ÙŠØ­Ø¸Ø± Ø³ÙŠØ±ÙØ±Ø§Øª GitHub).
        """
        log(f"      ğŸ¦† DuckDuckGo: Hunting Reddit threads for '{keyword}'...")
        threads = []
        
        try:
            # Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¨Ø­Ø« ÙŠØ±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù‚Ø§Ø´Ø§Øª ÙˆØ§Ù„ØªØ¬Ø§Ø±Ø¨
            query = f'site:reddit.com "{keyword}" (review OR "is it worth" OR "my experience" OR guide)'
            
            with DDGS() as ddgs:
                # DuckDuckGo Ø³Ø±ÙŠØ¹ Ø¬Ø¯Ø§Ù‹ ÙˆÙ„Ø§ ÙŠØ·Ù„Ø¨ Captcha Ø¹Ø§Ø¯Ø©Ù‹
                results = list(ddgs.text(query, max_results=8))
                
                for r in results:
                    link = r.get('href')
                    title = r.get('title')
                    
                    # ØªØ£ÙƒØ¯ Ø£Ù†Ù‡ Ø±Ø§Ø¨Ø· Ù„Ù…Ù†Ø´ÙˆØ± ÙˆÙ„ÙŠØ³ ØµÙØ­Ø© Ø¹Ø§Ù…Ø©
                    if "/comments/" in link:
                        threads.append({"title": title, "link": link})
                        if len(threads) >= limit: break
            
            return threads
        except Exception as e:
            log(f"      âŒ Search Error: {e}")
            return []

    def extract_thread_data(self, thread_url):
        """
        Ø§Ù„Ø³Ø­Ø± Ù‡Ù†Ø§: Ù†Ø¶ÙŠÙ .json Ù„Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø±Ø§Ø¨Ø· Ù„Ù†Ø­ØµÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù†Ø¸ÙŠÙØ© ØªÙ…Ø§Ù…Ø§Ù‹
        Ø¨Ø¯ÙˆÙ† Ø§Ù„Ø­Ø§Ø¬Ø© Ù„ØªØ­Ù„ÙŠÙ„ HTML Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ø§Ù… Selenium.
        """
        try:
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø±Ø§Ø¨Ø· ÙˆØ¥Ø¶Ø§ÙØ© .json
            clean_url = thread_url.split('?')[0].rstrip('/')
            json_url = f"{clean_url}.json"
            
            # ØªØ£Ø®ÙŠØ± Ø¨Ø³ÙŠØ· Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø´Ùƒ
            time.sleep(random.uniform(1, 2))
            
            resp = self.session.get(json_url, timeout=10)
            
            if resp.status_code == 429:
                log("      âš ï¸ Reddit Rate Limit (429). Skipping this thread.")
                return None
                
            if resp.status_code != 200:
                return None
            
            data = resp.json()
            # Reddit JSON returns a list: [PostData, CommentsData]
            if not isinstance(data, list) or len(data) < 2: return None

            post_info = data[0]['data']['children'][0]['data']
            comments_info = data[1]['data']['children']

            result = {
                "title": post_info.get('title'),
                "subreddit": post_info.get('subreddit'),
                "url": clean_url,
                "media": [],
                "codes": [],
                "insights": []
            }

            # 1. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙŠØ¯ÙŠØ§
            self._extract_media(post_info, result["media"])
            
            # 2. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£ÙƒÙˆØ§Ø¯ Ù…Ù† Ø§Ù„Ù…Ù†Ø´ÙˆØ± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
            self._extract_codes(post_info.get('selftext', ''), result["codes"])

            # 3. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª
            for comment in comments_info[:10]:
                c_data = comment.get('data', {})
                body = c_data.get('body')
                
                if not body or body in ["[deleted]", "[removed]"]: continue
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£ÙƒÙˆØ§Ø¯ Ù…Ù† Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª
                self._extract_codes(body, result["codes"])

                # Ø­ÙØ¸ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø§Ù„Ù…ÙÙŠØ¯Ø©
                if len(body) > 50 and c_data.get('score', 0) > 2:
                    clean_body = re.sub(r'\[(.*?)\]\(.*?\)', r'\1', body) # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
                    result["insights"].append({
                        "author": c_data.get('author', 'user'),
                        "text": clean_body.strip(),
                        "score": c_data.get('score', 0)
                    })

            # ØªØ±ØªÙŠØ¨ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ø£Ù‡Ù…ÙŠØ©
            result["insights"].sort(key=lambda x: x['score'], reverse=True)
            result["insights"] = result["insights"][:5]

            return result

        except Exception as e:
            log(f"      âš ï¸ Failed to parse thread JSON: {e}")
            return None

    def _extract_media(self, post_data, media_list):
        # ØµÙˆØ±
        if post_data.get('url', '').endswith(('.jpg', '.png', '.jpeg', '.gif')):
            media_list.append({"type": "image", "url": post_data.get('url'), "caption": post_data.get('title')})
        
        # ÙÙŠØ¯ÙŠÙˆ
        if post_data.get('is_video') and post_data.get('media'):
            try:
                vid = post_data['media']['reddit_video']['fallback_url']
                media_list.append({"type": "video", "url": vid, "caption": post_data.get('title')})
            except: pass
            
        # Ù…Ø¹Ø±Ø¶ ØµÙˆØ±
        if post_data.get('is_gallery') and post_data.get('media_metadata'):
            for k, v in post_data['media_metadata'].items():
                if v['status'] == 'valid':
                    try:
                        u = v['s']['u'].replace('&amp;', '&')
                        media_list.append({"type": "image", "url": u, "caption": "Gallery"})
                    except: pass

    def _extract_codes(self, text, code_list):
        if not text: return
        matches = re.findall(r'```(?:[a-z]*\n)?(.*?)```', text, re.DOTALL)
        for m in matches:
            c = m.strip()
            if c and c not in code_list: code_list.append(c)

# --- Main Entry Point ---
def get_community_intel(keyword):
    log(f"ğŸ§  [Reddit JSON] Mining discussions for: '{keyword}'...")
    manager = RedditManager()
    
    threads = manager.search_threads(keyword, limit=4)
    if not threads:
        log("   - No threads found via DuckDuckGo.")
        return "", []

    all_data = []
    for t in threads:
        d = manager.extract_thread_data(t['link'])
        if d: all_data.append(d)

    if not all_data: return "", []

    report = "\n\n=== ğŸ“¢ REAL HUMAN EXPERIENCES (REDDIT) ===\n"
    all_media = []
    
    for post in all_data:
        report += f"--- r/{post['subreddit']}: {post['title']} ---\n"
        report += f"URL: {post['url']}\n"
        if post['insights']:
            report += "TOP COMMENTS:\n"
            for c in post['insights']:
                report += f"- u/{c['author']}: \"{c['text'][:300]}...\"\n"
        if post['codes']:
            report += "CODE SNIPPETS FOUND.\n"
        report += "\n"
        all_media.extend(post['media'])

    unique_media = []
    seen = set()
    for m in all_media:
        if m['url'] not in seen:
            unique_media.append(m)
            seen.add(m['url'])

    log(f"   âœ… Gathered intel from {len(all_data)} threads. Found {len(unique_media)} media items.")
    return report, unique_media
