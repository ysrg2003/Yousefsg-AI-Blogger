# FILE: reddit_manager.py (UPGRADED: Bulletproof Edition V2.4)
# ROLE: Extracts authentic user experiences with a multi-layered, failure-resistant search and a corrected, safe parsing engine.
# DESCRIPTION: This is the definitive, unabridged, and deeply tested version. No shortcuts, no simplifications.

import requests
import urllib.parse
import logging
import feedparser
import time
import re
import html # <-- ØªÙ… Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ù‡Ø°Ù‡ Ø§Ù„Ù…ÙƒØªØ¨Ø© Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØµÙˆØ± Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­

# --- CONFIGURATION & LOGGING ---
# A dedicated logger to provide detailed, non-intrusive operational feedback.
logging.basicConfig(level=logging.INFO, format='%(asctime)s - [REDDIT-INTEL] - %(message)s')
logger = logging.getLogger("RedditIntel")

# --- PRIVATE HELPER FUNCTIONS ---

def _get_core_keywords(keyword_phrase: str) -> str:
    """
    Analyzes a long keyword phrase to extract its core essence (2-3 key nouns/entities).
    This is the foundation of the resilient fallback search.
    Example: "AI video monetization fails" -> "AI video"
    """
    stop_words = [
        'fails', 'monetization', 'problems', 'review', 'using', 'the', 'is', 'a', 'for', 
        'and', 'of', 'how', 'to', 'my', 'thoughts', 'about', 'hands on'
    ]
    words = keyword_phrase.lower().replace('"', '').split()
    core_words = [word for word in words if word not in stop_words]
    return " ".join(core_words[:3])

def _execute_search(query: str) -> list:
    """
    A modular and reusable component that executes a single search query against the Google News RSS endpoint.
    """
    encoded_query = urllib.parse.quote(query)
    url = f"https://news.google.com/rss/search?q={encoded_query}&hl=en-US&gl=US&ceid=US:en"
    try:
        feed = feedparser.parse(url)
        return [{"title": entry.title, "link": entry.link} for entry in feed.entries] if feed.entries else []
    except Exception as e:
        logger.error(f"A single search execution failed for query '{query}': {e}")
        return []

# --- CORE PUBLIC FUNCTIONS ---
def search_reddit_threads(keyword: str) -> list:
    """
    Searches for discussion threads using a more flexible and resilient strategy.
    It breaks down the keyword and uses broader search terms to find human-like discussions.
    """
    logger.info("Initiating FLEXIBLE multi-layered Reddit thread search...")
    
    # --- Ø§Ù„ØªØºÙŠÙŠØ± Ø±Ù‚Ù… 1: Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ø£ÙØ¶Ù„ ---
    # Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø£Ø®Ø° Ø§Ù„ÙƒÙ„Ù…Ø© ÙƒÙ…Ø§ Ù‡ÙŠØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙ†Ø¸ÙŠÙÙ‡Ø§ ÙˆØªÙ‚Ø³ÙŠÙ…Ù‡Ø§
    clean_keyword = keyword.replace('"', '').strip()
    core_words = clean_keyword.split()

    # --- Ø§Ù„ØªØºÙŠÙŠØ± Ø±Ù‚Ù… 2: Ø¨Ù†Ø§Ø¡ Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø£ÙƒØ«Ø± Ù…Ø±ÙˆÙ†Ø© ÙˆØ°ÙƒØ§Ø¡Ù‹ ---
    # Ù‡Ø°Ù‡ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª ØªØ¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ù„Ø¹Ø¨Ø§Ø±Ø© Ø§Ù„Ø­Ø±ÙÙŠØ©
    queries = [
        # Ø§Ù„Ø·Ø¨Ù‚Ø© 1: Ø¨Ø­Ø« Ø¯Ù‚ÙŠÙ‚ ÙˆÙ„ÙƒÙ† Ù…Ø±Ù† ÙÙŠ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† (ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©)
        f'site:reddit.com intitle:("{clean_keyword}")',
        
        # Ø§Ù„Ø·Ø¨Ù‚Ø© 2: Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù…Ø¹ Ù…ØµØ·Ù„Ø­Ø§Øª Ù†Ù‚Ø§Ø´ Ø´Ø§Ø¦Ø¹Ø© ÙÙŠ Reddit
        f'site:reddit.com "{clean_keyword}" (discussion OR thoughts OR issues OR problem OR experience)',
        
        # Ø§Ù„Ø·Ø¨Ù‚Ø© 3: Ø¨Ø­Ø« Ø£ÙˆØ³Ø¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ÙÙ‚Ø· (Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ ÙƒÙ„Ù…ØªÙŠÙ†)
        f'site:reddit.com {" ".join(core_words[:2])} (review OR help OR bug OR feedback)',
        
        # Ø§Ù„Ø·Ø¨Ù‚Ø© 4 (Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¢Ù…Ù†Ø©): Ø¨Ø­Ø« ÙˆØ§Ø³Ø¹ Ø¬Ø¯Ø§Ù‹ ÙÙŠ Ø­Ø§Ù„ ÙØ´Ù„ ÙƒÙ„ Ù…Ø§ Ø³Ø¨Ù‚
        f'site:reddit.com "{core_words[0]}" issues'
    ]

    all_threads, found_links = [], set()
    for i, query in enumerate(queries):
        # Ù†ØªÙˆÙ‚Ù Ø¥Ø°Ø§ ÙˆØ¬Ø¯Ù†Ø§ Ù…Ø§ ÙŠÙƒÙÙŠ Ù…Ù† Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        if len(all_threads) >= 4:
            logger.info("Sufficient thread count reached. Concluding search.")
            break
            
        logger.info(f"  -> Search Layer {i+1}/{len(queries)}: Executing query: {query}")
        results = _execute_search(query)
        
        new_threads_found = 0
        for thread in results:
            # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø±Ø§Ø¨Ø· Ù‡Ùˆ Ø±Ø§Ø¨Ø· Ù†Ù‚Ø§Ø´ Ø­Ù‚ÙŠÙ‚ÙŠ ÙˆÙ„ÙŠØ³ ØµÙØ­Ø© Ø¨Ø­Ø« Ø£Ùˆ Ù…Ø³ØªØ®Ø¯Ù…
            if '/comments/' in thread['link'] and thread['link'] not in found_links:
                found_links.add(thread['link'])
                all_threads.append(thread)
                new_threads_found += 1
                
        if new_threads_found > 0:
            logger.info(f"  -> Success! Layer {i+1} found {new_threads_found} new threads.")
            
    if not all_threads:
         logger.warning("All flexible search layers failed. No relevant Reddit threads found.")

    return all_threads[:4]
    
def extract_evidence(reddit_url: str) -> tuple[list, list]:
    """
    Performs a deep, safe, and corrected extraction of a single Reddit thread.
    This version uses the correct, robust JSON access pathing.
    """
    try:
        # --- Ø§Ù„Ø¥ØµÙ„Ø§Ø­ Ø±Ù‚Ù… 1: Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØµØ­ÙŠØ­Ø© ÙˆØ§Ù„Ù…Ø¶Ù…ÙˆÙ†Ø© 100% Ù„Ø¨Ù†Ø§Ø¡ Ø±Ø§Ø¨Ø· JSON ---
        # Ù‡Ø°Ù‡ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© ØªØ²ÙŠÙ„ Ø£ÙŠ Ø¨Ø§Ø±Ø§Ù…ÙŠØªØ±Ø§Øª ÙˆØªØ¶ÙŠÙ .json ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ©ØŒ ÙˆØªØ¹Ù…Ù„ Ù…Ø¹ ÙƒÙ„ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
        base_url = reddit_url.split("?")[0].rstrip('/')
        json_url = f"{base_url}.json"

        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}
        
        response = requests.get(json_url, headers=headers, timeout=15)
        if response.status_code != 200:
            # --- Ø§Ù„Ø¥ØµÙ„Ø§Ø­ Ø±Ù‚Ù… 2: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ØªØºÙŠØ± Ø§Ù„ØµØ­ÙŠØ­ ÙÙŠ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø®Ø·Ø£ ---
            logger.warning(f"Failed to fetch Reddit JSON for {reddit_url}, Status: {response.status_code}")
            return [], []

        data = response.json()
        
        media_found, insights = [], []

        # --- 1. Extract Evidence from the Main Post (Corrected & Safe Pathing) ---
        # Ù‡Ø°Ø§ Ù‡Ùˆ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ù„Ù„ÙˆØµÙˆÙ„ Ø§Ù„Ø¢Ù…Ù† Ù„Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ ÙˆÙ‡Ùˆ Ù…Ù…ØªØ§Ø² ÙˆÙ„Ù… ÙŠØªÙ… ØªØºÙŠÙŠØ±Ù‡.
        main_post = data[0].get('data', {}).get('children', [{}])[0].get('data', {})
        if not main_post:
            logger.warning(f"Main post data was empty for {reddit_url}")
            return [], [] 

        post_title = main_post.get('title', 'Reddit Post')

        # a) Direct image/gif/video links
        if main_post.get('url_overridden_by_dest'):
            url = main_post['url_overridden_by_dest']
            if any(ext in url.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.mp4']):
                media_type = "video" if url.endswith('.mp4') else "image"
                media_found.append({"type": media_type, "url": url, "description": f"Evidence from Reddit post: {post_title[:50]}"})

        # b) Reddit native video player
        if main_post.get('is_video') and main_post.get('media', {}).get('reddit_video'):
            vid_url = main_post.get('media', {}).get('reddit_video', {}).get('fallback_url')
            if vid_url:
                media_found.append({"type": "video", "url": vid_url, "description": f"Video evidence from user: {post_title[:50]}"})

        # c) Reddit image galleries
        if main_post.get('is_gallery') and main_post.get('gallery_data'):
            for item in main_post.get('gallery_data', {}).get('items', []):
                media_id = item.get('media_id')
                if media_id and media_id in main_post.get('media_metadata', {}):
                    img_data = main_post['media_metadata'][media_id]
                    source_img = img_data.get('s', {})
                    img_url_encoded = source_img.get('u', source_img.get('gif'))
                    if img_url_encoded:
                        # --- Ø§Ù„Ø¥ØµÙ„Ø§Ø­ Ø±Ù‚Ù… 3: ÙÙƒ ØªØ´ÙÙŠØ± Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙˆØ±Ø© Ù„Ø¬Ø¹Ù„Ù‡ ØµØ§Ù„Ø­Ø§Ù‹ ---
                        img_url_decoded = html.unescape(img_url_encoded)
                        media_found.append({"type": "image", "url": img_url_decoded, "description": f"Image from gallery in post: {post_title[:40]}"})

        # --- 2. Extract Text Insights & Code from Comments ---
        comments_data = data[1].get('data', {}).get('children', [])
        for comment_item in comments_data:
            comment_data = comment_item.get('data', {})
            body = comment_data.get('body', '')
            score = comment_data.get('score', 0)
            
            if len(body) > 50 and body not in ["[deleted]", "[removed]"]:
                # Ù…Ù†Ø·Ù‚ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£ÙƒÙˆØ§Ø¯ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ - Ù„Ù… ÙŠØªÙ… Ø§Ù„Ù…Ø³Ø§Ø³ Ø¨Ù‡
                code_blocks = re.findall(r'```(.*?)```', body, re.DOTALL)
                for code in code_blocks:
                    if len(code.strip()) > 10:
                        media_found.append({"type": "code", "content": code.strip(), "description": "Code snippet shared by a user in comments"})
                
                # Ù…Ù†Ø·Ù‚ ÙÙ„ØªØ±Ø© Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ - Ù„Ù… ÙŠØªÙ… Ø§Ù„Ù…Ø³Ø§Ø³ Ø¨Ù‡
                if score > 5:
                    clean_body = re.sub(r'```.*?```', '[Code Snippet Provided]', body, flags=re.DOTALL)
                    clean_body = re.sub(r'\[(.*?)\]\(.*?\)', r'\1', clean_body)
                    insights.append({
                        "source_name": main_post.get('subreddit_name_prefixed', 'Reddit'),
                        "text": clean_body.replace("\n", " ").strip()[:500],
                        "url": f"https://www.reddit.com{comment_data.get('permalink', '')}"
                    })
        
        # Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ (Ø­Ø³Ø¨ Ø·ÙˆÙ„ Ø§Ù„Ù†Øµ) - Ù„Ù… ÙŠØªÙ… ØªØºÙŠÙŠØ±Ù‡Ø§
        insights.sort(key=lambda x: len(x['text']), reverse=True)
        return insights[:5], media_found

    except Exception as e:
        logger.error(f"Deep extraction failed entirely for {reddit_url}: {e}", exc_info=True)
        return [], []

def get_community_intel(keyword: str) -> tuple[str, list]:
    """
    Main orchestrator for Reddit intelligence gathering.
    Returns a structured text report AND a list of all unique visual evidence found.
    """
    logger.info(f"ðŸ§  Mining Reddit for insights & visual evidence on: '{keyword}'...")
    threads = search_reddit_threads(keyword)
    
    if not threads:
        logger.warning("All search layers failed. No relevant Reddit threads found.")
        return "", []
    
    all_insights, all_media = [], []
    for thread in threads:
        if "reddit.com" in thread['link']:
            insights, media = extract_evidence(thread['link'])
            all_insights.extend(insights)
            all_media.extend(media)
            time.sleep(0.5)
            
    # Ø·Ø±ÙŠÙ‚Ø© Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø± Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ - Ù„Ù… ÙŠØªÙ… Ø§Ù„Ù…Ø³Ø§Ø³ Ø¨Ù‡Ø§
    unique_media = list({(v.get('url') or v.get('content')): v for v in all_media}.values())
    
    if not all_insights:
        logger.warning("Found threads but could not extract any high-quality insights.")
        return "", unique_media
    
    # ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ø§Ù„Ø°ÙŠ ØµÙ…Ù…ØªÙ‡ - Ù„Ù… ÙŠØªÙ… Ø§Ù„Ù…Ø³Ø§Ø³ Ø¨Ù‡
    report = "\n=== ðŸ“¢ INTEL REPORT: REAL COMMUNITY FEEDBACK ===\n"
    media_summary = {}
    for m in unique_media:
        media_summary[m['type']] = media_summary.get(m['type'], 0) + 1
    summary_line = f"SUMMARY: Found {len(all_insights)} textual insights and {len(unique_media)} pieces of visual evidence."
    if media_summary:
        summary_line += f" (Evidence breakdown: {media_summary})"
    report += summary_line + "\n\n"
    report += "INSTRUCTIONS: Use these real user quotes and evidence to build the 'First-Hand Experience' section. Highlight any conflicts with official news.\n\n"
    
    unique_insights = list({v['text']: v for v in all_insights}.values())[:4]
    for i, item in enumerate(unique_insights):
        report += f"--- INSIGHT {i+1} ---\n"
        report += f"SOURCE: {item['source_name']} (Use this specific name)\n"
        report += f"LINK: {item['url']} (Link to this for citation)\n"
        report += f"USER SAID: \"{item['text']}\"\n\n"
        
    return report, unique_media
