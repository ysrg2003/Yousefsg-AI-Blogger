# ==============================================================================
# FILE: reddit_manager.py
# DESCRIPTION: Advanced Reddit Scraper & Intelligence Manager.
#              Integrates Stealth Mode (Anti-403) and Deep Content Extraction.
#              Fully compatible with AI-Blogger-Automation Pipeline.
# ==============================================================================

import requests
import json
import time
import re
import random
import urllib.parse
from typing import List, Dict, Any, Optional

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ ÙˆØ¸ÙŠÙØ© Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ù…Ù† Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ù„Ø¶Ù…Ø§Ù† ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø³Ø¬Ù„Ø§Øª
try:
    from config import log
except ImportError:
    # Ø¯Ø§Ù„Ø© Ø¨Ø¯ÙŠÙ„Ø© ÙÙŠ Ø­Ø§Ù„Ø© ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ø¨Ø´ÙƒÙ„ Ù…Ù†ÙØµÙ„ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
    def log(msg): print(f"[LOG] {msg}")

# ------------------------------------------------------------------------------
# STEALTH CONFIGURATION
# ------------------------------------------------------------------------------
# Ù‚Ø§Ø¦Ù…Ø© Ù…ØªÙ†ÙˆØ¹Ø© Ù…Ù† ÙˆÙƒÙ„Ø§Ø¡ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù„Ø®Ø¯Ø§Ø¹ Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ø­Ù…Ø§ÙŠØ© ÙÙŠ Reddit
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15"
]

class RedditManager:
    """
    Ù…Ø¯ÙŠØ± Reddit Ù…ØªØ·ÙˆØ± Ù„Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØªØµØ¯ÙŠØ±Ù‡Ø§.
    ÙŠØ³ØªØ®Ø¯Ù… ØªØ¯ÙˆÙŠØ± Ø§Ù„ÙˆÙƒÙ„Ø§Ø¡ (User-Agent Rotation) ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø°ÙƒÙŠØ©.
    """
    
    BASE_URL = "https://www.reddit.com"

    def __init__(self):
        self.session = requests.Session()
        self._rotate_identity()

    def _rotate_identity(self):
        """ØªØ­Ø¯ÙŠØ« ØªØ±ÙˆÙŠØ³Ø§Øª Ø§Ù„Ø¬Ù„Ø³Ø© Ù„ØªØ¨Ø¯Ùˆ ÙƒÙ…ØªØµÙØ­ Ø¬Ø¯ÙŠØ¯."""
        agent = random.choice(USER_AGENTS)
        self.session.headers.update({
            "User-Agent": agent,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.5",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "none",
            "Sec-Fetch-User": "?1",
            "Cache-Control": "max-age=0",
        })

    def _get_json(self, url: str) -> Optional[Dict[str, Any]]:
        """Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨ØµÙŠØºØ© JSON Ù…Ø¹ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„Ø­Ø¸Ø±."""
        try:
            # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø±Ø§Ø¨Ø· ÙŠÙ†ØªÙ‡ÙŠ Ø¨Ù€ .json
            if ".json" not in url:
                if "?" in url:
                    base, params = url.split("?", 1)
                    url = f"{base.rstrip('/')}.json?{params}"
                else:
                    url = f"{url.rstrip('/')}.json"
            
            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø·Ù„Ø¨ (Retry Logic)
            attempts = 0
            max_attempts = 3
            
            while attempts < max_attempts:
                try:
                    response = self.session.get(url, timeout=15)
                    
                    # Ø¥Ø°Ø§ Ù†Ø¬Ø­ Ø§Ù„Ø·Ù„Ø¨
                    if response.status_code == 200:
                        return response.json()
                    
                    # Ø¥Ø°Ø§ ØªÙ… Ø§Ù„Ø­Ø¸Ø± (403/429)
                    elif response.status_code in [403, 429]:
                        log(f"   âš ï¸ Reddit blocked request ({response.status_code}). Rotating identity and retrying...")
                        self._rotate_identity()
                        time.sleep(3 + attempts) # Ø§Ù†ØªØ¸Ø§Ø± ØªØµØ§Ø¹Ø¯ÙŠ
                        attempts += 1
                        continue
                    
                    # Ø£Ø®Ø·Ø§Ø¡ Ø£Ø®Ø±Ù‰
                    else:
                        log(f"   âš ï¸ Reddit Error {response.status_code} for {url}")
                        return None

                except requests.exceptions.RequestException as re_err:
                    log(f"   âš ï¸ Connection Error: {re_err}")
                    attempts += 1
                    time.sleep(2)
            
            return None

        except Exception as e:
            log(f"   âš ï¸ Reddit General Exception: {e}")
            return None

    def _extract_media(self, data: Dict[str, Any]) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„ÙˆØ³Ø§Ø¦Ø· Ù…Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù†Ø´ÙˆØ±."""
        media = []
        url = data.get("url", "")
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
        if any(ext in url.lower() for ext in [".jpg", ".jpeg", ".png", ".gif", "imgur.com", "gallery"]):
            media.append(url)
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¯Ø§Ø®Ù„ Ø§Ù„Ù†Øµ
        text = data.get("selftext", "") or data.get("body", "")
        urls = re.findall(r'(https?://[^\s)\]]+\.(?:jpg|jpeg|png|gif))', text, re.IGNORECASE)
        media.extend(urls)
        
        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØªØ§ (Gallery Metadata)
        if "media_metadata" in data and isinstance(data["media_metadata"], dict):
            for item in data["media_metadata"].values():
                if "s" in item and "u" in item["s"]:
                    # ÙÙƒ ØªØ±Ù…ÙŠØ² Ø§Ù„Ø±Ø§Ø¨Ø· (&amp; -> &)
                    media.append(item["s"]["u"].replace("&amp;", "&"))
        
        return list(set(media))

    def _extract_codes(self, text: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£ÙƒÙˆØ§Ø¯ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© Ù…Ù† Ø§Ù„Ù†Øµ."""
        if not text: return []
        codes = re.findall(r'```(?:[a-zA-Z]*\n)?([\s\S]*?)```', text)
        inline_codes = re.findall(r'`([^`\n]+)`', text)
        return list(set([c.strip() for c in codes + inline_codes if c.strip()]))

    def get_post_details(self, post_url: str) -> Dict[str, Any]:
        """Ø¬Ù„Ø¨ ØªÙØ§ØµÙŠÙ„ Ù…Ù†Ø´ÙˆØ± Ù…Ø¹ÙŠÙ† ÙˆØ§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª."""
        data = self._get_json(post_url)
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        if not data or not isinstance(data, list) or len(data) < 2:
            return {"post": {}, "comments": []}

        try:
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù†Ø´ÙˆØ± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
            p_data = data[0]["data"]["children"][0]["data"]
            post_details = {
                "title": p_data.get("title"),
                "text": p_data.get("selftext"),
                "author": p_data.get("author"),
                "url": f"{self.BASE_URL}{p_data.get('permalink')}",
                "media": self._extract_media(p_data),
                "score": p_data.get("score", 0),
                "codes": self._extract_codes(p_data.get("selftext", ""))
            }
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª (Ø£ÙØ¶Ù„ 3 ØªØ¹Ù„ÙŠÙ‚Ø§Øª)
            comments = []
            if len(data) > 1 and "data" in data[1] and "children" in data[1]["data"]:
                children = data[1]["data"]["children"]
                for child in children:
                    if len(comments) >= 3: break # Ø§ÙƒØªÙÙŠ Ø¨Ø£ÙØ¶Ù„ 3 ØªØ¹Ù„ÙŠÙ‚Ø§Øª
                    if child["kind"] == "t1":
                        d = child["data"]
                        # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø§Ù„Ù…Ø­Ø°ÙˆÙØ© Ø£Ùˆ ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø§Ù„Ø¨ÙˆØªØ§Øª
                        if d.get("body") in ["[deleted]", "[removed]"] or "bot" in str(d.get("author")).lower():
                            continue
                            
                        comments.append({
                            "author": d.get("author"),
                            "body": d.get("body", ""),
                            "score": d.get("score", 0),
                            "media": self._extract_media(d),
                            "codes": self._extract_codes(d.get("body", ""))
                        })

            return {"post": post_details, "comments": comments}
        except Exception as e:
            log(f"   âš ï¸ Error parsing details for {post_url}: {e}")
            return {"post": {}, "comments": []}

    def get_all_data(self, query: str, limit: int = 3) -> Dict[str, Any]:
        """Ø§Ù„Ø¨Ø­Ø« ÙˆØ¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ù„Ø¹Ø¯Ø© Ù…Ù†Ø´ÙˆØ±Ø§Øª."""
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙ‚Ø¯Ù…: Ø§Ù„ÙØ±Ø² Ø­Ø³Ø¨ Ø§Ù„ØµÙ„Ø© ÙˆØ§Ù„ÙˆÙ‚Øª (Ø´Ù‡Ø±)
        encoded_query = urllib.parse.quote(query)
        search_url = f"{self.BASE_URL}/search.json?q={encoded_query}&limit={limit}&sort=relevance&t=month"
            
        search_data = self._get_json(search_url)
        results = {
            "query": query,
            "posts": []
        }

        if search_data and "data" in search_data and "children" in search_data["data"]:
            for post_item in search_data["data"]["children"]:
                try:
                    permalink = post_item['data'].get('permalink')
                    if permalink:
                        post_url = f"{self.BASE_URL}{permalink}"
                        details = self.get_post_details(post_url)
                        if details["post"]: # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ù†Ø´ÙˆØ± ÙÙ‚Ø· Ø¥Ø°Ø§ ØªÙ… Ø¬Ù„Ø¨Ù‡ Ø¨Ù†Ø¬Ø§Ø­
                            results["posts"].append(details)
                except Exception as e:
                    continue
        
        return results

    def generate_writer_brief(self, data: Dict[str, Any]) -> str:
        """ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù… Ø¥Ù„Ù‰ Ù†Øµ Ù…Ù†Ø³Ù‚ (Context) Ù„ÙŠÙÙ‡Ù…Ù‡ ÙƒØ§ØªØ¨ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ."""
        if not data["posts"]: return ""

        brief = f"--- REAL USER DISCUSSIONS & INSIGHTS (REDDIT) ---\n"
        brief += f"Query Focus: {data['query']}\n\n"
        
        for i, item in enumerate(data["posts"], 1):
            post = item["post"]
            brief += f"THREAD #{i}: {post['title']}\n"
            brief += f"Source URL: {post['url']}\n"
            brief += f"Upvotes: {post['score']}\n"
            
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ ÙˆØ¹Ø±Ø¶ Ù…Ù‚ØªØ·Ù Ù…Ù†Ù‡
            content = str(post['text']).replace('\n', ' ').strip()
            if len(content) > 600: content = content[:600] + "..."
            brief += f"Content Summary: {content}\n"
            
            if item["comments"]:
                brief += "TOP COMMUNITY COMMENTS:\n"
                for c in item["comments"]:
                    c_body = str(c['body']).replace('\n', ' ').strip()
                    if len(c_body) > 300: c_body = c_body[:300] + "..."
                    brief += f"   - u/{c['author']} ({c['score']} pts): {c_body}\n"
            
            brief += "-"*30 + "\n"
            
        return brief


# ==============================================================================
# ADAPTER FUNCTION (THE BRIDGE TO MAIN.PY)
# ==============================================================================

def get_community_intel(keyword: str):
    """
    Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø§Ù„ØªÙŠ ÙŠØ³ØªØ¯Ø¹ÙŠÙ‡Ø§ Ù…Ù„Ù main.py.
    
    Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª:
        keyword (str): Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ù„Ù„Ø¨Ø­Ø«.
        
    Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª:
        tuple: (text_context, media_list)
        - text_context (str): Ù†Øµ Ù…Ù„Ø®Øµ Ù„Ù„Ù…Ù†Ø§Ù‚Ø´Ø§Øª Ù„ÙŠØ³ØªØ®Ø¯Ù…Ù‡ Ø§Ù„ÙƒØ§ØªØ¨.
        - media_list (list): Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ù‚ÙˆØ§Ù…ÙŠØ³ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ±.
    """
    log(f"ğŸ§  [Reddit Manager] Mining deep intelligence for: '{keyword}'...")
    
    try:
        # 1. ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø¯ÙŠØ± ÙˆØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨Ø­Ø«
        manager = RedditManager()
        raw_data = manager.get_all_data(keyword, limit=3)
        
        # 2. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù†ØªØ§Ø¦Ø¬
        if not raw_data["posts"]:
            log("   âš ï¸ No Reddit discussions found for this topic.")
            return "", []

        # 3. ØªÙˆÙ„ÙŠØ¯ Ø³ÙŠØ§Ù‚ Ø§Ù„Ù†Øµ (Text Context)
        text_context = manager.generate_writer_brief(raw_data)
        
        # 4. Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ÙˆØ³Ø§Ø¦Ø· (Media Formatting)
        # Ù†Ø­ÙˆÙ„ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ Ø§Ù„Ø´ÙƒÙ„ Ø§Ù„Ø°ÙŠ ÙŠØªÙˆÙ‚Ø¹Ù‡ main.py
        media_assets = []
        for item in raw_data["posts"]:
            # ØµÙˆØ± Ø§Ù„Ù…Ù†Ø´ÙˆØ±
            for img_url in item["post"].get("media", []):
                media_assets.append({
                    "type": "image",
                    "url": img_url,
                    "description": f"Community Image: {item['post']['title']}",
                    "score": item['post'].get("score", 0),
                    "source": "Reddit"
                })
            # ØµÙˆØ± Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª (Ø¥Ù† ÙˆØ¬Ø¯Øª)
            for comment in item["comments"]:
                for img_url in comment.get("media", []):
                    media_assets.append({
                        "type": "image",
                        "url": img_url,
                        "description": f"Comment Image by u/{comment['author']}",
                        "score": comment.get("score", 0),
                        "source": "Reddit"
                    })

        log(f"   âœ… Reddit Intel: Found {len(raw_data['posts'])} threads and {len(media_assets)} media assets.")
        return text_context, media_assets

    except Exception as e:
        log(f"   âš ï¸ Reddit Manager Critical Error: {e}")
        # Ø¥Ø±Ø¬Ø§Ø¹ Ù‚ÙŠÙ… ÙØ§Ø±ØºØ© Ø¢Ù…Ù†Ø© Ù„Ù…Ù†Ø¹ ØªÙˆÙ‚Ù Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬
        return "", []

# ------------------------------------------------------------------------------
# TESTING BLOCK (Run this file directly to verify)
# ------------------------------------------------------------------------------
if __name__ == "__main__":
    print("--- STARTING REDDIT MANAGER TEST ---")
    test_keyword = "AutoGPT Agents"
    
    context, media = get_community_intel(test_keyword)
    
    print("\n\n=== GENERATED CONTEXT ===")
    print(context[:1000]) # Ø·Ø¨Ø§Ø¹Ø© Ø£ÙˆÙ„ 1000 Ø­Ø±Ù
    
    print("\n=== EXTRACTED MEDIA ===")
    for m in media:
        print(m)
    
    print("\n--- TEST COMPLETE ---")
