import requests
import urllib.parse
import logging
import feedparser
import time
import re

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù„ÙˆØ¬Ø±
logging.basicConfig(level=logging.INFO, format='%(asctime)s - [REDDIT-INTEL] - %(message)s')
logger = logging.getLogger("RedditIntel")

def search_reddit_threads(keyword):
    """
    ÙŠØ¨Ø­Ø« Ø¹Ù† Ù†Ù‚Ø§Ø´Ø§Øª Ø­Ù‚ÙŠÙ‚ÙŠØ© (Ù„ÙŠØ³Øª Ø£Ø®Ø¨Ø§Ø±Ø§Ù‹) Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙÙ„Ø§ØªØ± Ø¬ÙˆØ¬Ù„ Ø§Ù„Ø°ÙƒÙŠØ©.
    """
    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙƒÙ„Ù…Ø§Øª ØªØ¯Ù„ Ø¹Ù„Ù‰ ØªØ¬Ø±Ø¨Ø© Ø­Ù‚ÙŠÙ‚ÙŠØ©
    search_query = f"site:reddit.com {keyword} (review OR 'after using' OR 'problem with' OR 'my thoughts') -giveaway"
    encoded = urllib.parse.quote(search_query)
    # Ù†Ø³ØªØ®Ø¯Ù… Ø¨Ø­Ø« Ø¬ÙˆØ¬Ù„ Ø§Ù„Ø¹Ø§Ù… Ø¨ØµÙŠØºØ© RSS Ù„Ø£Ù†Ù‡ Ø£Ø¯Ù‚ Ù…Ù† Ø¨Ø­Ø« Reddit Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ
    url = f"https://services.google.com/feed/cse/json?q={encoded}" 
    # Ù…Ù„Ø§Ø­Ø¸Ø©: Ø³Ù†Ø³ØªØ®Ø¯Ù… RSS Ø§Ù„Ù‚ÙŠØ§Ø³ÙŠ Ù„Ø£Ù†Ù‡ Ø£ÙƒØ«Ø± Ø§Ø³ØªÙ‚Ø±Ø§Ø±Ø§Ù‹
    url = f"https://news.google.com/rss/search?q={encoded}&hl=en-US&gl=US&ceid=US:en"
    
    try:
        feed = feedparser.parse(url)
        threads = []
        if feed.entries:
            for entry in feed.entries[:4]: 
                # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø±Ø§Ø¨Ø· Ù…Ù† Ø¥Ø¶Ø§ÙØ§Øª Ø¬ÙˆØ¬Ù„
                real_link = entry.link
                # ÙÙŠ Ø¨Ø¹Ø¶ Ø§Ù„Ø£Ø­ÙŠØ§Ù† Ø±ÙˆØ§Ø¨Ø· RSS ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ google redirectionØŒ Ù†Ø­Ø§ÙˆÙ„ Ø§Ø³ØªØ®Ù„Ø§Øµ Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ø£ØµÙ„ÙŠ
                # Ù„ÙƒÙ† ØºØ§Ù„Ø¨Ø§Ù‹ RSS Search ÙŠØ¹Ø·ÙŠ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±.
                threads.append({
                    "title": entry.title,
                    "link": real_link
                })
        return threads
    except Exception as e:
        logger.error(f"Search failed: {e}")
        return []

def extract_smart_opinions(reddit_url):
    """
    ÙŠØ³Ø­Ø¨ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª ÙˆÙŠØ­Ù„Ù„ Ù…Ø­ØªÙˆØ§Ù‡Ø§ Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ 'Ø§Ù„Ø°Ù‡Ø¨' ÙÙ‚Ø·.
    """
    try:
        clean_url = reddit_url.split("?")[0]
        if not clean_url.endswith(".json"):
            json_url = f"{clean_url}.json"
        else:
            json_url = clean_url

        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0'
        }
        
        r = requests.get(json_url, headers=headers, timeout=10)
        if r.status_code != 200: return []

        data = r.json()
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ù… Ø§Ù„Ù…Ø¬ØªÙ…Ø¹ (Subreddit)
        try:
            subreddit = data[0]['data']['children'][0]['data']['subreddit_name_prefixed'] # ex: r/Android
        except:
            subreddit = "Reddit"

        comments_data = data[1]['data']['children']
        insights = []
        
        for comm in comments_data:
            c_data = comm.get('data', {})
            body = c_data.get('body', '')
            score = c_data.get('score', 0)
            permalink = c_data.get('permalink', '')
            author = c_data.get('author', 'User')
            
            # 1. ÙÙ„ØªØ± Ø§Ù„Ø¬ÙˆØ¯Ø© (Ø§Ù„Ø·ÙˆÙ„ + ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªØ¬Ø±Ø¨Ø©)
            if len(body) > 60 and body not in ["[deleted]", "[removed]"]:
                markers = ["i noticed", "in my experience", "battery life", "bug", "glitch", "crash", "actually", "worth it"]
                if any(m in body.lower() for m in markers) or score > 5:
                    
                    # Ø¨Ù†Ø§Ø¡ Ø±Ø§Ø¨Ø· Ø°ÙƒÙŠ Ù„Ù„ØªØ¹Ù„ÙŠÙ‚
                    full_link = f"https://www.reddit.com{permalink}"
                    
                    insights.append({
                        "source_name": subreddit, # r/Tech
                        "author": author,
                        "text": body[:400].replace("\n", " "), # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ
                        "url": full_link,
                        "score": score
                    })
        
        # Ù†Ø±ØªØ¨ Ø­Ø³Ø¨ Ø§Ù„Ø£Ù‡Ù…ÙŠØ© (Score)
        insights.sort(key=lambda x: x['score'], reverse=True)
        return insights[:3] # Ù†Ø£Ø®Ø° Ø£ÙØ¶Ù„ 3 ÙÙ‚Ø· Ù…Ù† ÙƒÙ„ Ø®ÙŠØ·

    except Exception as e:
        # logger.error(f"Extraction error: {e}")
        return []

def get_community_intel(keyword):
    """
    Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ: ÙŠØ¹ÙŠØ¯ ØªÙ‚Ø±ÙŠØ±Ø§Ù‹ Ù†ØµÙŠØ§Ù‹ Ù…Ù‡ÙŠÙƒÙ„Ø§Ù‹ Ù„Ù„Ù€ AI
    """
    logger.info(f"ğŸ§  Mining Reddit intelligence for: '{keyword}'...")
    threads = search_reddit_threads(keyword)
    
    if not threads: return ""
    
    all_insights = []
    for thread in threads:
        if "reddit.com" in thread['link']:
            ops = extract_smart_opinions(thread['link'])
            all_insights.extend(ops)
            time.sleep(0.5)
            
    if not all_insights: return ""
    
    # ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø´ÙƒÙ„ ØµØ§Ø±Ù… Ù„Ù„Ù€ Prompt
    # Ù‡Ø°Ø§ Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ ÙŠØ¬Ø¨Ø± Ø§Ù„Ù€ AI Ø¹Ù„Ù‰ ÙÙ‡Ù… Ø§Ù„Ø±Ø§Ø¨Ø· ÙˆØ§Ù„Ù…ØµØ¯Ø±
    report = "\n=== ğŸ“¢ REAL COMMUNITY FEEDBACK (INTEGRATE THIS) ===\n"
    report += "INSTRUCTIONS: Use these real user quotes to validate or criticize the news. \n"
    report += "CRITICAL: When citing, you MUST hyperlink the text 'community discussion' or the Subreddit name (e.g., r/Gadgets) to the provided URL.\n\n"
    
    # Ù†Ø®ØªØ§Ø± Ø£ÙØ¶Ù„ 4 Ø¢Ø±Ø§Ø¡ Ù…ØªÙ†ÙˆØ¹Ø©
    unique_insights = list({v['text']:v for v in all_insights}.values())[:4]
    
    for i, item in enumerate(unique_insights):
        report += f"--- INSIGHT {i+1} ---\n"
        report += f"SOURCE: {item['source_name']} (Use this name)\n"
        report += f"LINK: {item['url']} (Link to this)\n"
        report += f"USER SAID: \"{item['text']}\"\n"
        
    return report
