import requests
import urllib.parse
import logging
import feedparser
import time
import re
import html

# --- CONFIGURATION & LOGGING ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - [REDDIT-INTEL] - %(message)s')
logger = logging.getLogger("RedditIntel")

# --- PRIVATE HELPER FUNCTIONS ---

def _execute_search(query: str) -> list:
    """
    ÙŠÙ†ÙØ° Ø¨Ø­Ø«Ø§Ù‹ ÙˆØ§Ø­Ø¯Ø§Ù‹ Ø¹Ø¨Ø± Google News RSS
    """
    encoded_query = urllib.parse.quote(query)
    url = f"https://news.google.com/rss/search?q={encoded_query}&hl=en-US&gl=US&ceid=US:en"
    try:
        feed = feedparser.parse(url)
        return [{"title": entry.title, "link": entry.link} for entry in feed.entries] if feed.entries else []
    except Exception as e:
        logger.error(f"Search execution failed: {e}")
        return []

# --- CORE PUBLIC FUNCTIONS ---

def search_reddit_threads(keyword: str) -> list:
    """
    ÙŠØ¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù†Ù‚Ø§Ø´Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø·Ø¨Ù‚Ø§Øª (Ø¯Ù‚ÙŠÙ‚ -> Ø¹Ø§Ù…)
    """
    logger.info(f"ğŸ” Searching Reddit for: {keyword}")
    
    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
    clean_keyword = keyword.replace('"', '').strip()
    
    queries = [
        # Ø§Ù„Ø·Ø¨Ù‚Ø© 1: Ø¨Ø­Ø« Ø¯Ù‚ÙŠÙ‚ Ø¬Ø¯Ø§Ù‹ ÙÙŠ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†
        f'site:reddit.com intitle:"{clean_keyword}" (review OR problem OR "hands on" OR guide)',
        # Ø§Ù„Ø·Ø¨Ù‚Ø© 2: Ø¨Ø­Ø« Ø¹Ø§Ù… ÙÙŠ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†
        f'site:reddit.com intitle:{clean_keyword}',
        # Ø§Ù„Ø·Ø¨Ù‚Ø© 3: Ø¨Ø­Ø« ÙˆØ§Ø³Ø¹ ÙÙŠ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        f'site:reddit.com "{clean_keyword}" (review OR solved OR code OR example)'
    ]

    all_threads = []
    seen_links = set()

    for q in queries:
        if len(all_threads) >= 4: break
        results = _execute_search(q)
        for item in results:
            if item['link'] not in seen_links:
                seen_links.add(item['link'])
                all_threads.append(item)
    
    return all_threads[:5]

def extract_evidence(reddit_url: str) -> tuple[list, list]:
    """
    ÙŠØ³ØªØ®Ø±Ø¬ Ø§Ù„Ù†ØµÙˆØµ + Ø§Ù„ÙˆØ³Ø§Ø¦Ø· (ØµÙˆØ±/ÙÙŠØ¯ÙŠÙˆ/Ø£ÙƒÙˆØ§Ø¯) Ø¨Ø´ÙƒÙ„ Ø¢Ù…Ù† ÙˆØµØ­ÙŠØ­.
    """
    try:
        # 1. ØªØµØ­ÙŠØ­ Ø±Ø§Ø¨Ø· JSON (Ø§Ù„Ø®Ø·Ø£ ÙƒØ§Ù† Ù‡Ù†Ø§ Ø³Ø§Ø¨Ù‚Ø§Ù‹)
        # Ù†Ù‚ÙˆÙ… Ø¨Ø¥Ø²Ø§Ù„Ø© Ø£ÙŠ Ø¨Ø§Ø±Ø§Ù…ÙŠØªØ±Ø§Øª ÙˆÙ†Ø¶ÙŠÙ .json Ù„Ù„Ù†Ù‡Ø§ÙŠØ©
        base_url = reddit_url.split("?")[0].rstrip("/")
        json_url = f"{base_url}.json"

        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        
        response = requests.get(json_url, headers=headers, timeout=10)
        if response.status_code != 200:
            return [], []

        data = response.json()
        
        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ø¨Ø§Ø±Ø© Ø¹Ù† Ù‚Ø§Ø¦Ù…Ø© (Ù‡ÙŠÙƒÙ„ Reddit Ø§Ù„ØµØ­ÙŠØ­)
        if not isinstance(data, list) or len(data) < 2:
            return [], []

        media_found = []
        insights = []

        # --- ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù†Ø´ÙˆØ± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ (Main Post) ---
        main_post = data[0]['data']['children'][0]['data']
        post_title = main_post.get('title', 'Reddit Post')
        subreddit = main_post.get('subreddit_name_prefixed', 'Reddit')

        # Ø£) Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø© Ø£Ùˆ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©
        if 'url_overridden_by_dest' in main_post:
            url = main_post['url_overridden_by_dest']
            ext = url.split('?')[0].lower()
            if any(x in ext for x in ['.jpg', '.png', '.gif', '.jpeg']):
                media_found.append({"type": "image", "url": url, "description": f"Post Image: {post_title[:50]}"})
            elif '.mp4' in ext:
                media_found.append({"type": "video", "url": url, "description": f"Post Video: {post_title[:50]}"})

        # Ø¨) Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙÙŠØ¯ÙŠÙˆ Reddit Ø§Ù„Ø£ØµÙ„ÙŠ (Hosted Video)
        if main_post.get('is_video') and main_post.get('media'):
            try:
                vid_url = main_post['media']['reddit_video']['fallback_url']
                media_found.append({"type": "video", "url": vid_url, "description": f"Reddit Video: {post_title[:50]}"})
            except: pass

        # Ø¬) Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ø§Ø±Ø¶ Ø§Ù„ØµÙˆØ± (Galleries) - Ø¬Ø²Ø¡ Ù…Ø¹Ù‚Ø¯ ØªÙ… Ø¥ØµÙ„Ø§Ø­Ù‡
        if main_post.get('is_gallery') and main_post.get('media_metadata'):
            gallery_items = main_post.get('gallery_data', {}).get('items', [])
            for item in gallery_items:
                media_id = item['media_id']
                meta = main_post['media_metadata'].get(media_id, {})
                if 's' in meta and 'u' in meta['s']:
                    # ÙÙƒ ØªØ´ÙÙŠØ± Ø§Ù„Ø±Ø§Ø¨Ø· (Reddit ÙŠØ³ØªØ®Ø¯Ù… &amp;)
                    img_url = html.unescape(meta['s']['u'])
                    media_found.append({"type": "image", "url": img_url, "description": "Gallery Image"})

        # --- ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª (Comments) ---
        comments_data = data[1]['data']['children']
        
        for comm in comments_data:
            c_data = comm.get('data', {})
            body = c_data.get('body', '')
            score = c_data.get('score', 0)
            permalink = c_data.get('permalink', '')
            
            if body and body not in ["[deleted]", "[removed]"]:
                
                # 1. Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£ÙƒÙˆØ§Ø¯ Ø¨Ø±Ù…Ø¬ÙŠØ© Ø¯Ø§Ø®Ù„ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª
                code_blocks = re.findall(r'```(.*?)```', body, re.DOTALL)
                for code in code_blocks:
                    if len(code.strip()) > 10: # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø£ÙƒÙˆØ§Ø¯ Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹
                        media_found.append({
                            "type": "code", 
                            "content": code.strip(), 
                            "description": "Code Snippet from comments"
                        })

                # 2. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ÙÙŠØ¯Ø©
                # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ù…Ù† Ø§Ù„Ø£ÙƒÙˆØ§Ø¯ Ù„Ø¹Ø¯Ù… ØªÙƒØ±Ø§Ø±Ù‡Ø§ ÙÙŠ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†ØµÙŠ
                clean_body = re.sub(r'```.*?```', '[Code Block]', body, flags=re.DOTALL)
                
                if len(clean_body) > 40 and (score > 2 or any(w in clean_body.lower() for w in ['tried', 'worked', 'failed', 'bug', 'fix'])):
                    insights.append({
                        "source_name": subreddit,
                        "text": clean_body.replace("\n", " ").strip()[:600],
                        "url": f"https://www.reddit.com{permalink}",
                        "score": score
                    })

        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø­Ø³Ø¨ Ø§Ù„Ø£Ù‡Ù…ÙŠØ© (Score)
        insights.sort(key=lambda x: x['score'], reverse=True)
        return insights[:4], media_found

    except Exception as e:
        logger.error(f"Extraction error for {reddit_url}: {e}")
        return [], []

def get_community_intel(keyword: str) -> tuple[str, list]:
    """
    Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ: ÙŠØ¹ÙŠØ¯ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†ØµÙŠ + Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø£Ø¯Ù„Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©
    """
    logger.info(f"ğŸš€ Starting Intel Gathering for: '{keyword}'...")
    threads = search_reddit_threads(keyword)
    
    if not threads:
        return "No relevant discussions found.", []
    
    all_insights = []
    all_media = []
    
    for thread in threads:
        if "reddit.com" in thread['link']:
            ops, media = extract_evidence(thread['link'])
            all_insights.extend(ops)
            all_media.extend(media)
            time.sleep(0.5) # Ø§Ø­ØªØ±Ø§Ù…Ø§Ù‹ Ù„Ø³ÙŠØ±ÙØ±Ø§Øª Ø±ÙŠØ¯ÙŠØª
            
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø± Ù…Ù† Ø§Ù„ÙˆØ³Ø§Ø¦Ø·
    unique_media = []
    seen_urls = set()
    for m in all_media:
        identifier = m.get('url') or m.get('content')[:20] # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§Ø¨Ø· Ø£Ùˆ Ø¬Ø²Ø¡ Ù…Ù† Ø§Ù„ÙƒÙˆØ¯ ÙƒØ¨ØµÙ…Ø©
        if identifier not in seen_urls:
            seen_urls.add(identifier)
            unique_media.append(m)
    
    if not all_insights:
        return "Found threads but no significant insights.", unique_media
    
    # Ø¨Ù†Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
    report = "\n=== ğŸ›¡ï¸ COMMUNITY INTEL REPORT ===\n"
    report += f"STATS: Analyzed {len(threads)} threads, found {len(unique_media)} visual proofs (Images/Code/Video).\n\n"
    
    unique_insights = list({v['text']:v for v in all_insights}.values())[:5]
    
    for i, item in enumerate(unique_insights):
        report += f"--- OPINION {i+1} ({item['source_name']}) ---\n"
        report += f"ğŸ’¬ \"{item['text']}\"\n"
        report += f"ğŸ”— Source: {item['url']}\n\n"
        
    return report, unique_media
