import requests
import urllib.parse
import logging
import feedparser
import time
import re

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù„ÙˆØ¬Ø±
logging.basicConfig(level=logging.INFO, format='%(asctime)s - [REDDIT-INTEL] - %(message)s')
logger = logging.getLogger("RedditIntel")

def search_reddit_threads(keyword):
    """
    ÙŠØ¨Ø­Ø« Ø¹Ù† Ù†Ù‚Ø§Ø´Ø§Øª Ø­Ù‚ÙŠÙ‚ÙŠØ© (Ù„ÙŠØ³Øª Ø£Ø®Ø¨Ø§Ø±Ø§Ù‹) Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙÙ„Ø§ØªØ± Ø¬ÙˆØ¬Ù„ Ø§Ù„Ø°ÙƒÙŠØ©.
    """
    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙƒÙ„Ù…Ø§Øª ØªØ¯Ù„ Ø¹Ù„Ù‰ ØªØ¬Ø±Ø¨Ø© Ø­Ù‚ÙŠÙ‚ÙŠØ©
    # Ù†Ø³ØªØ®Ø¯Ù… -giveaway Ù„Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø¨Ù‚Ø§Øª
    search_query = f"site:reddit.com {keyword} (review OR 'after using' OR 'problem with' OR 'my thoughts') -giveaway"
    encoded = urllib.parse.quote(search_query)
    
    # Ù†Ø³ØªØ®Ø¯Ù… Ø¨Ø­Ø« Ø¬ÙˆØ¬Ù„ Ø§Ù„Ø¹Ø§Ù… Ø¨ØµÙŠØºØ© RSS Ù„Ø£Ù†Ù‡ Ø£Ø¯Ù‚ ÙˆØ£Ø³Ø±Ø¹ Ù…Ù† API Ø±ÙŠØ¯ÙŠØª ÙˆÙ„Ø§ ÙŠØ­ØªØ§Ø¬ Ù…ÙØ§ØªÙŠØ­
    url = f"https://news.google.com/rss/search?q={encoded}&hl=en-US&gl=US&ceid=US:en"
    
    try:
        feed = feedparser.parse(url)
        threads = []
        if feed.entries:
            for entry in feed.entries[:4]: 
                # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø±Ø§Ø¨Ø·
                real_link = entry.link
                threads.append({
                    "title": entry.title,
                    "link": real_link
                })
        return threads
    except Exception as e:
        logger.error(f"Search failed: {e}")
        return []

def extract_smart_opinions(reddit_url):
    """
    ÙŠØ³Ø­Ø¨ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª ÙˆÙŠØ­Ù„Ù„ Ù…Ø­ØªÙˆØ§Ù‡Ø§ Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ 'Ø§Ù„Ø°Ù‡Ø¨' ÙÙ‚Ø· (JSON Trick).
    """
    try:
        clean_url = reddit_url.split("?")[0]
        if not clean_url.endswith(".json"):
            json_url = f"{clean_url}.json"
        else:
            json_url = clean_url

        # User-Agent Ø¶Ø±ÙˆØ±ÙŠ Ø¬Ø¯Ø§Ù‹ Ù„ØªØ¬Ù†Ø¨ Ø®Ø·Ø£ 429
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0'
        }
        
        r = requests.get(json_url, headers=headers, timeout=10)
        if r.status_code != 200: return []

        data = r.json()
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ù… Ø§Ù„Ù…Ø¬ØªÙ…Ø¹ (Subreddit) Ù„Ø¥Ø¹Ø·Ø§Ø¡ Ù…ØµØ¯Ø§Ù‚ÙŠØ©
        try:
            subreddit = data[0]['data']['children'][0]['data']['subreddit_name_prefixed'] # ex: r/Android
        except:
            subreddit = "Reddit Discussion"

        comments_data = data[1]['data']['children']
        insights = []
        
        for comm in comments_data:
            c_data = comm.get('data', {})
            body = c_data.get('body', '')
            score = c_data.get('score', 0)
            permalink = c_data.get('permalink', '')
            author = c_data.get('author', 'User')
            
            # ÙÙ„ØªØ± Ø§Ù„Ø¬ÙˆØ¯Ø©:
            # 1. Ø§Ù„Ø·ÙˆÙ„ Ù…Ù†Ø§Ø³Ø¨ (Ù„ÙŠØ³ ÙƒÙ„Ù…Ø© ÙˆØ§Ø­Ø¯Ø© ÙˆÙ„ÙŠØ³ Ù…Ù‚Ø§Ù„Ø§Ù‹ Ø·ÙˆÙŠÙ„Ø§Ù‹ Ø¬Ø¯Ø§Ù‹)
            # 2. ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª ØªØ¯Ù„ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¬Ø±Ø¨Ø©
            if len(body) > 60 and body not in ["[deleted]", "[removed]"]:
                markers = ["i noticed", "in my experience", "battery", "bug", "glitch", "crash", "actually", "worth it", "slow", "fast", "update"]
                if any(m in body.lower() for m in markers) or score > 5:
                    
                    # Ø¨Ù†Ø§Ø¡ Ø±Ø§Ø¨Ø· Ø¯Ù‚ÙŠÙ‚ Ù„Ù„ØªØ¹Ù„ÙŠÙ‚ Ù†ÙØ³Ù‡
                    full_link = f"https://www.reddit.com{permalink}"
                    
                    insights.append({
                        "source_name": subreddit,
                        "author": author,
                        "text": body[:500].replace("\n", " "), # ØªÙ†Ø¸ÙŠÙ
                        "url": full_link,
                        "score": score
                    })
        
        # Ù†Ø±ØªØ¨ Ø­Ø³Ø¨ Ø§Ù„Ø£Ù‡Ù…ÙŠØ© (Score)
        insights.sort(key=lambda x: x['score'], reverse=True)
        return insights[:3] # Ù†Ø£Ø®Ø° Ø£ÙØ¶Ù„ 3 Ù…Ù† ÙƒÙ„ Ø®ÙŠØ·

    except Exception as e:
        # logger.error(f"Extraction error: {e}")
        return []

def get_community_intel(keyword):
    """
    Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ: ÙŠØ¹ÙŠØ¯ ØªÙ‚Ø±ÙŠØ±Ø§Ù‹ Ù†ØµÙŠØ§Ù‹ Ù…Ù‡ÙŠÙƒÙ„Ø§Ù‹ Ù„Ù„Ù€ AI
    """
    logger.info(f"ğŸ§  Mining Reddit intelligence for: '{keyword}'...")
    threads = search_reddit_threads(keyword)
    
    if not threads: return ""
    
    all_insights = []
    for thread in threads:
        if "reddit.com" in thread['link']:
            ops = extract_smart_opinions(thread['link'])
            all_insights.extend(ops)
            time.sleep(0.5)
            
    if not all_insights: return ""
    
    # ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø´ÙƒÙ„ ØµØ§Ø±Ù… Ù„Ù„Ù€ Prompt
    # Ù‡Ø°Ø§ Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ ÙŠØ¬Ø¨Ø± Ø§Ù„Ù€ AI Ø¹Ù„Ù‰ ÙÙ‡Ù… Ø§Ù„Ø±Ø§Ø¨Ø· ÙˆØ§Ù„Ù…ØµØ¯Ø±
    report = "\n=== ğŸ“¢ REAL COMMUNITY FEEDBACK (INTEGRATE THIS) ===\n"
    report += "INSTRUCTIONS: Use these real user quotes to validate or criticize the news. \n"
    report += "CRITICAL: When citing, you MUST hyperlink the text 'community discussion' or the Subreddit name (e.g., r/Gadgets) to the provided URL.\n\n"
    
    # Ù†Ø®ØªØ§Ø± Ø£ÙØ¶Ù„ 4 Ø¢Ø±Ø§Ø¡ Ù…ØªÙ†ÙˆØ¹Ø© ÙˆÙØ±ÙŠØ¯Ø©
    unique_insights = list({v['text']:v for v in all_insights}.values())[:4]
    
    for i, item in enumerate(unique_insights):
        report += f"--- INSIGHT {i+1} ---\n"
        report += f"SOURCE: {item['source_name']} (Use this specific name)\n"
        report += f"LINK: {item['url']} (Link strictly to this)\n"
        report += f"USER SAID: \"{item['text']}\"\n"
        
    return report
