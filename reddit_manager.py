import requests
import urllib.parse
import logging
import feedparser
import time
import re
from bs4 import BeautifulSoup

# ==============================================================================
# CONFIGURATION & LOGGING
# ==============================================================================
logging.basicConfig(level=logging.INFO, format='%(asctime)s - [REDDIT-INTEL] - %(message)s')
logger = logging.getLogger("RedditIntel")

def search_reddit_threads(keyword):
    """
    ÙŠØ¨Ø­Ø« Ø¹Ù† Ù†Ù‚Ø§Ø´Ø§Øª Ø­Ù‚ÙŠÙ‚ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… 3 Ù…Ø³ØªÙˆÙŠØ§Øª Ù…Ù† Ø§Ù„Ø¨Ø­Ø« Ù„Ø¶Ù…Ø§Ù† Ø¹Ø¯Ù… Ø§Ù„Ø¹ÙˆØ¯Ø© Ø¨Ù†ØªØ§Ø¦Ø¬ ØµÙØ±ÙŠØ©.
    """
    # Ø§Ù„Ù…Ø³ØªÙˆÙ‰ 1: Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø°ÙƒÙŠ (Ù…Ø´Ø§ÙƒÙ„ ØªÙ‚Ù†ÙŠØ© Ù…Ø­Ø¯Ø¯Ø©)
    query_v1 = f"site:reddit.com {keyword} (review OR 'problem with' OR bug OR crash OR slow OR issue) -giveaway"
    
    # Ø§Ù„Ù…Ø³ØªÙˆÙ‰ 2: Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø£ØµÙ„ÙŠ (ØªØ¬Ø§Ø±Ø¨ Ø¹Ø§Ù…Ø©)
    query_v2 = f"site:reddit.com {keyword} (review OR 'after using' OR 'my thoughts' OR 'demo') -giveaway"
    
    # Ø§Ù„Ù…Ø³ØªÙˆÙ‰ 3: Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø´Ø§Ù…Ù„ (Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© ÙÙ‚Ø· Ø¯Ø§Ø®Ù„ Ø±ÙŠØ¯ÙŠØª - Ø¨Ø¯ÙˆÙ† Ù‚ÙŠÙˆØ¯)
    query_v3 = f"site:reddit.com {keyword}"

    search_attempts = [
        {"name": "Improved Search", "query": query_v1},
        {"name": "Original Fallback", "query": query_v2},
        {"name": "Ultra-Broad Search", "query": query_v3} # Ø´Ø¨ÙƒØ© Ø§Ù„Ø£Ù…Ø§Ù† Ø§Ù„Ø£Ø®ÙŠØ±Ø©
    ]

    for attempt in search_attempts:
        try:
            logger.info(f"ğŸ” Attempting {attempt['name']}: '{attempt['query'][:60]}...'")
            encoded_query = urllib.parse.quote(attempt['query'])
            url = f"https://news.google.com/rss/search?q={encoded_query}&hl=en-US&gl=US&ceid=US:en"
            
            feed = feedparser.parse(url)
            
            if feed.entries:
                threads = []
                for entry in feed.entries[:4]:
                    threads.append({
                        "title": entry.title,
                        "link": entry.link
                    })
                logger.info(f"âœ… {attempt['name']} succeeded! Found {len(threads)} threads.")
                return threads
            else:
                logger.warning(f"âš ï¸ {attempt['name']} returned no results.")
                
        except Exception as e:
            logger.error(f"ğŸš¨ Error during {attempt['name']}: {e}")
            continue # Ø¬Ø±Ø¨ Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªØ§Ù„ÙŠ

    logger.error(f"âŒ All 3 search levels failed for: {keyword}")
    return []

def extract_smart_opinions(reddit_url):
    """
    Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ø¹Ù…ÙŠÙ‚: ÙŠØ³Ø­Ø¨ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§ØªØŒ Ø§Ù„ØµÙˆØ±ØŒ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§ØªØŒ Ø§Ù„Ù…Ø¹Ø§Ø±Ø¶ØŒ ÙˆØ§Ù„Ø£ÙƒÙˆØ§Ø¯ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ©.
    """
    try:
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø±Ø§Ø¨Ø· ÙˆØªØ­ÙˆÙŠÙ„Ù‡ Ø¥Ù„Ù‰ JSON
        clean_url = reddit_url.split("?")[0]
        json_url = f"{clean_url}.json" if not clean_url.endswith(".json") else clean_url

        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        
        r = requests.get(json_url, headers=headers, timeout=15)
        if r.status_code != 200: 
            logger.warning(f"âš ï¸ Failed to access: {json_url} (Status: {r.status_code})")
            return [], []

        data = r.json()
        
        # 1. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù†Ø´ÙˆØ± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ (Main Post)
        main_post = data[0]['data']['children'][0]['data']
        subreddit = main_post.get('subreddit_name_prefixed', "r/Reddit")
        post_title = main_post.get('title', 'Reddit Post')
        
        media_found = []

        # --- Ø£) Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ø§Ø±Ø¶ Ø§Ù„ØµÙˆØ± (Galleries) Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ ---
        if main_post.get('is_gallery') and 'media_metadata' in main_post:
            for media_id, meta in main_post['media_metadata'].items():
                if meta.get('status') == 'valid' and meta.get('e') == 'Image':
                    # ØªØ­ÙˆÙŠÙ„ Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ø¹Ø§ÙŠÙ†Ø© Ø¥Ù„Ù‰ Ø±Ø§Ø¨Ø· ØµÙˆØ±Ø© Ù…Ø¨Ø§Ø´Ø±
                    img_url = meta['s'].get('u', '').replace('preview.redd.it', 'i.redd.it')
                    if img_url:
                        media_found.append({
                            "type": "image",
                            "url": img_url,
                            "description": f"Gallery Evidence: {post_title[:60]}"
                        })

        # --- Ø¨) Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Reddit Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø© Ù…Ø¨Ø§Ø´Ø±Ø© ---
        if main_post.get('is_video') and main_post.get('media'):
            try:
                vid_url = main_post['media']['reddit_video']['fallback_url']
                media_found.append({
                    "type": "video",
                    "url": vid_url,
                    "description": f"User Video Evidence: {post_title[:60]}"
                })
            except: pass

        # --- Ø¬) Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø© (URL Overridden) ---
        if 'url_overridden_by_dest' in main_post:
            dest_url = main_post['url_overridden_by_dest']
            if any(ext in dest_url.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.mp4']):
                m_type = "video" if '.mp4' in dest_url.lower() else "image"
                media_found.append({
                    "type": m_type,
                    "url": dest_url,
                    "description": f"Direct Media Evidence: {post_title[:60]}"
                })

        # --- Ø¯) ØªØ­Ù„ÙŠÙ„ Ù†Øµ Ø§Ù„Ù…Ù†Ø´ÙˆØ± (Selftext) Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø±ÙˆØ§Ø¨Ø· Ù…Ø¶Ù…Ù†Ø© (BeautifulSoup) ---
        if 'selftext_html' in main_post and main_post['selftext_html']:
            # ÙÙƒ ØªØ±Ù…ÙŠØ² HTML ÙˆØªØ­Ù„ÙŠÙ„Ù‡
            html_content = main_post['selftext_html']
            # Reddit ÙŠØ±Ø³Ù„ HTML Ù…Ø±Ù…Ø²Ø§Ù‹ØŒ Ù†Ø­ØªØ§Ø¬ Ù„ÙÙƒ ØªØ±Ù…ÙŠØ²Ù‡ Ø£Ø­ÙŠØ§Ù†Ø§Ù‹
            soup = BeautifulSoup(html_content, 'html.parser')
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØªÙŠ ØªØ´ÙŠØ± Ù„ØµÙˆØ± Ø£Ùˆ ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø£Ùˆ ÙŠÙˆØªÙŠÙˆØ¨
            for a in soup.find_all('a', href=True):
                link = a['href']
                if re.search(r'\.(jpg|jpeg|png|gif|mp4)$', link.lower()):
                    m_type = "video" if '.mp4' in link.lower() else "image"
                    media_found.append({
                        "type": m_type,
                        "url": link,
                        "description": "Embedded Media Evidence"
                    })
                elif 'youtube.com/watch' in link or 'youtu.be/' in link:
                    media_found.append({
                        "type": "video",
                        "url": link,
                        "description": "Embedded YouTube Evidence"
                    })

        # 2. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª (Comments) ÙˆØ§Ù„Ø¢Ø±Ø§Ø¡ ÙˆØ§Ù„Ø£ÙƒÙˆØ§Ø¯
        comments_data = data[1]['data']['children']
        insights = []
        
        for comm in comments_data:
            c_data = comm.get('data', {})
            body = c_data.get('body', '')
            score = c_data.get('score', 0)
            author = c_data.get('author', 'User')
            
            if not body or len(body) < 50 or body in ["[deleted]", "[removed]"]:
                continue

            # --- Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£ÙƒÙˆØ§Ø¯ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© (Code Blocks) ---
            # Ù†Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙƒØªÙ„ Ø§Ù„Ù…Ø­Ø§Ø·Ø© Ø¨Ù€ ```
            code_blocks = re.findall(r'```(.*?)```', body, re.DOTALL)
            for code in code_blocks:
                clean_code = code.strip()
                if len(clean_code) > 15:
                    media_found.append({
                        "type": "code",
                        "content": clean_code,
                        "description": f"Technical Evidence (Code) by {author}"
                    })
            
            # --- ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ù…Ù† Ø§Ù„Ø£ÙƒÙˆØ§Ø¯ Ù‚Ø¨Ù„ Ø¥Ø±Ø³Ø§Ù„Ù‡ Ù„Ù„Ù€ AI ---
            # Ù†Ù‚ÙˆÙ… Ø¨Ø­Ø°Ù ÙƒØªÙ„ Ø§Ù„Ø£ÙƒÙˆØ§Ø¯ Ù…Ù† Ø§Ù„Ù†Øµ Ù„ÙŠØ¨Ù‚Ù‰ Ø§Ù„Ø±Ø£ÙŠ ÙÙ‚Ø·
            body_cleaned = re.sub(r'```.*?```', '[Technical Code Block]', body, flags=re.DOTALL)
            body_cleaned = body_cleaned.replace("\n", " ").strip()

            # ÙÙ„Ø§ØªØ± Ø§Ù„Ø¬ÙˆØ¯Ø©
            markers = ["i noticed", "in my experience", "battery", "bug", "glitch", "crash", "actually", "worth it", "slow", "fast", "update", "the problem is"]
            if any(m in body_cleaned.lower() for m in markers) or score > 5:
                insights.append({
                    "source_name": subreddit,
                    "author": author,
                    "text": body_cleaned[:500], # Ù†Ø£Ø®Ø° Ø£ÙˆÙ„ 500 Ø­Ø±Ù ÙÙ‚Ø·
                    "url": f"https://www.reddit.com{c_data.get('permalink', '')}",
                    "score": score
                })
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø¢Ø±Ø§Ø¡ Ø­Ø³Ø¨ Ø§Ù„Ø£Ø¹Ù„Ù‰ ØªØµÙˆÙŠØªØ§Ù‹
        insights.sort(key=lambda x: x['score'], reverse=True)
        return insights[:4], media_found

    except Exception as e:
        logger.error(f"âŒ Extraction Error at {reddit_url}: {e}")
        return [], []

def get_community_intel(keyword):
    """
    Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ: ÙŠØ¹ÙŠØ¯ ØªÙ‚Ø±ÙŠØ±Ø§Ù‹ Ù†ØµÙŠØ§Ù‹ Ù…Ù‡ÙŠÙƒÙ„Ø§Ù‹ + Ù‚Ø§Ø¦Ù…Ø© ÙˆØ³Ø§Ø¦Ø· ÙØ±ÙŠØ¯Ø©.
    """
    logger.info(f"ğŸ§  Starting Deep Intelligence Mining for: '{keyword}'...")
    threads = search_reddit_threads(keyword)
    
    if not threads: 
        return "", []
    
    all_insights = []
    all_media = []
    
    for thread in threads:
        if "reddit.com" in thread['link']:
            ops, media = extract_smart_opinions(thread['link'])
            all_insights.extend(ops)
            all_media.extend(media)
            time.sleep(0.8) # ØªØ£Ø®ÙŠØ± Ø¨Ø³ÙŠØ· Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø­Ø¸Ø±
            
    # --- Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø± ÙÙŠ Ø§Ù„ÙˆØ³Ø§Ø¦Ø· (Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø§Ø¨Ø· Ø£Ùˆ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙƒÙˆØ¯) ---
    unique_media_map = {}
    for item in all_media:
        # Ø¥Ø°Ø§ ÙƒØ§Ù† ÙƒÙˆØ¯ØŒ Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙƒÙ…ÙØªØ§Ø­ØŒ ÙˆØ¥Ø°Ø§ ÙƒØ§Ù† Ù…ÙŠØ¯ÙŠØ§ Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø±Ø§Ø¨Ø·
        key = item.get('url') if item['type'] != 'code' else item.get('content')
        if key and key not in unique_evidence_map:
            unique_media_map[key] = item
    
    unique_media = list(unique_media_map.values())
    
    if not all_insights and not unique_media:
        return "", []
    
    # --- Ø¨Ù†Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†ØµÙŠ Ø§Ù„Ù…Ù‡ÙŠÙƒÙ„ Ù„Ù„Ù€ AI ---
    report = "\n=== ğŸ“¢ REAL COMMUNITY FEEDBACK (TEXTUAL INSIGHTS) ===\n"
    report += "INSTRUCTIONS FOR AI WRITER: Use these real user quotes to add credibility, highlight bugs, or contrast marketing claims.\n"
    report += "CRITICAL: You MUST hyperlink the Subreddit name or the phrase 'community discussion' to the provided URL.\n\n"
    
    # Ø¥Ø²Ø§Ù„Ø© ØªÙƒØ±Ø§Ø± Ø§Ù„Ù†ØµÙˆØµ
    seen_texts = set()
    final_insights = []
    for ins in all_insights:
        if ins['text'] not in seen_texts:
            final_insights.append(ins)
            seen_texts.add(ins['text'])
    
    for i, item in enumerate(final_insights[:4]):
        report += f"--- INSIGHT {i+1} ---\n"
        report += f"SOURCE: {item['source_name']}\n"
        report += f"LINK: {item['url']}\n"
        report += f"USER EXPERIENCE: \"{item['text']}\"\n"
        report += f"COMMUNITY SCORE: {item['score']} upvotes\n\n"

    # Ø¥Ø¶Ø§ÙØ© Ù…Ù„Ø®Øµ Ø§Ù„Ø£Ø¯Ù„Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ© Ù„Ù„ØªÙ‚Ø±ÙŠØ± Ù„ÙŠØ¹Ø±Ù Ø§Ù„Ù€ AI Ø¨ÙˆØ¬ÙˆØ¯Ù‡Ø§
    if unique_media:
        report += "=== ğŸ–¼ï¸ AVAILABLE VISUAL/TECHNICAL EVIDENCE ===\n"
        report += "INSTRUCTIONS: I have extracted the following evidence. Use the placeholders [[EVIDENCE_TYPE_ID]] in your draft where appropriate.\n"
        for i, m in enumerate(unique_media):
            report += f"- Evidence {i+1}: Type={m['type']}, Description={m['description']}\n"

    return report, unique_media
