import requests
import urllib.parse
import logging
import feedparser
import time
import re

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù„ÙˆØ¬Ø±
logging.basicConfig(level=logging.INFO, format='%(asctime)s - [REDDIT-INTEL] - %(message)s')
logger = logging.getLogger("RedditIntel")

def search_reddit_threads(keyword):
    """
    ÙŠØ¨Ø­Ø« Ø¹Ù† Ù†Ù‚Ø§Ø´Ø§Øª Ø­Ù‚ÙŠÙ‚ÙŠØ© (Ù„ÙŠØ³Øª Ø£Ø®Ø¨Ø§Ø±Ø§Ù‹) Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙÙ„Ø§ØªØ± Ø¬ÙˆØ¬Ù„ Ø§Ù„Ø°ÙƒÙŠØ©.
    """
    search_query = f"site:reddit.com {keyword} (review OR 'after using' OR 'problem with' OR 'my thoughts' OR 'demo') -giveaway"
    encoded = urllib.parse.quote(search_query)
    
    url = f"https://news.google.com/rss/search?q={encoded}&hl=en-US&gl=US&ceid=US:en"
    
    try:
        feed = feedparser.parse(url)
        threads = []
        if feed.entries:
            for entry in feed.entries[:4]: 
                threads.append({
                    "title": entry.title,
                    "link": entry.link
                })
        return threads
    except Exception as e:
        logger.error(f"Search failed: {e}")
        return []

def extract_smart_opinions(reddit_url):
    """
    ÙŠØ³Ø­Ø¨ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª + Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ø§Ù„Ø¨ØµØ±ÙŠØ© (ÙÙŠØ¯ÙŠÙˆ/GIF) Ù…Ù† Ø§Ù„Ù…Ù†Ø´ÙˆØ±.
    """
    try:
        clean_url = reddit_url.split("?")[0]
        json_url = f"{clean_url}.json" if not clean_url.endswith(".json") else clean_url

        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0'
        }
        
        r = requests.get(json_url, headers=headers, timeout=10)
        if r.status_code != 200: return [], []

        data = r.json()
        
        # 1. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù†Ø´ÙˆØ± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ (Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ³Ø§Ø¦Ø·)
        main_post = data[0]['data']['children'][0]['data']
        subreddit = main_post.get('subreddit_name_prefixed', "Reddit")
        post_title = main_post.get('title', 'Reddit Post')
        
        media_found = []

        # Ø£) Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø±ÙˆØ§Ø¨Ø· Ù…Ø¨Ø§Ø´Ø±Ø© (ØµÙˆØ±/GIFs)
        if 'url_overridden_by_dest' in main_post:
            url = main_post['url_overridden_by_dest']
            if any(ext in url.lower() for ext in ['.jpg', '.png', '.gif', '.mp4']):
                m_type = "video" if url.endswith('.mp4') else "gif" if url.endswith('.gif') else "image"
                media_found.append({
                    "type": m_type,
                    "url": url,
                    "description": f"Community Demo: {post_title[:60]}"
                })

        # Ø¨) Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Reddit Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø© Ù…Ø¨Ø§Ø´Ø±Ø©
        if main_post.get('is_video') and main_post.get('media'):
            try:
                vid_url = main_post['media']['reddit_video']['fallback_url']
                media_found.append({
                    "type": "video",
                    "url": vid_url,
                    "description": f"User Video Review: {post_title[:60]}"
                })
            except: pass

        # 2. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª (Ø§Ù„Ù†ØµÙˆØµ)
        comments_data = data[1]['data']['children']
        insights = []
        
        for comm in comments_data:
            c_data = comm.get('data', {})
            body = c_data.get('body', '')
            score = c_data.get('score', 0)
            
            if len(body) > 60 and body not in ["[deleted]", "[removed]"]:
                markers = ["i noticed", "in my experience", "battery", "bug", "glitch", "crash", "actually", "worth it", "slow", "fast", "update"]
                if any(m in body.lower() for m in markers) or score > 5:
                    insights.append({
                        "source_name": subreddit,
                        "author": c_data.get('author', 'User'),
                        "text": body[:500].replace("\n", " "),
                        "url": f"https://www.reddit.com{c_data.get('permalink', '')}",
                        "score": score
                    })
        
        insights.sort(key=lambda x: x['score'], reverse=True)
        return insights[:3], media_found # Ù†Ø¹ÙŠØ¯ (Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§ØªØŒ Ø§Ù„ÙˆØ³Ø§Ø¦Ø·)

    except Exception as e:
        return [], []

def get_community_intel(keyword):
    """
    Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ: ÙŠØ¹ÙŠØ¯ ØªÙ‚Ø±ÙŠØ±Ø§Ù‹ Ù†ØµÙŠØ§Ù‹ + Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ø§Ù„Ø¨ØµØ±ÙŠØ©.
    """
    logger.info(f"ğŸ§  Mining Reddit intelligence & visuals for: '{keyword}'...")
    threads = search_reddit_threads(keyword)
    
    if not threads: return "", []
    
    all_insights = []
    all_media = [] # Ù‚Ø§Ø¦Ù…Ø© Ø¬Ø¯ÙŠØ¯Ø© Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„ÙˆØ³Ø§Ø¦Ø·
    
    for thread in threads:
        if "reddit.com" in thread['link']:
            ops, media = extract_smart_opinions(thread['link'])
            all_insights.extend(ops)
            all_media.extend(media) # Ø¥Ø¶Ø§ÙØ© Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ø§Ù„Ù…ÙƒØªØ´ÙØ©
            time.sleep(0.5)
            
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ø§Ù„Ù…ÙƒØ±Ø±Ø©
    unique_media = list({v['url']:v for v in all_media}.values())
    
    if not all_insights: return "", unique_media
    
    # Ø¨Ù†Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†ØµÙŠ
    report = "\n=== ğŸ“¢ REAL COMMUNITY FEEDBACK (INTEGRATE THIS) ===\n"
    report += "INSTRUCTIONS: Use these real user quotes to validate or criticize the news. \n"
    report += "CRITICAL: When citing, you MUST hyperlink the text 'community discussion' or the Subreddit name (e.g., r/Gadgets) to the provided URL.\n\n"
    
    unique_insights = list({v['text']:v for v in all_insights}.values())[:4]
    
    for i, item in enumerate(unique_insights):
        report += f"--- INSIGHT {i+1} ---\n"
        report += f"SOURCE: {item['source_name']} (Use this specific name)\n"
        report += f"LINK: {item['url']} (Link strictly to this)\n"
        report += f"USER SAID: \"{item['text']}\"\n"
        
    # Ù†Ø¹ÙŠØ¯ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†ØµÙŠ + Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø·
    return report, unique_media
