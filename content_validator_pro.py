import re
import requests
import logging
import json
from bs4 import BeautifulSoup
from google.genai import types
from tenacity import retry, stop_after_attempt, wait_fixed
from urllib.parse import urlparse
from google import genai

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù„ÙˆØ¬Ø± Ø§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠ ÙˆØ§Ù„Ù…ØªÙ‚Ø¯Ù…
logging.basicConfig(level=logging.INFO, format='%(asctime)s - [CORE-SURGEON-3.0] - %(message)s')
logger = logging.getLogger("CoreSurgeon")

class AdvancedContentValidator:
    def __init__(self, google_client, model_name="gemini-2.5-flash"):
        self.client = google_client
        self.model_name = model_name
        self.session = requests.Session()
        # Ù‡ÙˆÙŠØ© Ù…Ø®ØµØµØ© Ù„Ù„Ù…Ø¯Ù‚Ù‚ Ù„Ø¶Ù…Ø§Ù† Ø¹Ø¯Ù… Ø­Ø¸Ø±Ù‡ Ø¹Ù†Ø¯ ÙØ­Øµ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36 ProValidator/3.0'
        })

    def _normalize(self, text):
        """
        Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø© ÙˆØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ù„Ø­Ø±ÙˆÙ ØµØºÙŠØ±Ø© Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©.
        (Ù…ÙˆØ¬ÙˆØ¯Ø© ÙƒÙ…Ø§ ÙÙŠ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù‚Ø¯ÙŠÙ… Ù„Ø¶Ù…Ø§Ù† Ø§Ù„ØªÙˆØ§ÙÙ‚ÙŠØ©)
        """
        return re.sub(r'\s+', ' ', text.strip().lower())

    # ==============================================================================
    # 1. PROACTIVE FACT SURGERY (Ø§Ù„Ø¬Ø±Ø§Ø­Ø© Ø§Ù„ÙˆÙ‚Ø§Ø¦ÙŠØ© Ù„Ù„Ø­Ù‚Ø§Ø¦Ù‚)
    # ==============================================================================
    def perform_fact_surgery(self, html_content, full_source_text):
        """
        ÙŠÙ‚ÙˆÙ… Ø¨ÙØ­Øµ ÙƒÙ„ Ø§Ø¯Ø¹Ø§Ø¡ Ø±Ù‚Ù…ÙŠ Ø£Ùˆ ØªÙ‚Ù†ÙŠ ÙˆØªØµØ­ÙŠØ­Ù‡ ÙÙˆØ±Ø§Ù‹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù….
        Ø¥Ø°Ø§ ÙˆØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø© "Ù‡Ù„ÙˆØ³Ø©"ØŒ ÙŠÙ‚ÙˆÙ… Ø¨Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØªÙ‡Ø§ Ù„ØªÙƒÙˆÙ† Ø±Ø£ÙŠØ§Ù‹ Ø§Ø­ØªØ±Ø§ÙÙŠØ§Ù‹.
        """
        soup = BeautifulSoup(html_content, 'html.parser')
        # Ù†Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„ÙÙ‚Ø±Ø§Øª ÙˆØ§Ù„Ø¬Ø¯Ø§ÙˆÙ„ ÙˆØ§Ù„Ø®Ù„Ø§ÙŠØ§ Ù„Ø£Ù†Ù‡Ø§ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ "Ø§Ù„Ø°Ù‡Ø¨ Ø§Ù„ØªÙ‚Ù†ÙŠ"
        elements = soup.find_all(['p', 'td', 'li', 'span', 'h3'])
        
        chunks_to_verify = []
        # Regex Ù…ØªØ·ÙˆØ± Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø£Ø±Ù‚Ø§Ù…ØŒ Ø§Ù„Ù†Ø³Ø¨ØŒ Ø§Ù„Ø£Ø³Ø¹Ø§Ø±ØŒ Ø§Ù„Ø¥ØµØ¯Ø§Ø±Ø§ØªØŒ ÙˆØ§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„Ø­Ø³Ø§Ø³Ø©
        pattern = r'(\d+%?|\$\d+|\bv\d+\.\d+|\d+\s(hours|GB|TB)|\b(vs|better than|faster than|release date)\b)'
        
        for el in elements:
            text = el.get_text()
            if re.search(pattern, text, re.IGNORECASE):
                chunks_to_verify.append(str(el))

        if not chunks_to_verify:
            return html_content

        logger.info(f"ğŸ’‰ Starting Fact Surgery on {len(chunks_to_verify)} sensitive elements...")
        
        prompt = f"""
        TASK: Technical Content Surgery.
        TRUTH DATA (Raw Sources): {full_source_text[:15000]}
        DRAFT HTML ELEMENTS: {json.dumps(chunks_to_verify)}
        
        INSTRUCTIONS:
        1. For each HTML element, compare every number, date, and technical claim with the TRUTH DATA.
        2. IF WRONG: Rewrite the entire HTML element with the CORRECT information. Preserve all original HTML tags (<a>, <strong>, etc.).
        3. IF HALLUCINATED (claim is not in TRUTH DATA): Do NOT delete. Rewrite it to be a logical professional observation based on what IS in the source. (e.g., If source says a chip is fast, you can infer 'This could improve gaming performance').
        4. IF CORRECT: Return it exactly as is.
        
        OUTPUT: JSON dictionary {{ "original_html_string": "corrected_html_string" }}
        """
        try:
            resp = self.client.models.generate_content(
                model=self.model_name, 
                contents=prompt,
                config=types.GenerateContentConfig(response_mime_type="application/json")
            )
            corrections = json.loads(resp.text)
            
            final_html = str(soup)
            for original, corrected in corrections.items():
                if original in final_html and corrected:
                    # ØªÙ†Ø¸ÙŠÙ Ù…Ø®Ø±Ø¬Ø§Øª Ø§Ù„Ù€ AI Ù…Ù† Ø£ÙŠ Ø²ÙˆØ§Ø¦Ø¯
                    clean_fix = re.sub(r'</?(html|body|head)>', '', corrected, flags=re.IGNORECASE)
                    final_html = final_html.replace(original, clean_fix)
            
            return final_html
        except Exception as e:
            logger.error(f"âŒ Fact Surgery Failed: {e}")
            return html_content

    # ==============================================================================
    # 2. WIDGET RECONSTRUCTION (Ø¥Ø¹Ø§Ø¯Ø© Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„ØªØ§Ù„ÙØ©)
    # ==============================================================================
    def rebuild_damaged_widgets(self, html_content, full_source_text):
        """
        ÙŠÙØ­Øµ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ ÙˆØ§Ù„Ù‚ÙˆØ§Ø¦Ù…. Ø¥Ø°Ø§ ÙˆØ¬Ø¯Ù‡Ø§ ÙØ§Ø±ØºØ© Ø£Ùˆ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ "N/A" Ø¨ÙƒØ«Ø±Ø©ØŒ 
        ÙŠØ¹ÙŠØ¯ Ø¨Ù†Ø§Ø¡Ù‡Ø§ Ù…Ù† Ø§Ù„ØµÙØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©.
        """
        if "comparison-table" not in html_content:
            return html_content

        soup = BeautifulSoup(html_content, 'html.parser')
        modified = False
        
        # ÙØ­Øµ Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¬Ø¯ÙˆÙ„
        table = soup.find('table', class_='comparison-table')
        if table:
            cells = table.find_all('td')
            # Ø¥Ø°Ø§ ÙƒØ§Ù† Ù†ØµÙ Ø§Ù„Ø¬Ø¯ÙˆÙ„ ØªÙ‚Ø±ÙŠØ¨Ø§Ù‹ ÙØ§Ø±ØºØ§Ù‹ Ø£Ùˆ ÙŠØ­ØªÙˆÙŠ ÙƒÙ„Ù…Ø§Øª ØªØ§ÙÙ‡Ø©
            if cells:
                empty_cells = [c for c in cells if len(c.get_text(strip=True)) < 2 or "n/a" in c.get_text(strip=True).lower()]
                if len(empty_cells) > (len(cells) / 2):
                    logger.warning("ğŸ”¨ Comparison table is low quality. Rebuilding from source...")
                    new_table = self._generate_element_from_ai("Comparison Table", full_source_text)
                    if new_table:
                        table.replace_with(BeautifulSoup(new_table, 'html.parser'))
                        modified = True

        return str(soup) if modified else html_content

    @retry(stop=stop_after_attempt(2), wait=wait_fixed(2))
    def _generate_element_from_ai(self, element_type, source_text):
        prompt = f"REBUILD TASK: Create a high-quality HTML {element_type} using ONLY facts from: {source_text[:8000]}. Use clean CSS classes like 'comparison-table'."
        try:
            resp = self.client.models.generate_content(model=self.model_name, contents=prompt)
            return resp.text.replace("```html", "").replace("```", "").strip()
        except: return None

    # ==============================================================================
    # 3. INTELLIGENT LINK RESTORATION (ØªØ±Ù…ÙŠÙ… Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…ÙƒØ³ÙˆØ±Ø©)
    # ==============================================================================
    def restore_link_integrity(self, html_content, sources_metadata):
        """
        Ø¥Ø°Ø§ ÙˆØ¬Ø¯ Ø±Ø§Ø¨Ø·Ø§Ù‹ Ù…ÙƒØ³ÙˆØ±Ø§Ù‹ØŒ ÙŠØ±Ù…Ù…Ù‡ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø­Ø°ÙÙ‡ Ø£Ùˆ Ø§Ø³ØªØ¨Ø¯Ø§Ù„Ù‡ Ø¨Ø±Ø§Ø¨Ø· Ø¹Ø´ÙˆØ§Ø¦ÙŠ.
        """
        soup = BeautifulSoup(html_content, 'html.parser')
        links = soup.find_all('a', href=True)
        
        for link in links:
            url = link['href']
            # ØªØ®Ø·ÙŠ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ© ÙˆØ±ÙˆØ§Ø¨Ø· Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ
            if any(x in url for x in ["latestai.me", "facebook.com", "instagram.com", "x.com", "youtube.com", "reddit.com", "pinterest.com"]) or url.startswith('#'):
                continue

            try:
                # Ù…Ø­Ø§ÙˆÙ„Ø© ÙØ­Øµ Ø§Ù„Ø±Ø§Ø¨Ø· Ø¨Ø³Ø±Ø¹Ø©
                r = self.session.head(url, timeout=3, allow_redirects=True)
                if r.status_code >= 400: raise Exception("Dead Link")
            except:
                logger.warning(f"ğŸ©¹ Healing broken link: {url}")
                parsed_url = urlparse(url)
                target_domain = parsed_url.netloc.replace('www.', '')
                
                # Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„ØªØ±Ù…ÙŠÙ…: Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø±Ø§Ø¨Ø· ØµØ­ÙŠØ­ Ù„Ù†ÙØ³ Ø§Ù„Ø¯ÙˆÙ…ÙŠÙ† ÙÙŠ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚Ø©
                replacement_url = None
                for src in sources_metadata:
                    if target_domain in src['url']:
                        replacement_url = src['url']
                        break
                
                if replacement_url:
                    link['href'] = replacement_url
                    logger.info(f"âœ… Link restored to source: {replacement_url}")
                else:
                    # Ø¥Ø°Ø§ Ù„Ù… ÙŠØ¬Ø¯ØŒ ÙŠØ¨Ø­Ø« Ø¹Ù† Ø£ÙŠ Ø°ÙƒØ± Ù„Ø§Ø³Ù… Ø§Ù„Ø¯ÙˆÙ…ÙŠÙ† ÙˆÙŠØ±Ø¨Ø·Ù‡ Ø¨Ø£ÙˆÙ„ Ù…ØµØ¯Ø± Ø¯Ø³Ù…
                    if sources_metadata:
                        link['href'] = sources_metadata[0]['url']
                        logger.info(f"âœ… Link pivoted to main source: {sources_metadata[0]['url']}")
        
        return str(soup)

    # ==============================================================================
    # 4. QUOTE VERIFIER & ANCHORING (Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ø§Ù‚ØªØ¨Ø§Ø³Ø§Øª ÙˆØªØ«Ø¨ÙŠØªÙ‡Ø§)
    # ==============================================================================
    def verify_quotes(self, html_content, source_text):
        """
        ÙŠØªØ£ÙƒØ¯ Ø£Ù† ÙƒÙ„ Ø§Ù‚ØªØ¨Ø§Ø³ Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø§Ù„Ù†Øµ Ø§Ù„Ù…ØµØ¯Ø±ÙŠ. Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…Ø²ÙŠÙØ§Ù‹ØŒ ÙŠØ³ØªØ¨Ø¯Ù„Ù‡ Ø¨Ø¢Ø®Ø± Ø­Ù‚ÙŠÙ‚ÙŠ.
        """
        soup = BeautifulSoup(html_content, 'html.parser')
        quotes = soup.find_all('blockquote')
        
        for bq in quotes:
            quote_text = bq.get_text(strip=True)
            words = quote_text.split()
            # ÙØ­Øµ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø¬Ø²Ø¡ Ù…Ù† Ø§Ù„Ø§Ù‚ØªØ¨Ø§Ø³ Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹ ÙÙŠ Ø§Ù„Ù…ØµØ¯Ø± Ù„Ù…Ù†Ø¹ Ø§Ù„Ù‡Ù„ÙˆØ³Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©
            if len(words) > 3 and " ".join(words[:4]).lower() not in source_text.lower():
                logger.warning("âš ï¸ Replacing hallucinated quote with a real one...")
                real_quote = self._find_real_quote_from_ai(source_text)
                if real_quote:
                    # Ø§Ø³ØªØ®Ø¯Ø§Ù… beautifulsoup Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„ÙƒÙˆØ¯ Ø³Ù„ÙŠÙ… Ù‚Ø¨Ù„ Ø§Ù„Ø¥Ø¶Ø§ÙØ©
                    bq.replace_with(BeautifulSoup(real_quote, 'html.parser'))
                else:
                    bq.decompose() # Ø­Ø°Ù Ø§Ù„Ø§Ù‚ØªØ¨Ø§Ø³ Ø§Ù„Ù…Ø²ÙŠÙ Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ Ø¨Ø¯ÙŠÙ„Ø§Ù‹
        
        return str(soup)

    def _find_real_quote_from_ai(self, source_text):
        prompt = f"EXTRACT VERBATIM QUOTE: Find one powerful, real sentence from this text: {source_text[:5000]}. Return it as a single HTML <blockquote> with a <footer> if possible."
        try:
            resp = self.client.models.generate_content(model=self.model_name, contents=prompt)
            return resp.text.replace("```html", "").replace("```", "").strip()
        except: return None

    # ==============================================================================
    # MASTER RUNNER (Ø§Ù„Ù…Ù†ÙØ° Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ)
    # ==============================================================================
    def run_professional_validation(self, html_content, full_source_text, sources_metadata):
        logger.info("ğŸ›¡ï¸ CORE SURGEON 3.0: COMMENCING FULL RESTORATION...")
        
        # 1. Ø¬Ø±Ø§Ø­Ø© Ø§Ù„Ø­Ù‚Ø§Ø¦Ù‚ (Active Correction) - Ø§Ù„Ø£Ù‡Ù… Ø£ÙˆÙ„Ø§Ù‹
        html = self.perform_fact_surgery(html_content, full_source_text)
        
        # 2. Ø§Ù„Ù‡ÙŠÙƒÙ„ ÙˆØ§Ù„ÙˆÙŠØ¯Ø¬Ø§Øª
        html = self.rebuild_damaged_widgets(html, full_source_text)
        
        # 3. Ø§Ù„Ø§Ù‚ØªØ¨Ø§Ø³Ø§Øª
        html = self.verify_quotes(html, full_source_text)
        
        # 4. Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
        html = self.restore_link_integrity(html, sources_metadata)
        
        # ØªÙ†Ø¸ÙŠÙ Ù†Ù‡Ø§Ø¦ÙŠ ØµØ§Ø±Ù… Ù„Ù„Ù€ HTML Ù…Ù† Ø£ÙŠ Ø²ÙˆØ§Ø¦Ø¯
        html = re.sub(r'</?(html|body|head|meta|title)>', '', html, flags=re.IGNORECASE)
        html = re.sub(r'>\s+<', '><', html).strip() # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø¨ÙŠÙ† Ø§Ù„ØªØ§Ø¬Ø§Øª
        
        logger.info("âœ… RESTORATION COMPLETE. Article is clinically clean and verified.")
        return html
