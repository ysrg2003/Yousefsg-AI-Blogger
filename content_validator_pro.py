import re
import requests
import logging
import json
from bs4 import BeautifulSoup
from google.genai import types
from tenacity import retry, stop_after_attempt, wait_fixed

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù„ÙˆØ¬Ø± Ø§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - [AUTO-HEALER] - %(message)s')
logger = logging.getLogger("AutoHealer")

class AdvancedContentValidator:
    def __init__(self, google_client, model_name="models/gemini-2.5-flash"):
        self.client = google_client
        self.model_name = model_name

    def _normalize(self, text):
        return re.sub(r'\s+', ' ', text.strip().lower())

    # ==============================================================================
    # 1. STRUCTURAL HEALING (Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…ÙÙ‚ÙˆØ¯)
    # ==============================================================================
    def ensure_structure_integrity(self, html_content, required_elements, full_source_text):
        """
        ÙŠÙØ­Øµ ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ø¥Ø¬Ø¨Ø§Ø±ÙŠØ© (Ø¬Ø¯ÙˆÙ„ØŒ ÙˆÙŠØ¯Ø¬Øª).
        Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ÙÙ‚ÙˆØ¯Ø©ØŒ ÙŠÙ‚ÙˆÙ… Ø¨ØªÙˆÙ„ÙŠØ¯Ù‡Ø§ ÙˆØ­Ù‚Ù†Ù‡Ø§.
        """
        soup = BeautifulSoup(html_content, 'html.parser')
        modified = False

        # Ø£ÙŠÙ† Ù†Ø­Ù‚Ù† Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©ØŸ (Ø¨Ø¹Ø¯ Ø£ÙˆÙ„ H2 ØºØ§Ù„Ø¨Ø§Ù‹)
        injection_point = soup.find('h2')
        if not injection_point:
            injection_point = soup.find('p')

        # 1. ÙØ­Øµ Ø§Ù„Ø¬Ø¯ÙˆÙ„
        if "comparison-table" not in html_content:
            logger.warning("âš ï¸ Critical: Comparison Table Missing. Initiating regeneration...")
            table_html = self._generate_missing_element("Comparison Table", full_source_text)
            if table_html:
                # Ù†Ø­Ù‚Ù† Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø¨Ø¹Ø¯ Ù†Ù‚Ø·Ø© Ø§Ù„Ø­Ù‚Ù†
                new_tag = BeautifulSoup(table_html, 'html.parser')
                injection_point.insert_after(new_tag)
                modified = True
                logger.info("âœ… Fixed: Comparison Table injected successfully.")

        # 2. ÙØ­Øµ Ø¹Ù†ØµØ± Ø§Ù„Ø«Ù‚Ø© (Widget)
        widgets = ['code-snippet', 'specs-box', 'roi-box', 'pros-cons-grid']
        has_widget = any(w in html_content for w in widgets)
        
        if not has_widget:
            logger.warning("âš ï¸ Critical: Authority Widget Missing. Initiating regeneration...")
            # Ù†Ø­Ø¯Ø¯ Ù†ÙˆØ¹ Ø§Ù„ÙˆÙŠØ¯Ø¬Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ (ØªØ®Ù…ÙŠÙ† Ø°ÙƒÙŠ)
            widget_type = "Pros & Cons Grid" # Ø§ÙØªØ±Ø§Ø¶ÙŠ
            if "code" in full_source_text.lower(): widget_type = "Code Snippet"
            elif "battery" in full_source_text.lower() or "specs" in full_source_text.lower(): widget_type = "Specs Box"
            
            widget_html = self._generate_missing_element(widget_type, full_source_text)
            if widget_html:
                injection_point.insert_after(BeautifulSoup(widget_html, 'html.parser'))
                modified = True
                logger.info(f"âœ… Fixed: {widget_type} injected successfully.")

        return str(soup) if modified else html_content

    @retry(stop=stop_after_attempt(2), wait=wait_fixed(2))
    def _generate_missing_element(self, element_type, source_text):
        """ÙŠØ·Ù„Ø¨ Ù…Ù† AI ØªÙˆÙ„ÙŠØ¯ ÙƒÙˆØ¯ HTML Ù„Ù„Ø¹Ù†ØµØ± Ø§Ù„Ù…ÙÙ‚ÙˆØ¯ ÙÙ‚Ø·"""
        prompt = f"""
        TASK: Generate missing HTML element.
        ELEMENT TYPE: {element_type}
        SOURCE DATA: {source_text[:5000]}
        
        REQUIREMENTS:
        - Generate ONLY the HTML for the requested element.
        - Strictly follow these CSS classes:
          - Table: <div class="table-wrapper"><table class="comparison-table">...</table></div>
          - Code: <div class="code-snippet">...</div>
          - Specs: <div class="specs-box">...</div>
          - Pros/Cons: <div class="pros-cons-grid">...</div>
        - Populate with REAL data from source.
        
        OUTPUT: HTML String ONLY.
        """
        resp = self.client.models.generate_content(model=self.model_name, contents=prompt)
        return resp.text.replace("```html", "").replace("```", "").strip()

    # ==============================================================================
    # 2. FACT HEALING (ØªØµØ­ÙŠØ­ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… ÙˆØ§Ù„Ø­Ù‚Ø§Ø¦Ù‚)
    # ==============================================================================
    def verify_and_heal_facts(self, html_content, source_text):
        """
        ÙŠØ³ØªØ®Ø±Ø¬ Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù…ØŒ ÙˆÙŠØ·Ù„Ø¨ Ù…Ù† AI Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù†Ù‡Ø§ ÙˆØªØµØ­ÙŠØ­Ù‡Ø§ Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±.
        """
        soup = BeautifulSoup(html_content, 'html.parser')
        # Ù†ÙØ­Øµ Ø§Ù„ÙÙ‚Ø±Ø§Øª ÙÙ‚Ø· Ù„ØªÙˆÙÙŠØ± Ø§Ù„ÙˆÙ‚Øª
        paragraphs = soup.find_all('p')
        
        suspicious_sentences = []
        
        # Regex Ù„Ù„Ø£Ø±Ù‚Ø§Ù… (ÙŠØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø³Ù†ÙˆØ§Øª 2020-2030)
        number_pattern = r'\b(?!(?:202[0-9]|2030)\b)\d+(?:\.\d+)?'

        for p in paragraphs:
            text = p.get_text()
            if re.search(number_pattern, text):
                # Ø¥Ø°Ø§ ÙˆØ¬Ø¯Ù†Ø§ Ø±Ù‚Ù…Ø§Ù‹ØŒ Ù‡Ø°Ù‡ Ø¬Ù…Ù„Ø© Ø­Ø³Ø§Ø³Ø© ØªØ­ØªØ§Ø¬ ØªØ¯Ù‚ÙŠÙ‚
                suspicious_sentences.append(str(p))

        if not suspicious_sentences:
            return html_content

        # Ù†Ø±Ø³Ù„ Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ù…Ø´Ø¨ÙˆÙ‡Ø© Ù„Ù„Ù€ AI Ø¯ÙØ¹Ø© ÙˆØ§Ø­Ø¯Ø© Ù„ÙŠØµØ­Ø­Ù‡Ø§
        logger.info(f"ğŸ” Audit: Checking {len(suspicious_sentences)} paragraphs containing numbers...")
        
        correction_map = self._ai_batch_fact_check(suspicious_sentences, source_text)
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØµØ­ÙŠØ­Ø§Øª
        new_html = str(soup)
        fixed_count = 0
        for original, corrected in correction_map.items():
            if original != corrected:
                # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø°ÙƒÙŠ (Ù‚Ø¯ ÙŠÙØ´Ù„ Ø¥Ø°Ø§ ØªØºÙŠØ± Ø§Ù„Ù€ HTML Ù‚Ù„ÙŠÙ„Ø§Ù‹ØŒ Ù„Ø°Ø§ Ù†Ø³ØªØ®Ø¯Ù… replace Ø¨Ø­Ø°Ø±)
                # Ø§Ù„Ø£ÙØ¶Ù„: Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ù†Øµ Ø¯Ø§Ø®Ù„ Ø§Ù„ØªØ§Ø¬ØŒ Ù„ÙƒÙ† Ù‡Ù†Ø§ Ø³Ù†Ø³ØªØ¨Ø¯Ù„ Ø§Ù„Ø³ØªØ±ÙŠÙ†Øº
                if original in new_html: # ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯Ù‡
                     # ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØªØ§Ø¬Ø§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø© Ø§Ù„ØªÙŠ Ù‚Ø¯ ÙŠØ¶ÙŠÙÙ‡Ø§ AI
                    clean_corrected = corrected.replace('<html>', '').replace('</html>', '').replace('<body>', '')
                    new_html = new_html.replace(original, clean_corrected)
                    fixed_count += 1

        if fixed_count > 0:
            logger.info(f"âœ… Healed: {fixed_count} factual errors corrected.")
            
        return new_html

    def _ai_batch_fact_check(self, sentences_html, source_text):
        """
        ÙŠØ±Ø³Ù„ Ù‚Ø§Ø¦Ù…Ø© Ø¬Ù…Ù„ HTML ÙˆÙŠØ·Ù„Ø¨ Ù‚Ø§Ù…ÙˆØ³Ø§Ù‹ Ø¨Ø§Ù„ØªØµØ­ÙŠØ­Ø§Øª.
        """
        prompt = f"""
        TASK: Fact-Check & Fix.
        SOURCE TEXT (TRUTH): {source_text[:15000]}
        
        INPUT HTML SNIPPETS TO CHECK:
        {json.dumps(sentences_html)}
        
        INSTRUCTIONS:
        1. For each HTML snippet, check if the NUMBERS or CLAIMS match the Source Text.
        2. IF CORRECT: Return it exactly as is.
        3. IF WRONG/HALLUCINATED: Rewrite the text with the CORRECT number/fact from source. Keep HTML tags (<p>, <a>) intact.
        4. IF NOT IN SOURCE AT ALL: Rewrite it to be vague/safe (e.g., change "500mAh" to "a large battery") OR remove the sentence if it's a lie.
        
        OUTPUT: JSON Object {{ "original_html_string": "corrected_html_string" }}
        """
        try:
            resp = self.client.models.generate_content(
                model=self.model_name, 
                contents=prompt,
                config=types.GenerateContentConfig(response_mime_type="application/json")
            )
            return json.loads(resp.text)
        except Exception as e:
            logger.error(f"âŒ Fact Check Error: {e}")
            return {} # ÙÙŠ Ø­Ø§Ù„ Ø§Ù„ÙØ´Ù„ØŒ Ù„Ø§ Ù†ØºÙŠØ± Ø´ÙŠØ¦Ø§Ù‹

    # ==============================================================================
    # 3. LINK HEALING (Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…ÙƒØ³ÙˆØ±Ø©)
    # ==============================================================================
    def heal_broken_links(self, html_content, valid_sources_list):
        """
        Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø±Ø§Ø¨Ø· 404ØŒ ÙŠØ­Ø§ÙˆÙ„ Ø§Ø³ØªØ¨Ø¯Ø§Ù„Ù‡ Ø¨Ø±Ø§Ø¨Ø· ØµØ­ÙŠØ­ Ù…Ù† Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ØµØ§Ø¯Ø± (valid_sources_list).
        """
        soup = BeautifulSoup(html_content, 'html.parser')
        links = soup.find_all('a', href=True)
        headers = {'User-Agent': 'Mozilla/5.0'}
        
        healed_count = 0
        
        for link in links:
            url = link['href']
            if "blogger" in url or "#" in url: continue
            
            is_dead = False
            try:
                r = requests.head(url, headers=headers, timeout=3)
                if r.status_code >= 400: is_dead = True
            except: is_dead = True
            
            if is_dead:
                logger.warning(f"âš ï¸ Dead Link: {url}. Attempting to recover...")
                
                # Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø¹Ù„Ø§Ø¬:
                # Ù†Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø£ØµÙ„ÙŠØ© Ø¹Ù† Ø±Ø§Ø¨Ø· Ù…Ù† Ù†ÙØ³ Ø§Ù„Ø¯ÙˆÙ…ÙŠÙ†
                # Ø£Ùˆ Ù†Ø³ØªØ¨Ø¯Ù„Ù‡ Ø¨Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„Ù…Ù‚Ø§Ù„ (Ø£ÙˆÙ„ Ù…ØµØ¯Ø±)
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¯ÙˆÙ…ÙŠÙ† Ø§Ù„Ù…ÙƒØ³ÙˆØ±
                try: broken_domain = re.search(r'https?://([^/]+)', url).group(1)
                except: broken_domain = ""
                
                replacement_url = None
                
                # 1. Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ø¯ÙˆÙ…ÙŠÙ†
                if broken_domain:
                    for src in valid_sources_list:
                        if broken_domain in src['url']:
                            replacement_url = src['url']
                            break
                
                # 2. Ø¥Ø°Ø§ ÙØ´Ù„ØŒ Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…ØµØ¯Ø± Ø§Ù„Ø£Ù‚ÙˆÙ‰ (Ø§Ù„Ø£ÙˆÙ„)
                if not replacement_url and valid_sources_list:
                    replacement_url = valid_sources_list[0]['url']
                
                if replacement_url:
                    link['href'] = replacement_url
                    logger.info(f"âœ… Healed Link: Swapped dead URL with {replacement_url}")
                    healed_count += 1
                else:
                    # Ø§Ù„Ø­Ù„ Ø§Ù„Ø£Ø®ÙŠØ±: Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±Ø§Ø¨Ø·
                    link.replace_with(link.text)
                    
        return str(soup)

    # ==============================================================================
    # 4. QUOTE HEALING (Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ø§Ù‚ØªØ¨Ø§Ø³Ø§Øª Ø§Ù„Ù…Ø²ÙŠÙØ©)
    # ==============================================================================
    def verify_and_swap_quotes(self, html_content, source_text):
        soup = BeautifulSoup(html_content, 'html.parser')
        quotes = soup.find_all('blockquote')
        
        for bq in quotes:
            quote_text = bq.get_text()
            # ÙØ­Øµ Ø¨Ø³ÙŠØ·: Ù‡Ù„ Ø¬Ø²Ø¡ Ù…Ù† Ø§Ù„Ù†Øµ Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø§Ù„Ù…ØµØ¯Ø±ØŸ
            # Ù†Ø£Ø®Ø° Ø£ÙƒØ¨Ø± 4 ÙƒÙ„Ù…Ø§Øª Ù…ØªØªØ§Ù„ÙŠØ© ÙˆÙ†Ø¨Ø­Ø« Ø¹Ù†Ù‡Ø§
            words = quote_text.split()
            is_fake = True
            if len(words) > 5:
                chunk = " ".join(words[3:7]) # Ø¹ÙŠÙ†Ø©
                if chunk.lower() in source_text.lower():
                    is_fake = False
            
            if is_fake:
                logger.warning(f"âš ï¸ Fake Quote Detected. Finding a REAL substitute...")
                
                # Ù†Ø·Ù„Ø¨ Ù…Ù† AI Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù‚ØªØ¨Ø§Ø³ Ø­Ù‚ÙŠÙ‚ÙŠ Ø¨Ø¯ÙŠÙ„
                real_quote_html = self._find_real_quote(source_text)
                
                if real_quote_html and "<blockquote>" in real_quote_html:
                    # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ù€ Blockquote Ø§Ù„Ù‚Ø¯ÙŠÙ… Ø¨Ø§Ù„Ø¬Ø¯ÙŠØ¯
                    new_tag = BeautifulSoup(real_quote_html, 'html.parser')
                    bq.replace_with(new_tag)
                    logger.info("âœ… Fixed: Replaced fake quote with real one.")
                else:
                    # Ø¥Ø°Ø§ ÙØ´Ù„ ÙÙŠ Ø¥ÙŠØ¬Ø§Ø¯ Ø¨Ø¯ÙŠÙ„ØŒ Ù†Ø­Ø°ÙÙ‡
                    bq.decompose()
                    
        return str(soup)

    def _find_real_quote(self, source_text):
        prompt = f"""
        TASK: Extract a VERBATIM Quote.
        SOURCE: {source_text[:10000]}
        INSTRUCTION: Find ONE strong, real sentence/quote from the text representing the main opinion.
        FORMAT: Return HTML <blockquote>...</blockquote> with <footer> citation.
        IF NONE FOUND: Return "NONE".
        """
        try:
            resp = self.client.models.generate_content(model=self.model_name, contents=prompt)
            if "NONE" in resp.text: return None
            return resp.text.replace("```html", "").replace("```", "")
        except: return None

    # ==============================================================================
    # MASTER RUNNER
    # ==============================================================================
    def run_professional_validation(self, html_content, full_source_text, sources_list_metadata):
        logger.info("ğŸ›¡ï¸ STARTING PROFESSIONAL SELF-HEALING PROTOCOL...")
        
        # 1. Structure (Inject Missing Parts)
        html = self.ensure_structure_integrity(html_content, [], full_source_text)
        
        # 2. Facts (Correct Numbers) - AI Heavy
        html = self.verify_and_heal_facts(html, full_source_text)
        
        # 3. Quotes (Swap Fakes)
        html = self.verify_and_swap_quotes(html, full_source_text)
        
        # 4. Links (Recover Dead Ones)
        html = self.heal_broken_links(html, sources_list_metadata)
        
        logger.info("âœ… PROTOCOL COMPLETE. Content is clean.")
        return html
