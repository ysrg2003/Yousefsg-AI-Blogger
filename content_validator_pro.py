# FILE: content_validator_pro.py
# DESCRIPTION: Advanced content validation and healing using Gemini & BeautifulSoup.

import re
import requests
import logging
import json
from bs4 import BeautifulSoup
from google.genai import types
from tenacity import retry, stop_after_attempt, wait_fixed
from urllib.parse import urlparse

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù„ÙˆØ¬Ø±
logging.basicConfig(level=logging.INFO, format='%(asctime)s - [CORE-SURGEON-3.0] - %(message)s')
logger = logging.getLogger("CoreSurgeon")

class AdvancedContentValidator:
    def __init__(self, google_client, model_name="gemini-2.5-flash"):
        self.client = google_client
        self.model_name = model_name
        self.session = requests.Session()
        # Ù‡ÙˆÙŠØ© Ù…Ø®ØµØµØ© Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø­Ø¸Ø±
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36 ProValidator/3.0'
        })

    def _clean_json_text(self, text):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ù…Ù† Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ù…Ø§Ø±Ùƒ Ø¯Ø§ÙˆÙ† Ù„Ø¶Ù…Ø§Ù† ØªØ­ÙˆÙŠÙ„Ù‡ Ù„Ù€ JSON Ø¨Ù†Ø¬Ø§Ø­"""
        if not text: return "{}"
        clean = text.replace("```json", "").replace("```", "").strip()
        return clean

    # ==============================================================================
    # 1. PROACTIVE FACT SURGERY (Ø§Ù„Ø¬Ø±Ø§Ø­Ø© Ø§Ù„ÙˆÙ‚Ø§Ø¦ÙŠØ© Ù„Ù„Ø­Ù‚Ø§Ø¦Ù‚)
    # ==============================================================================
    def perform_fact_surgery(self, html_content, full_source_text):
        """
        ÙŠÙØ­Øµ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… ÙˆØ§Ù„Ø­Ù‚Ø§Ø¦Ù‚ Ø§Ù„ØªÙ‚Ù†ÙŠØ© ÙˆÙŠØµØ­Ø­Ù‡Ø§ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ØµØ§Ø¯Ø±.
        """
        soup = BeautifulSoup(html_content, 'html.parser')
        elements = soup.find_all(['p', 'td', 'li', 'span', 'h3'])
        
        chunks_to_verify = []
        # Regex Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø£Ø±Ù‚Ø§Ù… ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ø³Ø§Ø³Ø©
        pattern = r'(\d+%?|\$\d+|\bv\d+\.\d+|\d+\s(hours|GB|TB)|\b(vs|better than|faster than|release date)\b)'
        
        for el in elements:
            text = el.get_text()
            if len(text) < 300 and re.search(pattern, text, re.IGNORECASE): # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ÙÙ‚Ø±Ø§Øª Ø§Ù„Ø·ÙˆÙŠÙ„Ø© Ø¬Ø¯Ø§Ù‹ Ù„ØªÙˆÙÙŠØ± Ø§Ù„ØªÙˆÙƒÙ†Ø²
                chunks_to_verify.append(str(el))

        if not chunks_to_verify:
            return html_content

        # Ù†Ø±Ø³Ù„ Ø¯ÙØ¹Ø§Øª ØµØºÙŠØ±Ø© (50 Ø¹Ù†ØµØ± ÙƒØ­Ø¯ Ø£Ù‚ØµÙ‰) Ù„ØªØ¬Ù†Ø¨ ØªØ¬Ø§ÙˆØ² Ø­Ø¯ÙˆØ¯ Ø§Ù„ØªÙˆÙƒÙ†Ø²
        chunks_to_verify = chunks_to_verify[:50]

        logger.info(f"ğŸ’‰ Starting Fact Surgery on {len(chunks_to_verify)} sensitive elements...")
        
        prompt = f"""
        TASK: Technical Content Surgery.
        TRUTH DATA (Raw Sources): {full_source_text[:20000]}
        DRAFT HTML ELEMENTS: {json.dumps(chunks_to_verify)}
        
        INSTRUCTIONS:
        1. Compare numbers/facts in HTML elements with TRUTH DATA.
        2. IF WRONG: Rewrite the element with CORRECT info. Keep HTML tags.
        3. IF HALLUCINATED: Rewrite as a logical inference or delete the specific claim.
        4. IF CORRECT: Return null or skip.
        
        OUTPUT: JSON dictionary {{ "original_html_string": "corrected_html_string" }}
        """
        try:
            resp = self.client.models.generate_content(
                model=self.model_name, 
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_mime_type="application/json",
                    temperature=0.1 # Ø­Ø±Ø§Ø±Ø© Ù…Ù†Ø®ÙØ¶Ø© Ù„Ù„Ø¯Ù‚Ø©
                )
            )
            
            # Ø­Ù…Ø§ÙŠØ© Ø¶Ø¯ Ø£Ø®Ø·Ø§Ø¡ JSON
            json_text = self._clean_json_text(resp.text)
            corrections = json.loads(json_text)
            
            final_html = str(soup)
            for original, corrected in corrections.items():
                if original in final_html and corrected and corrected != original:
                    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØªØµØ­ÙŠØ­ Ù…Ù† Ø£ÙŠ ØªØ§Ø¬Ø§Øª Ù‡ÙŠÙƒÙ„ÙŠØ© Ø²Ø§Ø¦Ø¯Ø©
                    clean_fix = re.sub(r'</?(html|body|head)>', '', corrected, flags=re.IGNORECASE)
                    final_html = final_html.replace(original, clean_fix)
            
            return final_html
        except Exception as e:
            logger.error(f"âŒ Fact Surgery Failed: {e}")
            return html_content

    # ==============================================================================
    # 2. WIDGET RECONSTRUCTION (Ø¥Ø¹Ø§Ø¯Ø© Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„ØªØ§Ù„ÙØ©)
    # ==============================================================================
    def rebuild_damaged_widgets(self, html_content, full_source_text):
        if "comparison-table" not in html_content:
            return html_content

        soup = BeautifulSoup(html_content, 'html.parser')
        modified = False
        
        table = soup.find('table', class_='comparison-table')
        if table:
            cells = table.find_all('td')
            if cells:
                # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø¬Ø¯ÙˆÙ„ ÙØ§Ø±ØºØ§Ù‹ Ø£Ùˆ Ø³ÙŠØ¦Ø§Ù‹
                empty_cells = [c for c in cells if len(c.get_text(strip=True)) < 2 or "n/a" in c.get_text(strip=True).lower()]
                if len(empty_cells) > (len(cells) / 2):
                    logger.warning("ğŸ”¨ Comparison table is low quality. Rebuilding...")
                    new_table_html = self._generate_element_from_ai("Comparison Table", full_source_text)
                    if new_table_html:
                        # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø¢Ù…Ù† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… BeautifulSoup
                        new_soup = BeautifulSoup(new_table_html, 'html.parser')
                        if new_soup.find('table'):
                            table.replace_with(new_soup.find('table'))
                        else:
                            table.replace_with(new_soup)
                        modified = True

        return str(soup) if modified else html_content

    @retry(stop=stop_after_attempt(2), wait=wait_fixed(2))
    def _generate_element_from_ai(self, element_type, source_text):
        prompt = f"REBUILD TASK: Create a high-quality HTML {element_type} using ONLY facts from: {source_text[:8000]}. Use clean CSS classes like 'comparison-table'. Output ONLY HTML."
        try:
            resp = self.client.models.generate_content(model=self.model_name, contents=prompt)
            return resp.text.replace("```html", "").replace("```", "").strip()
        except: return None

    # ==============================================================================
    # 3. INTELLIGENT LINK RESTORATION (ØªØ±Ù…ÙŠÙ… Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…ÙƒØ³ÙˆØ±Ø©)
    # ==============================================================================
    def restore_link_integrity(self, html_content, sources_metadata):
        soup = BeautifulSoup(html_content, 'html.parser')
        links = soup.find_all('a', href=True)
        
        for link in links:
            url = link['href']
            # ØªØ®Ø·ÙŠ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ© ÙˆØ±ÙˆØ§Ø¨Ø· Ø§Ù„Ø³ÙˆØ´ÙŠØ§Ù„ Ù…ÙŠØ¯ÙŠØ§
            if any(x in url for x in ["latestai.me", "facebook.com", "instagram.com", "x.com", "youtube.com", "reddit.com"]) or url.startswith('#'):
                continue

            try:
                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø· (Head Request)
                r = self.session.head(url, timeout=3, allow_redirects=True)
                if r.status_code >= 400: raise Exception("Dead Link")
            except:
                logger.warning(f"ğŸ©¹ Healing broken link: {url}")
                parsed_url = urlparse(url)
                target_domain = parsed_url.netloc.replace('www.', '')
                
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¨Ø¯ÙŠÙ„ ÙÙŠ Ø§Ù„Ù…ØµØ§Ø¯Ø±
                replacement_url = None
                for src in sources_metadata:
                    if target_domain in src['url']:
                        replacement_url = src['url']
                        break
                
                if replacement_url:
                    link['href'] = replacement_url
                    logger.info(f"âœ… Link restored to source: {replacement_url}")
                else:
                    # Ø§Ù„Ø±Ø¨Ø· Ø¨Ø§Ù„Ù…ØµØ¯Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ÙƒØ­Ù„ Ø£Ø®ÙŠØ±
                    if sources_metadata:
                        link['href'] = sources_metadata[0]['url']
                        logger.info(f"âœ… Link pivoted to main source: {sources_metadata[0]['url']}")
        
        return str(soup)

    # ==============================================================================
    # 4. QUOTE VERIFIER (Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø§Ù‚ØªØ¨Ø§Ø³Ø§Øª)
    # ==============================================================================
    def verify_quotes(self, html_content, source_text):
        soup = BeautifulSoup(html_content, 'html.parser')
        quotes = soup.find_all('blockquote')
        
        for bq in quotes:
            quote_text = bq.get_text(strip=True)
            words = quote_text.split()
            # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø§Ù‚ØªØ¨Ø§Ø³ ÙÙŠ Ø§Ù„Ù…ØµØ¯Ø±
            if len(words) > 3 and " ".join(words[:4]).lower() not in source_text.lower():
                logger.warning("âš ï¸ Replacing hallucinated quote...")
                real_quote_html = self._find_real_quote_from_ai(source_text)
                if real_quote_html:
                    new_soup = BeautifulSoup(real_quote_html, 'html.parser')
                    # Ù†Ø­Ø§ÙˆÙ„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ blockquote ÙÙ‚Ø· Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ØªØ§Ø¬Ø§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
                    if new_soup.find('blockquote'):
                        bq.replace_with(new_soup.find('blockquote'))
                    else:
                        bq.replace_with(new_soup)
                else:
                    bq.decompose() # Ø­Ø°Ù Ø§Ù„Ø§Ù‚ØªØ¨Ø§Ø³ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙˆØ¬Ø¯ Ø¨Ø¯ÙŠÙ„
        
        return str(soup)

    def _find_real_quote_from_ai(self, source_text):
        prompt = f"EXTRACT VERBATIM QUOTE: Find one powerful, real sentence from this text: {source_text[:5000]}. Return it as a single HTML <blockquote> with a <footer> if possible. Output ONLY HTML."
        try:
            resp = self.client.models.generate_content(model=self.model_name, contents=prompt)
            return resp.text.replace("```html", "").replace("```", "").strip()
        except: return None

    # ==============================================================================
    # MASTER RUNNER
    # ==============================================================================
    def run_professional_validation(self, html_content, full_source_text, sources_metadata):
        logger.info("ğŸ›¡ï¸ CORE SURGEON 3.0: COMMENCING FULL RESTORATION...")
        
        # 1. Facts
        html = self.perform_fact_surgery(html_content, full_source_text)
        
        # 2. Widgets
        html = self.rebuild_damaged_widgets(html, full_source_text)
        
        # 3. Quotes
        html = self.verify_quotes(html, full_source_text)
        
        # 4. Links
        html = self.restore_link_integrity(html, sources_metadata)
        
        # Final Cleanup
        html = re.sub(r'</?(html|body|head|meta|title)>', '', html, flags=re.IGNORECASE)
        html = re.sub(r'>\s+<', '><', html).strip()
        
        logger.info("âœ… RESTORATION COMPLETE.")
        return html
