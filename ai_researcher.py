# FILE: ai_researcher.py
# ROLE: An elite autonomous agent that uses Google Grounding to find verified, high-quality sources.
# FEATURES: Strict quality filtering, JSON structured output, real-time verification.

import json
import re
import time
from google import genai
from google.genai import types
from config import log
from api_manager import key_manager

# Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø³ÙˆØ¯Ø§Ø¡ Ø§Ù„ØµØ§Ø±Ù…Ø© Ù„Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„ØªÙŠ Ù„Ø§ Ù†Ø±ÙŠØ¯Ù‡Ø§ ÙƒÙ…Ø±Ø§Ø¬Ø¹ ØªÙ‚Ù†ÙŠØ©
LOW_QUALITY_DOMAINS = [
    "reddit.com", "quora.com", "pinterest.com", "linkedin.com", "medium.com", 
    "facebook.com", "instagram.com", "tiktok.com", "vocal.media", "newsbreak.com",
    "msn.com", "aol.com", "yahoo.com"
]

def extract_urls_fallback(text):
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Regex ÙÙŠ Ø­Ø§Ù„ ÙØ´Ù„ ØªØ­Ù„ÙŠÙ„ JSON.
    """
    url_pattern = re.compile(r'https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+')
    found = url_pattern.findall(text)
    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ø£ÙŠ Ø¨Ù‚Ø§ÙŠØ§ Ø¬ÙˆØ¬Ù„
    clean_links = []
    for link in found:
        if "google.com" not in link and not any(bad in link for bad in LOW_QUALITY_DOMAINS):
            clean_links.append(link)
    return list(set(clean_links)) # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±

def smart_hunt(topic, config):
    """
    Ø§Ù„Ù…Ù‡Ù…Ø©: Ø§Ù„Ø°Ù‡Ø§Ø¨ Ø¥Ù„Ù‰ Ø¬ÙˆØ¬Ù„ØŒ Ø§Ù„Ø¨Ø­Ø«ØŒ Ø§Ù„ÙÙ„ØªØ±Ø©ØŒ ÙˆØ¥Ø¹Ø§Ø¯Ø© Ø£ÙØ¶Ù„ 3-5 Ù…ØµØ§Ø¯Ø± Ù…ÙˆØ«ÙˆÙ‚Ø©.
    """
    # Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ø°ÙŠ ÙŠØ¯Ø¹Ù… Grounding (Flash 2.0 Ù…Ù…ØªØ§Ø² ÙÙŠ Ø§Ù„Ø³Ø±Ø¹Ø© ÙˆØ§Ù„Ø¯Ù‚Ø©)
    # Ù…Ù„Ø§Ø­Ø¸Ø©: ÙŠÙ…ÙƒÙ†Ùƒ ØªØºÙŠÙŠØ±Ù‡ Ø­Ø³Ø¨ Ø§Ù„Ù…ØªØ§Ø­ ÙÙŠ Ù…ÙØªØ§Ø­ÙƒØŒ Ù„ÙƒÙ† 2.0 Ù‡Ùˆ Ø§Ù„Ø£ÙØ¶Ù„ Ù„Ù„Ø¨Ø­Ø« Ø­Ø§Ù„ÙŠØ§Ù‹
    model_name = "gemini-2.0-flash-exp" 
    
    log(f"   ğŸ•µï¸â€â™‚ï¸ [AI Researcher] Conducting deep web search for: '{topic}'...")
    
    key = key_manager.get_current_key()
    if not key:
        log("      âŒ API Key Error.")
        return []

    client = genai.Client(api_key=key)
    
    # 1. Ø¥Ø¹Ø¯Ø§Ø¯ Ø£Ø¯Ø§Ø© Ø§Ù„Ø¨Ø­Ø« (Google Search Tool)
    google_search_tool = types.Tool(
        google_search_retrieval=types.GoogleSearchRetrieval(
            dynamic_retrieval_config=types.DynamicRetrievalConfig(
                mode=types.DynamicRetrievalConfigMode.MODE_DYNAMIC,
                dynamic_threshold=0.3
            )
        )
    )

    # 2. Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª "Ø§Ù„ØµØ§Ø±Ù…" (The Strict Prompt)
    system_instruction = """
    You are an Elite Technical Researcher for a high-authority tech publication.
    Your Job: verify facts and find the PRIMARY sources for a specific tech topic.
    
    STRICT FILTERING RULES (DO NOT IGNORE):
    1. PRIORITIZE: Official Documentation, GitHub Repositories, Major Tech Publications (The Verge, TechCrunch, Arstechnica, Wired), and University Papers (.edu).
    2. BAN: User-Generated Content (Reddit, Quora, LinkedIn), Social Media, Generic News Aggregators (MSN, Yahoo), and Content Farms.
    3. FRESHNESS: Sources must be RECENT (last 30 days) unless the topic is a fundamental tutorial.
    4. ACCURACY: Return the DIRECT article URL, not a home page.
    """

    user_prompt = f"""
    TOPIC: "{topic}"
    
    MISSION: 
    Search Google, analyze the results, and select the TOP 3-5 absolute best, most authoritative articles covering this exact topic.
    
    OUTPUT FORMAT (Return RAW JSON only):
    [
        {{
            "title": "Actual Page Title",
            "link": "https://exact-url.com/article",
            "snippet": "Brief summary of technical value",
            "date": "Published Date or 'Recent'"
        }}
    ]
    """

    try:
        response = client.models.generate_content(
            model=model_name,
            contents=user_prompt,
            config=types.GenerateContentConfig(
                tools=[google_search_tool],
                system_instruction=system_instruction,
                response_mime_type="application/json",
                temperature=0.3 # Ø­Ø±Ø§Ø±Ø© Ù…Ù†Ø®ÙØ¶Ø© Ù„Ù„Ø¯Ù‚Ø©
            )
        )
        
        # 3. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø¯ ÙˆØªÙ†Ø¸ÙŠÙÙ‡
        raw_text = response.text.replace("```json", "").replace("```", "").strip()
        
        sources = []
        try:
            parsed_data = json.loads(raw_text)
            
            # 4. Ø§Ù„ØªØ¯Ù‚ÙŠÙ‚ Ø§Ù„Ø«Ø§Ù†ÙˆÙŠ (Double-Check) ÙÙŠ Ø§Ù„ÙƒÙˆØ¯
            for item in parsed_data:
                url = item.get('link') or item.get('url') # Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ø®ØªÙ„Ø§Ù Ø§Ù„ØªØ³Ù…ÙŠØ© Ø§Ù„Ù…Ø­ØªÙ…Ù„
                title = item.get('title', 'Source')
                
                if not url: continue
                
                # ØªØµÙÙŠØ© Ø§Ù„Ù†Ø·Ø§Ù‚Ø§Øª Ø§Ù„Ø³ÙŠØ¦Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ù„Ù„ØªØ£ÙƒÙŠØ¯
                domain = url.split("//")[-1].split("/")[0].lower()
                if any(bad in domain for bad in LOW_QUALITY_DOMAINS):
                    continue
                
                # ØªØ¬Ø§Ù‡Ù„ Ø±ÙˆØ§Ø¨Ø· Ø¬ÙˆØ¬Ù„ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©
                if "google.com" in domain:
                    continue

                sources.append({
                    "title": title,
                    "link": url,
                    "date": item.get('date', 'Today'),
                    "snippet": item.get('snippet', '')
                })

        except json.JSONDecodeError:
            log("      âš ï¸ JSON Parsing failed. Attempting regex extraction...")
            # Ø®Ø·Ø© Ø¨Ø¯ÙŠÙ„Ø©: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø¨Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù†ØµÙŠ
            found_links = extract_urls_fallback(raw_text)
            for link in found_links:
                sources.append({"title": "AI Discovered Source", "link": link, "date": "Recent"})

        if sources:
            log(f"      âœ… [AI Researcher] Identified {len(sources)} high-quality targets.")
            return sources[:5] # Ù†ÙƒØªÙÙŠ Ø¨Ø£ÙØ¶Ù„ 5
        else:
            log("      âš ï¸ AI Researcher searched but found no quality sources matching criteria.")
            return []

    except Exception as e:
        log(f"      âŒ AI Researcher Critical Error: {e}")
        return []
