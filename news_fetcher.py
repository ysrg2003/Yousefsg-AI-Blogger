import requests
import urllib.parse
import feedparser
import random
import datetime
import os
import json
from config import log
from api_manager import generate_step_strict

# ==============================================================================
# 1. SMART REPUTATION SYSTEM (MEMORY)
# ==============================================================================

REPUTATION_FILE = "source_reputation.json"

# Ù‚Ø§Ø¦Ù…Ø© Ø·ÙˆØ§Ø±Ø¦ Ù„Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø³ÙŠØ¦Ø© Ø¬Ø¯Ø§Ù‹ (Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ù€ AI Ù…Ø§Ø°Ø§ ÙŠÙƒØ±Ù‡)
SEED_BLACKLIST = [
    "vocal.media", "aol.com", "msn.com", "yahoo.com", "marketwatch.com", 
    "indiacsr.in", "officechai.com", "analyticsinsight.net", "prweb.com",
    "businesswire.com", "globenewswire.com", "medium.com", "linkedin.com",
    "quora.com", "reddit.com", "youtube.com" # Ù†Ø³ØªØ¨Ø¹Ø¯ ÙŠÙˆØªÙŠÙˆØ¨ Ù…Ù† Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù†ØµÙŠØ©
]

def get_domain_reputation():
    """ØªØ­Ù…ÙŠÙ„ Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ (Ø§Ù„Ø¬ÙŠØ¯Ø© ÙˆØ§Ù„Ø³ÙŠØ¦Ø©)."""
    default_rep = {"blacklist": SEED_BLACKLIST, "whitelist": []}
    if os.path.exists(REPUTATION_FILE):
        try:
            with open(REPUTATION_FILE, 'r') as f:
                data = json.load(f)
                # Ø¯Ù…Ø¬ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ© Ù…Ø¹ Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø© Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø­Ù…Ø§ÙŠØ©
                data['blacklist'] = list(set(data.get('blacklist', []) + SEED_BLACKLIST))
                return data
        except: return default_rep
    return default_rep

def save_domain_reputation(data):
    """Ø­ÙØ¸ ØªØ­Ø¯ÙŠØ«Ø§Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø©."""
    try:
        with open(REPUTATION_FILE, 'w') as f: json.dump(data, f, indent=2)
    except: pass

def ai_vet_sources(items, model_name):
    """
    ÙŠÙ‚ÙˆÙ… Ø§Ù„Ù€ AI Ø¨ÙØ­Øµ Ø§Ù„Ø¯ÙˆÙ…ÙŠÙ†Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© ÙˆØªØµÙ†ÙŠÙÙ‡Ø§ (ØµØ­Ø§ÙØ© Ø­Ù‚ÙŠÙ‚ÙŠØ© vs Ø­Ø´Ùˆ).
    """
    reputation = get_domain_reputation()
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¯ÙˆÙ…ÙŠÙ† Ù…Ù† ÙƒÙ„ Ø±Ø§Ø¨Ø·
    item_domains = {}
    for item in items:
        try:
            domain = urllib.parse.urlparse(item['link']).netloc.replace('www.', '').lower()
            if domain not in item_domains: item_domains[domain] = []
            item_domains[domain].append(item)
        except: continue

    unique_domains = list(item_domains.keys())
    
    # ØªØµÙÙŠØ© Ù…Ø§ Ù‡Ùˆ Ù…Ø¹Ø±ÙˆÙ Ù…Ø³Ø¨Ù‚Ø§Ù‹
    unknown_domains = [d for d in unique_domains if d not in reputation['blacklist'] and d not in reputation['whitelist']]
    
    # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù‡Ù†Ø§Ùƒ Ø¯ÙˆÙ…ÙŠÙ†Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©ØŒ Ù†Ø³Ø£Ù„ Ø§Ù„Ù€ AI
    if unknown_domains:
        log(f"   ğŸ•µï¸â€â™‚ï¸ AI Auditor: Vetting {len(unknown_domains)} new domains...")
        
        prompt = f"""
        ROLE: Senior Tech Editor & Media Auditor.
        TASK: Evaluate these websites for credibility in covering AI & Tech news.
        
        CANDIDATE DOMAINS: {unknown_domains}
        
        CRITERIA FOR BLACKLIST (Reject):
        - User-Generated Content / Open Publishing (e.g., Vocal, Medium, LinkedIn).
        - Press Release Aggregators (PRWeb, BusinessWire).
        - General News Aggregators with low original tech reporting (MSN, AOL, Yahoo).
        - SEO Farms or Low-Quality Blogs.
        
        CRITERIA FOR WHITELIST (Accept):
        - Dedicated Tech Publications (The Verge, TechCrunch, Wired, Ars Technica).
        - Official Company Blogs (OpenAI, Google Blog, Microsoft).
        - Reputable News Outlets with Tech Desks (Reuters, Bloomberg, NYT).
        - Niche High-Quality AI Blogs.

        OUTPUT JSON ONLY:
        {{
          "blacklist": ["bad-site1.com", "spam-site2.com"],
          "whitelist": ["good-site1.com"]
        }}
        """
        try:
            decision = generate_step_strict(model_name, prompt, "Source Vetting")
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù‚ÙˆØ§Ø¦Ù…
            new_black = decision.get('blacklist', [])
            new_white = decision.get('whitelist', [])
            
            if new_black: log(f"      â›” AI Blocked: {new_black}")
            if new_white: log(f"      âœ… AI Approved: {new_white}")

            reputation['blacklist'].extend(new_black)
            reputation['whitelist'].extend(new_white)
            
            # Ø­ÙØ¸ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
            reputation['blacklist'] = list(set(reputation['blacklist']))
            reputation['whitelist'] = list(set(reputation['whitelist']))
            save_domain_reputation(reputation)
            
        except Exception as e:
            log(f"      âš ï¸ Vetting skipped (Error). Assuming safe for now.")

    # Ø§Ù„ØªØµÙÙŠØ© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
    approved_items = []
    reputation = get_domain_reputation() # Ø¥Ø¹Ø§Ø¯Ø© ØªØ­Ù…ÙŠÙ„ Ù„Ù„ØªØ£ÙƒØ¯
    
    for domain, domain_items in item_domains.items():
        if domain in reputation['blacklist']:
            continue # ØªØ®Ø·ÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø¯ÙˆÙ…ÙŠÙ†
        approved_items.extend(domain_items)
        
    return approved_items

# ==============================================================================
# 2. STANDARD FETCHERS (UPDATED TO USE NEGATIVE SEARCH)
# ==============================================================================

def get_gnews_api_sources(query, category):
    api_key = os.getenv('GNEWS_API_KEY')
    if not api_key: return []
    
    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
    clean_query = query.replace(" when:2d", "").replace(" when:1d", "")
    
    # Ø¥Ø¶Ø§ÙØ© ÙÙ„ØªØ± Ø³Ù„Ø¨ÙŠ Ù„Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø³ÙŠØ¦Ø© Ø§Ù„Ù…Ø¹Ø±ÙˆÙØ© Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø¶ÙˆØ¶Ø§Ø¡ Ù‚Ø¨Ù„ ÙˆØµÙˆÙ„Ù‡Ø§ Ù„Ù„Ù€ AI
    # Ù‡Ø°Ø§ ÙŠÙˆÙØ± Ø§Ù„Ù€ Quota
    hard_filters = " ".join([f"-site:{site}" for site in SEED_BLACKLIST[:5]]) # Ù†Ø³ØªØ®Ø¯Ù… Ø£Ù‡Ù… 5 ÙÙ‚Ø· Ù‡Ù†Ø§ Ù„Ø·ÙˆÙ„ Ø§Ù„Ø±Ø§Ø¨Ø·
    final_query = f"{clean_query} {hard_filters}"

    log(f"   ğŸ“¡ Querying GNews API for: '{clean_query}'...")
    url = f"https://gnews.io/api/v4/search?q={urllib.parse.quote(final_query)}&lang=en&country=us&max=5&apikey={api_key}"
    try:
        r = requests.get(url, timeout=10)
        data = r.json()
        if r.status_code != 200 or 'articles' not in data: return []
        formatted = []
        for art in data.get('articles', []):
            formatted.append({
                "title": art.get('title'),
                "link": art.get('url'),
                "date": art.get('publishedAt', str(datetime.date.today())),
                "image": art.get('image')
            })
        return formatted
    except: return []

def get_real_news_rss(query_keywords, category=None):
    try:
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù…Ù† Ø§Ù„ØªØ¹Ù‚ÙŠØ¯Ø§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø© Ù„Ø²ÙŠØ§Ø¯Ø© ÙØ±Øµ Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬
        base_query = query_keywords.replace('"', '').strip()
        
        # Ø¥Ø²Ø§Ù„Ø© when:2d Ø¥Ø°Ø§ ÙƒØ§Ù†Øª ØªØ³Ø¨Ø¨ Ù…Ø´Ø§ÙƒÙ„ØŒ Ø£Ùˆ ØªØ±ÙƒÙ‡Ø§ Ø¥Ø°Ø§ ÙƒÙ†Øª Ù…ØµØ±Ø§Ù‹ Ø¹Ù„ÙŠÙ‡Ø§
        # Ø³Ù†Ù‚ÙˆÙ… Ø¨ØªØ±Ù…ÙŠØ²Ù‡Ø§ Ø¨Ø´ÙƒÙ„ Ø¢Ù…Ù†
        if "when:" not in base_query:
            full_query = f"{base_query} when:7d" # ÙˆØ³Ø¹Ù†Ø§ Ø§Ù„Ù†Ø·Ø§Ù‚ Ù„Ù€ 7 Ø£ÙŠØ§Ù… Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        else:
            full_query = base_query

        log(f"   ğŸ“° Querying Google News RSS for: '{full_query}'...")
        encoded = urllib.parse.quote(full_query)
        url = f"https://news.google.com/rss/search?q={encoded}&hl=en-US&gl=US&ceid=US:en"
        
        feed = feedparser.parse(url)
        items = []
        
        if feed.entries:
            for entry in feed.entries[:10]:
                pub = entry.published if 'published' in entry else "Today"
                title_clean = entry.title.split(' - ')[0]
                items.append({"title": title_clean, "link": entry.link, "date": pub})
            return items 
        
        # --- Ø§Ù„ØªØºÙŠÙŠØ± Ø§Ù„Ø¬Ø°Ø±ÙŠ Ù‡Ù†Ø§ ---
        # Ø£Ù„ØºÙŠÙ†Ø§ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¹Ø§Ù… Ø¹Ù† Ø§Ù„Ù‚Ø³Ù… (Category Fallback)
        # Ù„ÙƒÙŠ Ù†Ø³Ù…Ø­ Ù„Ù€ GNews API Ø¨Ø§Ù„Ø¹Ù…Ù„ ÙÙŠ main.py
            log(f"   âš ï¸ RSS Empty for '{base_query}'. Returning empty list to trigger GNews.")
            return [] 
            
            except Exception as e:
                log(f"âŒ RSS Error: {e}")
                return []
        
        elif category:
            log(f"   âš ï¸ RSS Empty. Fallback to Category: {category}")
            fb = f"{category} news when:1d"
            url = f"https://news.google.com/rss/search?q={urllib.parse.quote(fb)}&hl=en-US&gl=US&ceid=US:en"
            feed = feedparser.parse(url)
            for entry in feed.entries[:5]:
                items.append({"title": entry.title, "link": entry.link, "date": "Today"})
            return items
            
        return []
            
    except Exception as e:
        log(f"âŒ RSS Error: {e}")
        return []
