{
  "categories": {
    "Machine Learning": {
      "trending_prompt": "You are an investigative tech reporter specialized in {section}. Search the modern index (Google Search, Google Scholar, arXiv, official blogs, SEC/10-Q when financial figures are used) for one *specific*, high-impact case, study, deployment, or company announcement that occurred within {date_range} (for example \"last 60 days\").\n\nSECTION FOCUS (choose the relevant focus for {section}; these must guide your search terms):\n- Machine Learning: SOTA comparisons, datasets, compute, energy cost, reproducibility.\n- Natural Language Processing: eval metrics (BLEU/ROUGE/EM/Hallucination risk), dataset biases, safety.\n- Computer Vision: datasets (ImageNet/COCO/medical), segmentation/tracking benchmarks, edge vs cloud deployment.\n- Robotics: sim2real, ISO safety standards, real deployments.\n- Generative AI: model provenance, licensing, watermarking, detectability.\n- AI Applications: enterprise ROI, integration case studies, deployment cost.\n- AI Research: arXiv/NeurIPS/CVPR papers, ablations, reproducibility, code availability.\n- AI Ethics: audits, impact assessments, governance, legal cases.\n\nMANDATORY SOURCE & VERIFICATION RULES (follow EXACTLY):\n1. Return exactly one headline (plain English, single line) — a journalist-style headline that connects technical advance to human/commercial impact.\n2. Provide 2–3 primary sources that verify the story. At least:\n   - One PRIMARY TECH SOURCE: (peer-reviewed DOI or arXiv paper with version/date OR official GitHub repo with commit/PR date OR official company technical blog post).\n   - One SECONDARY CORROBORATING SOURCE: (reputable news outlet, conference proceeding, or company press release).\n3. For each source, include:\n   - title, exact URL, publication date (YYYY-MM-DD), type (paper/blog/repo/press/SEC), and a one-line note: \"why it verifies\".\n4. For any numeric claim you will later assert (e.g., \"88% of queries\"), ensure the source actually contains that number. If the source gives a figure differently, report the exact figure and location of the figure in the source (e.g., \"see Figure 2, page 6\" or \"arXiv v2, paragraph 3\").\n5. If the story touches YMYL topics (health/finance/legal), include an immediate risk note listing which regulations may apply (e.g., HIPAA, FDA, CE, GDPR, SEC) and what kind of verification is required.\n6. Check basic credibility signals for each source and report them: (a) publisher credibility (Nature/IEEE/ACM/official corp blog), (b) if arXiv — indicate whether paper has code/benchmarks, (c) if GitHub — include last commit date and license, (d) if press release — confirm corporate domain and date/time.\n7. Output JSON only, EXACTLY in this format:\n\n{\n \"headline\": \"One-line headline\",\n \"sources\": [\n   {\n     \"title\":\"Exact source title\",\n     \"url\":\"https://...\",\n     \"date\":\"YYYY-MM-DD\",\n     \"type\":\"paper|arXiv|repo|blog|press|SEC\",\n     \"why\":\"One-line why this verifies (include exact page/figure if relevant)\",\n     \"credibility\":\"short note: Nature/IEEE/company blog/press/etc\",\n     \"notes\":\"any caveats about the source\"\n   },\n   ...\n ],\n \"riskNote\":\"If YMYL risk exists list regulators and verification steps; otherwise empty string\"\n}",
      "evergreen_prompt": "Write a comprehensive, evergreen article about Machine Learning. Explain core concepts (supervised vs. unsupervised learning, neural networks, common architectures), key datasets, reproducibility best practices, compute and energy considerations, and practical applications. Tailor the piece to be useful to both beginners and experienced practitioners; include recommended further reading and resource links."
    },
    "Natural Language Processing": {
      "trending_prompt": "You are an investigative tech reporter specialized in {section}. Search the modern index (Google Search, Google Scholar, arXiv, official blogs, SEC/10-Q when financial figures are used) for one *specific*, high-impact case, study, deployment, or company announcement that occurred within {date_range} (for example \"last 60 days\").\n\nSECTION FOCUS (choose the relevant focus for {section}; these must guide your search terms):\n- Machine Learning: SOTA comparisons, datasets, compute, energy cost, reproducibility.\n- Natural Language Processing: eval metrics (BLEU/ROUGE/EM/Hallucination risk), dataset biases, safety.\n- Computer Vision: datasets (ImageNet/COCO/medical), segmentation/tracking benchmarks, edge vs cloud deployment.\n- Robotics: sim2real, ISO safety standards, real deployments.\n- Generative AI: model provenance, licensing, watermarking, detectability.\n- AI Applications: enterprise ROI, integration case studies, deployment cost.\n- AI Research: arXiv/NeurIPS/CVPR papers, ablations, reproducibility, code availability.\n- AI Ethics: audits, impact assessments, governance, legal cases.\n\nMANDATORY SOURCE & VERIFICATION RULES (follow EXACTLY):\n1. Return exactly one headline (plain English, single line) — a journalist-style headline that connects technical advance to human/commercial impact.\n2. Provide 2–3 primary sources that verify the story. At least:\n   - One PRIMARY TECH SOURCE: (peer-reviewed DOI or arXiv paper with version/date OR official GitHub repo with commit/PR date OR official company technical blog post).\n   - One SECONDARY CORROBORATING SOURCE: (reputable news outlet, conference proceeding, or company press release).\n3. For each source, include:\n   - title, exact URL, publication date (YYYY-MM-DD), type (paper/blog/repo/press/SEC), and a one-line note: \"why it verifies\".\n4. For any numeric claim you will later assert (e.g., \"88% of queries\"), ensure the source actually contains that number. If the source gives a figure differently, report the exact figure and location of the figure in the source (e.g., \"see Figure 2, page 6\" or \"arXiv v2, paragraph 3\").\n5. If the story touches YMYL topics (health/finance/legal), include an immediate risk note listing which regulations may apply (e.g., HIPAA, FDA, CE, GDPR, SEC) and what kind of verification is required.\n6. Check basic credibility signals for each source and report them: (a) publisher credibility (Nature/IEEE/ACM/official corp blog), (b) if arXiv — indicate whether paper has code/benchmarks, (c) if GitHub — include last commit date and license, (d) if press release — confirm corporate domain and date/time.\n7. Output JSON only, EXACTLY in this format:\n\n{\n \"headline\": \"One-line headline\",\n \"sources\": [\n   {\n     \"title\":\"Exact source title\",\n     \"url\":\"https://...\",\n     \"date\":\"YYYY-MM-DD\",\n     \"type\":\"paper|arXiv|repo|blog|press|SEC\",\n     \"why\":\"One-line why this verifies (include exact page/figure if relevant)\",\n     \"credibility\":\"short note: Nature/IEEE/company blog/press/etc\",\n     \"notes\":\"any caveats about the source\"\n   },\n   ...\n ],\n \"riskNote\":\"If YMYL risk exists list regulators and verification steps; otherwise empty string\"\n}",
      "evergreen_prompt": "Write a clear, practical guide that explains how Natural Language Processing works. Cover tokenization, word embeddings, transformers, common evaluation metrics (BLEU, ROUGE, EM), and provide real-world examples (translation, summarization, QA). Include commentary on dataset bias and safety considerations."
    },
    "Computer Vision": {
      "trending_prompt": "You are an investigative tech reporter specialized in {section}. Search the modern index (Google Search, Google Scholar, arXiv, official blogs, SEC/10-Q when financial figures are used) for one *specific*, high-impact case, study, deployment, or company announcement that occurred within {date_range} (for example \"last 60 days\").\n\nSECTION FOCUS (choose the relevant focus for {section}; these must guide your search terms):\n- Machine Learning: SOTA comparisons, datasets, compute, energy cost, reproducibility.\n- Natural Language Processing: eval metrics (BLEU/ROUGE/EM/Hallucination risk), dataset biases, safety.\n- Computer Vision: datasets (ImageNet/COCO/medical), segmentation/tracking benchmarks, edge vs cloud deployment.\n- Robotics: sim2real, ISO safety standards, real deployments.\n- Generative AI: model provenance, licensing, watermarking, detectability.\n- AI Applications: enterprise ROI, integration case studies, deployment cost.\n- AI Research: arXiv/NeurIPS/CVPR papers, ablations, reproducibility, code availability.\n- AI Ethics: audits, impact assessments, governance, legal cases.\n\nMANDATORY SOURCE & VERIFICATION RULES (follow EXACTLY):\n1. Return exactly one headline (plain English, single line) — a journalist-style headline that connects technical advance to human/commercial impact.\n2. Provide 2–3 primary sources that verify the story. At least:\n   - One PRIMARY TECH SOURCE: (peer-reviewed DOI or arXiv paper with version/date OR official GitHub repo with commit/PR date OR official company technical blog post).\n   - One SECONDARY CORROBORATING SOURCE: (reputable news outlet, conference proceeding, or company press release).\n3. For each source, include:\n   - title, exact URL, publication date (YYYY-MM-DD), type (paper/blog/repo/press/SEC), and a one-line note: \"why it verifies\".\n4. For any numeric claim you will later assert (e.g., \"88% of queries\"), ensure the source actually contains that number. If the source gives a figure differently, report the exact figure and location of the figure in the source (e.g., \"see Figure 2, page 6\" or \"arXiv v2, paragraph 3\").\n5. If the story touches YMYL topics (health/finance/legal), include an immediate risk note listing which regulations may apply (e.g., HIPAA, FDA, CE, GDPR, SEC) and what kind of verification is required.\n6. Check basic credibility signals for each source and report them: (a) publisher credibility (Nature/IEEE/ACM/official corp blog), (b) if arXiv — indicate whether paper has code/benchmarks, (c) if GitHub — include last commit date and license, (d) if press release — confirm corporate domain and date/time.\n7. Output JSON only, EXACTLY in this format:\n\n{\n \"headline\": \"One-line headline\",\n \"sources\": [\n   {\n     \"title\":\"Exact source title\",\n     \"url\":\"https://...\",\n     \"date\":\"YYYY-MM-DD\",\n     \"type\":\"paper|arXiv|repo|blog|press|SEC\",\n     \"why\":\"One-line why this verifies (include exact page/figure if relevant)\",\n     \"credibility\":\"short note: Nature/IEEE/company blog/press/etc\",\n     \"notes\":\"any caveats about the source\"\n   },\n   ...\n ],\n \"riskNote\":\"If YMYL risk exists list regulators and verification steps; otherwise empty string\"\n}",
      "evergreen_prompt": "Write an accessible overview of Computer Vision fundamentals. Cover image formation, common datasets (ImageNet, COCO, medical datasets), CNNs and modern transformer-based vision models, and practical considerations for edge vs cloud deployment. Include real-world examples and evaluation benchmarks."
    },
    "Robotics": {
      "trending_prompt": "You are an investigative tech reporter specialized in {section}. Search the modern index (Google Search, Google Scholar, arXiv, official blogs, SEC/10-Q when financial figures are used) for one *specific*, high-impact case, study, deployment, or company announcement that occurred within {date_range} (for example \"last 60 days\").\n\nSECTION FOCUS (choose the relevant focus for {section}; these must guide your search terms):\n- Machine Learning: SOTA comparisons, datasets, compute, energy cost, reproducibility.\n- Natural Language Processing: eval metrics (BLEU/ROUGE/EM/Hallucination risk), dataset biases, safety.\n- Computer Vision: datasets (ImageNet/COCO/medical), segmentation/tracking benchmarks, edge vs cloud deployment.\n- Robotics: sim2real, ISO safety standards, real deployments.\n- Generative AI: model provenance, licensing, watermarking, detectability.\n- AI Applications: enterprise ROI, integration case studies, deployment cost.\n- AI Research: arXiv/NeurIPS/CVPR papers, ablations, reproducibility, code availability.\n- AI Ethics: audits, impact assessments, governance, legal cases.\n\nMANDATORY SOURCE & VERIFICATION RULES (follow EXACTLY):\n1. Return exactly one headline (plain English, single line) — a journalist-style headline that connects technical advance to human/commercial impact.\n2. Provide 2–3 primary sources that verify the story. At least:\n   - One PRIMARY TECH SOURCE: (peer-reviewed DOI or arXiv paper with version/date OR official GitHub repo with commit/PR date OR official company technical blog post).\n   - One SECONDARY CORROBORATING SOURCE: (reputable news outlet, conference proceeding, or company press release).\n3. For each source, include:\n   - title, exact URL, publication date (YYYY-MM-DD), type (paper/blog/repo/press/SEC), and a one-line note: \"why it verifies\".\n4. For any numeric claim you will later assert (e.g., \"88% of queries\"), ensure the source actually contains that number. If the source gives a figure differently, report the exact figure and location of the figure in the source (e.g., \"see Figure 2, page 6\" or \"arXiv v2, paragraph 3\").\n5. If the story touches YMYL topics (health/finance/legal), include an immediate risk note listing which regulations may apply (e.g., HIPAA, FDA, CE, GDPR, SEC) and what kind of verification is required.\n6. Check basic credibility signals for each source and report them: (a) publisher credibility (Nature/IEEE/ACM/official corp blog), (b) if arXiv — indicate whether paper has code/benchmarks, (c) if GitHub — include last commit date and license, (d) if press release — confirm corporate domain and date/time.\n7. Output JSON only, EXACTLY in this format:\n\n{\n \"headline\": \"One-line headline\",\n \"sources\": [\n   {\n     \"title\":\"Exact source title\",\n     \"url\":\"https://...\",\n     \"date\":\"YYYY-MM-DD\",\n     \"type\":\"paper|arXiv|repo|blog|press|SEC\",\n     \"why\":\"One-line why this verifies (include exact page/figure if relevant)\",\n     \"credibility\":\"short note: Nature/IEEE/company blog/press/etc\",\n     \"notes\":\"any caveats about the source\"\n   },\n   ...\n ],\n \"riskNote\":\"If YMYL risk exists list regulators and verification steps; otherwise empty string\"\n}",
      "evergreen_prompt": "Write a clear primer on Robotics fundamentals. Cover robot types (industrial, service, autonomous), sim-to-real transfer, key safety standards (ISO), and examples of real-world deployments. Discuss control stacks and common sensors/actuators."
    },
    "Generative AI": {
      "trending_prompt": "You are an investigative tech reporter specialized in {section}. Search the modern index (Google Search, Google Scholar, arXiv, official blogs, SEC/10-Q when financial figures are used) for one *specific*, high-impact case, study, deployment, or company announcement that occurred within {date_range} (for example \"last 60 days\").\n\nSECTION FOCUS (choose the relevant focus for {section}; these must guide your search terms):\n- Machine Learning: SOTA comparisons, datasets, compute, energy cost, reproducibility.\n- Natural Language Processing: eval metrics (BLEU/ROUGE/EM/Hallucination risk), dataset biases, safety.\n- Computer Vision: datasets (ImageNet/COCO/medical), segmentation/tracking benchmarks, edge vs cloud deployment.\n- Robotics: sim2real, ISO safety standards, real deployments.\n- Generative AI: model provenance, licensing, watermarking, detectability.\n- AI Applications: enterprise ROI, integration case studies, deployment cost.\n- AI Research: arXiv/NeurIPS/CVPR papers, ablations, reproducibility, code availability.\n- AI Ethics: audits, impact assessments, governance, legal cases.\n\nMANDATORY SOURCE & VERIFICATION RULES (follow EXACTLY):\n1. Return exactly one headline (plain English, single line) — a journalist-style headline that connects technical advance to human/commercial impact.\n2. Provide 2–3 primary sources that verify the story. At least:\n   - One PRIMARY TECH SOURCE: (peer-reviewed DOI or arXiv paper with version/date OR official GitHub repo with commit/PR date OR official company technical blog post).\n   - One SECONDARY CORROBORATING SOURCE: (reputable news outlet, conference proceeding, or company press release).\n3. For each source, include:\n   - title, exact URL, publication date (YYYY-MM-DD), type (paper/blog/repo/press/SEC), and a one-line note: \"why it verifies\".\n4. For any numeric claim you will later assert (e.g., \"88% of queries\"), ensure the source actually contains that number. If the source gives a figure differently, report the exact figure and location of the figure in the source (e.g., \"see Figure 2, page 6\" or \"arXiv v2, paragraph 3\").\n5. If the story touches YMYL topics (health/finance/legal), include an immediate risk note listing which regulations may apply (e.g., HIPAA, FDA, CE, GDPR, SEC) and what kind of verification is required.\n6. Check basic credibility signals for each source and report them: (a) publisher credibility (Nature/IEEE/ACM/official corp blog), (b) if arXiv — indicate whether paper has code/benchmarks, (c) if GitHub — include last commit date and license, (d) if press release — confirm corporate domain and date/time.\n7. Output JSON only, EXACTLY in this format:\n\n{\n \"headline\": \"One-line headline\",\n \"sources\": [\n   {\n     \"title\":\"Exact source title\",\n     \"url\":\"https://...\",\n     \"date\":\"YYYY-MM-DD\",\n     \"type\":\"paper|arXiv|repo|blog|press|SEC\",\n     \"why\":\"One-line why this verifies (include exact page/figure if relevant)\",\n     \"credibility\":\"short note: Nature/IEEE/company blog/press/etc\",\n     \"notes\":\"any caveats about the source\"\n   },\n   ...\n ],\n \"riskNote\":\"If YMYL risk exists list regulators and verification steps; otherwise empty string\"\n}",
      "evergreen_prompt": "Write an in-depth article on Generative AI. Cover model architectures (transformers, diffusion models, GANs), training methodologies, licensing and provenance issues, watermarking techniques, and detectability challenges. Include real-world applications and ethical considerations."
    },
    "AI Applications": {
      "trending_prompt": "You are an investigative tech reporter specialized in {section}. Search the modern index (Google Search, Google Scholar, arXiv, official blogs, SEC/10-Q when financial figures are used) for one *specific*, high-impact case, study, deployment, or company announcement that occurred within {date_range} (for example \"last 60 days\").\n\nSECTION FOCUS (choose the relevant focus for {section}; these must guide your search terms):\n- Machine Learning: SOTA comparisons, datasets, compute, energy cost, reproducibility.\n- Natural Language Processing: eval metrics (BLEU/ROUGE/EM/Hallucination risk), dataset biases, safety.\n- Computer Vision: datasets (ImageNet/COCO/medical), segmentation/tracking benchmarks, edge vs cloud deployment.\n- Robotics: sim2real, ISO safety standards, real deployments.\n- Generative AI: model provenance, licensing, watermarking, detectability.\n- AI Applications: enterprise ROI, integration case studies, deployment cost.\n- AI Research: arXiv/NeurIPS/CVPR papers, ablations, reproducibility, code availability.\n- AI Ethics: audits, impact assessments, governance, legal cases.\n\nMANDATORY SOURCE & VERIFICATION RULES (follow EXACTLY):\n1. Return exactly one headline (plain English, single line) — a journalist-style headline that connects technical advance to human/commercial impact.\n2. Provide 2–3 primary sources that verify the story. At least:\n   - One PRIMARY TECH SOURCE: (peer-reviewed DOI or arXiv paper with version/date OR official GitHub repo with commit/PR date OR official company technical blog post).\n   - One SECONDARY CORROBORATING SOURCE: (reputable news outlet, conference proceeding, or company press release).\n3. For each source, include:\n   - title, exact URL, publication date (YYYY-MM-DD), type (paper/blog/repo/press/SEC), and a one-line note: \"why it verifies\".\n4. For any numeric claim you will later assert (e.g., \"88% of queries\"), ensure the source actually contains that number. If the source gives a figure differently, report the exact figure and location of the figure in the source (e.g., \"see Figure 2, page 6\" or \"arXiv v2, paragraph 3\").\n5. If the story touches YMYL topics (health/finance/legal), include an immediate risk note listing which regulations may apply (e.g., HIPAA, FDA, CE, GDPR, SEC) and what kind of verification is required.\n6. Check basic credibility signals for each source and report them: (a) publisher credibility (Nature/IEEE/ACM/official corp blog), (b) if arXiv — indicate whether paper has code/benchmarks, (c) if GitHub — include last commit date and license, (d) if press release — confirm corporate domain and date/time.\n7. Output JSON only, EXACTLY in this format:\n\n{\n \"headline\": \"One-line headline\",\n \"sources\": [\n   {\n     \"title\":\"Exact source title\",\n     \"url\":\"https://...\",\n     \"date\":\"YYYY-MM-DD\",\n     \"type\":\"paper|arXiv|repo|blog|press|SEC\",\n     \"why\":\"One-line why this verifies (include exact page/figure if relevant)\",\n     \"credibility\":\"short note: Nature/IEEE/company blog/press/etc\",\n     \"notes\":\"any caveats about the source\"\n   },\n   ...\n ],\n \"riskNote\":\"If YMYL risk exists list regulators and verification steps; otherwise empty string\"\n}",
      "evergreen_prompt": "Write a comprehensive guide to AI Applications across industries. Cover healthcare, finance, manufacturing, retail, and education. For each domain, explain ROI metrics, integration challenges, deployment costs, and real-world case studies. Include best practices and lessons learned."
    },
    "AI Research": {
      "trending_prompt": "You are an investigative tech reporter specialized in {section}. Search the modern index (Google Search, Google Scholar, arXiv, official blogs, SEC/10-Q when financial figures are used) for one *specific*, high-impact case, study, deployment, or company announcement that occurred within {date_range} (for example \"last 60 days\").\n\nSECTION FOCUS (choose the relevant focus for {section}; these must guide your search terms):\n- Machine Learning: SOTA comparisons, datasets, compute, energy cost, reproducibility.\n- Natural Language Processing: eval metrics (BLEU/ROUGE/EM/Hallucination risk), dataset biases, safety.\n- Computer Vision: datasets (ImageNet/COCO/medical), segmentation/tracking benchmarks, edge vs cloud deployment.\n- Robotics: sim2real, ISO safety standards, real deployments.\n- Generative AI: model provenance, licensing, watermarking, detectability.\n- AI Applications: enterprise ROI, integration case studies, deployment cost.\n- AI Research: arXiv/NeurIPS/CVPR papers, ablations, reproducibility, code availability.\n- AI Ethics: audits, impact assessments, governance, legal cases.\n\nMANDATORY SOURCE & VERIFICATION RULES (follow EXACTLY):\n1. Return exactly one headline (plain English, single line) — a journalist-style headline that connects technical advance to human/commercial impact.\n2. Provide 2–3 primary sources that verify the story. At least:\n   - One PRIMARY TECH SOURCE: (peer-reviewed DOI or arXiv paper with version/date OR official GitHub repo with commit/PR date OR official company technical blog post).\n   - One SECONDARY CORROBORATING SOURCE: (reputable news outlet, conference proceeding, or company press release).\n3. For each source, include:\n   - title, exact URL, publication date (YYYY-MM-DD), type (paper/blog/repo/press/SEC), and a one-line note: \"why it verifies\".\n4. For any numeric claim you will later assert (e.g., \"88% of queries\"), ensure the source actually contains that number. If the source gives a figure differently, report the exact figure and location of the figure in the source (e.g., \"see Figure 2, page 6\" or \"arXiv v2, paragraph 3\").\n5. If the story touches YMYL topics (health/finance/legal), include an immediate risk note listing which regulations may apply (e.g., HIPAA, FDA, CE, GDPR, SEC) and what kind of verification is required.\n6. Check basic credibility signals for each source and report them: (a) publisher credibility (Nature/IEEE/ACM/official corp blog), (b) if arXiv — indicate whether paper has code/benchmarks, (c) if GitHub — include last commit date and license, (d) if press release — confirm corporate domain and date/time.\n7. Output JSON only, EXACTLY in this format:\n\n{\n \"headline\": \"One-line headline\",\n \"sources\": [\n   {\n     \"title\":\"Exact source title\",\n     \"url\":\"https://...\",\n     \"date\":\"YYYY-MM-DD\",\n     \"type\":\"paper|arXiv|repo|blog|press|SEC\",\n     \"why\":\"One-line why this verifies (include exact page/figure if relevant)\",\n     \"credibility\":\"short note: Nature/IEEE/company blog/press/etc\",\n     \"notes\":\"any caveats about the source\"\n   },\n   ...\n ],\n \"riskNote\":\"If YMYL risk exists list regulators and verification steps; otherwise empty string\"\n}",
      "evergreen_prompt": "Write a detailed overview of recent AI research trends. Discuss landmark papers from NeurIPS, CVPR, ICLR, and arXiv. Cover topics like scaling laws, few-shot learning, multimodal models, and reproducibility challenges. Include analysis of ablation studies and code availability in the research community."
    },
    "AI Ethics": {
      "trending_prompt": "You are an investigative tech reporter specialized in {section}. Search the modern index (Google Search, Google Scholar, arXiv, official blogs, SEC/10-Q when financial figures are used) for one *specific*, high-impact case, study, deployment, or company announcement that occurred within {date_range} (for example \"last 60 days\").\n\nSECTION FOCUS (choose the relevant focus for {section}; these must guide your search terms):\n- Machine Learning: SOTA comparisons, datasets, compute, energy cost, reproducibility.\n- Natural Language Processing: eval metrics (BLEU/ROUGE/EM/Hallucination risk), dataset biases, safety.\n- Computer Vision: datasets (ImageNet/COCO/medical), segmentation/tracking benchmarks, edge vs cloud deployment.\n- Robotics: sim2real, ISO safety standards, real deployments.\n- Generative AI: model provenance, licensing, watermarking, detectability.\n- AI Applications: enterprise ROI, integration case studies, deployment cost.\n- AI Research: arXiv/NeurIPS/CVPR papers, ablations, reproducibility, code availability.\n- AI Ethics: audits, impact assessments, governance, legal cases.\n\nMANDATORY SOURCE & VERIFICATION RULES (follow EXACTLY):\n1. Return exactly one headline (plain English, single line) — a journalist-style headline that connects technical advance to human/commercial impact.\n2. Provide 2–3 primary sources that verify the story. At least:\n   - One PRIMARY TECH SOURCE: (peer-reviewed DOI or arXiv paper with version/date OR official GitHub repo with commit/PR date OR official company technical blog post).\n   - One SECONDARY CORROBORATING SOURCE: (reputable news outlet, conference proceeding, or company press release).\n3. For each source, include:\n   - title, exact URL, publication date (YYYY-MM-DD), type (paper/blog/repo/press/SEC), and a one-line note: \"why it verifies\".\n4. For any numeric claim you will later assert (e.g., \"88% of queries\"), ensure the source actually contains that number. If the source gives a figure differently, report the exact figure and location of the figure in the source (e.g., \"see Figure 2, page 6\" or \"arXiv v2, paragraph 3\").\n5. If the story touches YMYL topics (health/finance/legal), include an immediate risk note listing which regulations may apply (e.g., HIPAA, FDA, CE, GDPR, SEC) and what kind of verification is required.\n6. Check basic credibility signals for each source and report them: (a) publisher credibility (Nature/IEEE/ACM/official corp blog), (b) if arXiv — indicate whether paper has code/benchmarks, (c) if GitHub — include last commit date and license, (d) if press release — confirm corporate domain and date/time.\n7. Output JSON only, EXACTLY in this format:\n\n{\n \"headline\": \"One-line headline\",\n \"sources\": [\n   {\n     \"title\":\"Exact source title\",\n     \"url\":\"https://...\",\n     \"date\":\"YYYY-MM-DD\",\n     \"type\":\"paper|arXiv|repo|blog|press|SEC\",\n     \"why\":\"One-line why this verifies (include exact page/figure if relevant)\",\n     \"credibility\":\"short note: Nature/IEEE/company blog/press/etc\",\n     \"notes\":\"any caveats about the source\"\n   },\n   ...\n ],\n \"riskNote\":\"If YMYL risk exists list regulators and verification steps; otherwise empty string\"\n}",
      "evergreen_prompt": "Write a comprehensive article on AI Ethics. Cover algorithmic bias, fairness metrics, transparency and explainability (XAI), privacy concerns (GDPR, CCPA), and governance frameworks. Include case studies of AI audits, impact assessments, and legal precedents. Discuss responsible AI principles."
    }
  },
  "settings": {
    "articles_per_day": 16,
    "language": "en",
    "article_length": "1500-2000 words",
    "publish_time": "09:00 UTC",
    "quality_focus": "high",
    "source_verification": "mandatory",
    "adsense_optimization": true
  }
}
